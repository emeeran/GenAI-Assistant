
Mastering Linux Shell Scripting
Second Edition
"QSBDUJDBMHVJEFUP-JOVYDPNNBOEMJOF#BTITDSJQUJOHBOE
4IFMMQSPHSBNNJOH
Mokhtar Ebrahim
Andrew Mallett
BIRMINGHAM - MUMBAI
Mastering Linux Shell Scripting
SSecond Edition
Copyright a 2018 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, without the prior written permission of the publisher, except in the case of brief quotations
embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied. Neither the
authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to
have been caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products
mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy
of this information.
Commissioning Editor:  Vijin Boricha
Acquisition Editor:  Rohit Rajkumar
Content Development Editor:  Ron Mathew
Technical Editor:  Prachi Sawant
Copy Editor:  Safis Editing
Project Coordinator:  Judie Jose
Proofreader:  Safis Editing
Indexer:  Mariammal Chettiyar
Graphics:  Tom Scaria
Production Coordinator:  Aparna Bhagat
First published: December 2015
Second edition: April 2018
Production reference: 1180418
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham
B3 2PB, UK.
ISBN 978-1-78899-055-4
XXXQBDLUQVCDPN
NBQUJP
Mapt is an online digital library that gives you full access to over 5,000 books and videos, as
well as industry leading tools to help you plan your personal development and advance
your career. For more information, please visit our website.
Why subscribe?
Spend less time learning and more time coding with practical eBooks and Videos
from over 4,000 industry professionals
Improve your learning with Skill Plans built especially for you
Get a free eBook or video every month
Mapt is fully searchable
Copy and paste, print, and bookmark content
PacktPub.com
Did you know that Packt offers eBook versions of every book published, with PDF and
ePub files available? You can upgrade to the eBook version at XXX1BDLU1VCDPN  and as a
print book customer, you are entitled to a discount on the eBook copy. Get in touch with us
at TFSWJDF!QBDLUQVCDPN  for more details.
At XXX1BDLU1VCDPN , you can also read a collection of free technical articles, sign up for a
range of free newsletters, and receive exclusive discounts and offers on Packt books and
eBooks.
Contributors
About the authors
Mokhtar Ebrahim  started working as a Linux system administrator in 2010. He is
responsible for maintaining, securing, and troubleshooting Linux servers for multiple
clients around the world. He loves writing shell and Python scripts to automate his work.
He writes technical articles on the Like Geeks website about Linux, Python, web
development, and server administration. He is a father to a beautiful girl and a husband to
a faithful wife.
I would like to thank my wife for helping me with all her efforts to finish this book. Thank
you, Doaa, for being a part of that. Also, I would like to thank everyone at Packt for
working with me to make sure the book is released. Last but not least, I'd like to thank
Brian Fox, the author of the bash shell, for creating such an awesome piece of software;
without it, such a book would not exist.
Andrew Mallett  is the owner of The Urban Penguin, and he is a comprehensive provider of
professional Linux software development, training, and services. Having always been a
command-line fan, he feels that so much time can be saved through knowing command-line
shortcuts and scripting. TheUrbanPenguin YouTube channel, maintained by Andrew, has
well over 800 videos to support this, and he has authored four other Packt titles.
About the reviewer
Sebastiaan Tammer  is a Linux enthusiast from The Netherlands. After attaining his BSc in
Information Sciences, he graduated with MSc in Business Informatics, both from Utrecht
University. His professional career started in Java development before he pivoted into 
Linux.
He has worked on number of technologies, such as Puppet, Chef, Docker, and Kubernetes.
He spends a lot of time in and around his terminal of choice: bash. Whether it is creating
complex scripting solutions or just automating simple tasks, there is hardly anything he
hasn't done with bash!
I would like to thank my girlfriend, Sanne, for all the help and support she has given me
throughout the years. She has had to endure the late nights studying, me fixing stuff
(which I had inevitably broken only hours earlier), and my endless storytelling about all
those exciting new technologies. Thanks for the enormous amount of patience and love, I
could not have done it without you!
Packt is searching for authors like you
If you're interested in becoming an author for Packt, please visit BVUIPSTQBDLUQVCDPN  and
apply today. We have worked with thousands of developers and tech professionals, just
like you, to help them share their insight with the global tech community. You can make a
general application, apply for a specific hot topic that we are recruiting an author for, or
submit your own idea.
Table of Contents
Preface 1
Chapter 1: The What and Why of Scripting with Bash 6
Technical requirements 7
Types of Linux shells 7
What is bash scripting? 8
The bash command hierarchy 9
Command type 9
Command PATH 10
Preparing text editors for scripting 11
Configuring vim 12
Configuring nano 12
Configuring gedit 12
Creating and executing scripts 14
Hello World! 15
Executing the script 15
Checking the exit status 16
Ensuring a unique name 17
Hello Dolly! 17
Running the script with arguments 18
The importance of correct quotes 19
Printing the script name 21
Declaring variables 22
User-defined variables 22
Environment variables 23
Variable scope 24
Command substitution 25
Debugging your scripts 26
Summary 33
Questions 34
Further reading 34
Chapter 2: Creating Interactive Scripts 35
Technical requirements 35
Using echo with options 36
Basic script using read 37
Script comments 38
Enhancing scripts with read prompts 38
Limiting the number of entered characters 39
Table of Contents
[ ii ]Controlling the visibility of the entered text 40
Passing options 40
Passing parameters with options 42
Read options values 44
Try to be standard 45
Enhancing learning with simple scripts 46
Backing-up with scripts 46
Connecting to a server 48
Version 1 â€“ ping 48
Version 2 â€“ SSH 48
Version 3 â€“ MySQL/MariaDB 49
Reading files 50
Summary 51
Questions 51
Further reading 52
Chapter 3: Conditions Attached 53
Technical requirements 54
Simple decision paths using command-line lists 54
Verifying user input with lists 56
Using the test shell built-in 56
Testing strings 57
Testing integers 58
Testing file types 59
Creating conditional statements using if 60
Extending if with else 61
Test command with the if command 62
Checking strings 62
Checking files and directories 65
Checking numbers 65
Combining tests 66
More conditions with elif 67
Creating the backup2.sh using elif 67
Using case statements 69
Recipe â€“ building a frontend with grep 71
Summary 72
Questions 73
Further reading 74
Chapter 4: Creating Code Snippets 75
Technical requirements 75
Abbreviations 76
Using code snippets 77
Bringing color to the Terminal 78
Table of Contents
[ iii ]Creating snippets using VS Code 80
Summary 85
Questions 85
Further reading 86
Chapter 5: Alternative Syntax 87
Technical requirement 87
Recapping the test command 88
Testing files 88
Adding logic 88
Square brackets as not seen before 88
Providing parameter defaults 90
Variables 90
Special parameters 90
Setting defaults 91
When in doubt â€“ quote! 93
Advanced tests using [[ 95
White space 95
Other advanced features 96
Pattern matching 96
Regular expressions 96
Regular expression script 97
Arithmetic operations using (( 98
Simple math 98
Parameter manipulation 98
Standard arithmetic tests 99
Summary 100
Questions 100
Further reading 101
Chapter 6: Iterating with Loops 102
Technical requirement 103
for loops 103
Advanced for loops 106
The IFS 107
Counting directories and files 108
C-style for loops 109
Nested loops 110
Redirecting loop output 111
Controlling the loop 111
while loops and until loops 113
Reading input from files 114
Creating operator menus 116
Summary 119
Table of Contents
[ iv ]Questions 119
Further reading 120
Chapter 7: Creating Building Blocks with Functions 121
Technical requirements 121
Introducing functions 122
Passing parameters to functions 125
Passing arrays 128
Variable scope 129
Returning values from functions 130
Recursive functions 132
Using functions in menus 133
Summary 135
Questions 135
Further reading 136
Chapter 8: Introducing the Stream Editor 137
Technical requirements 138
Using grep to display text 138
Displaying received data on an interface 138
Displaying user account data 139
Listing the number of CPUs in a system 140
Parsing CSV files 143
The CSV file 143
Isolating catalog entries 145
Understanding the basics of sed 146
The substitute command 147
Global replacement 148
Limiting substitution 150
Editing the file 150
Other sed commands 151
The delete command 151
The insert and append commands 152
The change command 153
The transform command 153
Multiple sed commands 154
Summary 154
Questions 155
Further reading 156
Chapter 9: Automating Apache Virtual Hosts 157
Technical requirements 158
Apache name-based Virtual Hosts 158
Creating the virtual host template 159
First steps 160
Table of Contents
[ v ]Isolating lines 160
sed script files 162
Automating virtual host creation 164
Prompting for data during site creation 166
Summary 169
Questions 170
Further reading 170
Chapter 10: AWK Fundamentals 171
Technical requirements 171
The history behind AWK 172
Displaying and filtering content from files 173
AWK variables 176
User-defined variables 178
Conditional statements 180
The if command 180
while loops 181
for loops 182
Formatting output 183
Further filtering to display users by UID 185
AWK control files 186
Built-in functions 187
Summary 187
Questions 188
Further reading 189
Chapter 11: Regular Expressions 190
Technical requirements 190
Regular expression engines 191
Defining BRE patterns 192
Anchor characters 193
The dot character 195
The character class 195
Ranges of characters 197
Special character classes 198
The asterisk 199
Defining ERE patterns 201
The question mark 201
The plus sign 203
Curly braces 204
The pipe character 206
Expression grouping 207
Using grep 208
Summary 209
Table of Contents
[ vi ]Questions 210
Further reading 210
Chapter 12: Summarizing Logs with AWK 211
Technical requirements 211
The HTTPD log file format 212
Displaying data from web logs 213
Selecting entries by date 213
Summarizing 404 errors 215
Summarizing HTTP access codes 215
Resources hits 217
Identify image hotlinking 218
Displaying the highest ranking IP address 219
Displaying the browser data 220
Working with email logs 221
Summary 221
Questions 222
Further reading 222
Chapter 13: A Better lastlog with AWK 223
Technical requirements 223
Using AWK ranges to exclude data 224
The lastlog command 224
Horizontally filtering rows with AWK 225
Counting matched rows 225
Conditions based on the number of fields 226
Manipulating the AWK record separator to report on XML data 228
Apache Virtual Hosts 228
XML catalog 229
Summary 231
Questions 232
Further reading 232
Chapter 14: Using Python as a Bash Scripting Alternative 233
Technical requirements 233
What is Python? 234
Saying Hello World the Python way 236
Pythonic arguments 236
Supplying arguments 237
Counting arguments 237
Significant whitespace 239
Reading user input 240
Using Python to write to files 241
String manipulation 242
Table of Contents
[ vii ]Summary 243
Questions 244
Further reading 244
Assessments 245
Other Books You May Enjoy 252
Index 255
Preface
First, you'll learn about Linux shells and why we chose the bash shell. Then, you'll learn
how to write a simple bash script and how to edit your bash script using Linux editors.
Following this, you will learn how to define a variable and the visibility of a variable. After
this, you will learn how to store command execution output into a variable, which is called
command substitution. Also, you will learn how to debug your code using bash options
and Visual Studio Code. You will learn how to make your bash script interactive to the user
by accepting input from the user using the read command. Then, you will learn how to read
options and its values if the user passed them to the script. Following this, you will learn
how to write conditional statements such as if statements and how to use case statements.
After this, you will learn how to create code snippets using vim and Visual Studio Code.
For repetitive tasks, you will see how to write for loops, how to iterate over simple values,
and how to iterate over directory content. Also, you will learn how to write nested loops.
Along with this, you will write while and until loops. Then, we will move on to functions,
the reusable chunks of code. You will learn how to write functions and how to use them.
After this, you will be introduced to one of the best tools in Linux, which is Stream Editor.
As we are still talking about text processing, we will introduce AWK, one of the best text
processing tools in Linux that you will ever see.
After this, you will learn how to empower your text processing skills by writing better
regular expressions. Finally, you will be introduced to Python as an alternative to bash
scripting.
Who this book is for
This book targets system administrators and developers who would like to write a better
shell script to automate their work. Some programming experience is preferable. If you
don't have any background in shell scripting, no problem, the book will discuss everything
from the beginning.
Preface
[ 2 ]What this book covers
$IBQUFS , The What and Why of Scripting with Bash , will introduce Linux shells, how to
write your first shell script, how to prepare your editor, how to debug your shell script, and
some basic bash programming, such as declaring variables, variable scope, and command
substitution.
$IBQUFS , Creating Interactive Scripts , covers how to read input from the user using SFBE
command, how to pass options to your script, how to control the visibility of the entered
text, and how to limit the number of entered characters.
$IBQUFS , Conditions Attached , will introduce the JG statement, the DBTF  statement, and
other testing command such as FMTF  and FMJG .
$IBQUFS , Creating Code Snippets , covers creating and using code snippets using editors,
such as vim and Visual Studio Code.
$IBQUFS , Alternative Syntax , will discuss advanced testing using << and how to perform
arithmetic operations. 
$IBQUFS , Iterating with Loops , will teach you how to use GPS loops, XIJMF  loops, and
VOUJM  loops to iterate over simple values and complex values. 
$IBQUFS , Creating Building Blocks with Functions , will introduce functions and explains
how to create a function, list builtin functions, pass parameters to functions, and writing
recursive functions. 
$IBQUFS , Introducing the Stream Editor , will introduce the basics of sed tool to manipulate
files, such as adding, replacing deleting, and transforming text.
$IBQUFS , Automating Apache Virtual Hosts , contains a practical example of sed and
explains how to create virtual hosts automatically using sed.
$IBQUFS , AWK Fundamentals , will discuss AWK and how to filter file content using it.
Also, we will discuss some AWK programming basics.
$IBQUFS , Regular Expressions , covers regular expressions, their engines, and how to use
them with sed and AWK to empower your script.
$IBQUFS , Summarizing Logs with AWK , will show how to process the IUUQEDPOG
Apache log file using AWK and extract useful well-formatted data.
Preface
[ 3 ]$IBQUFS , A Better lastlog with AWK , will show you how to use AWK to output beautiful
reports using the lastlog command by filtering and processing the lastlog output.
$IBQUFS , Using Python as a Bash Scripting Alternative , will discuss Python programming
language basics and explains how to write some Python scripts as a bash script alternative.
To get the most out of this book
I assume that you have a little programming background. Even if you don't have a
programming background, the book will start from the beginning.
You should know some Linux basics such as the basic commands such as MT, DE, and
XIJDI .
Download the example code files
You can download the example code files for this book from your account at
XXXQBDLUQVCDPN . If you purchased this book elsewhere, you can visit
XXXQBDLUQVCDPNTVQQPSU  and register to have the files emailed directly to you.
You can download the code files by following these steps:
Log in or register at XXXQBDLUQVCDPN . 1.
Select the SUPPORT  tab. 2.
Click on Code Downloads & Errata . 3.
Enter the name of the book in the Search  box and follow the onscreen 4.
instructions.
Once the file is downloaded, please make sure that you unzip or extract the folder using the
latest version of:
WinRAR/7-Zip for Windows
Zipeg/iZip/UnRarX for Mac
7-Zip/PeaZip for Linux
The code bundle for the book is also hosted on GitHub at IUUQTHJUIVCDPN
1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE&EJUJPO . In case there's an
update to the code, it will be updated on the existing GitHub repository.
We also have other code bundles from our rich catalog of books and videos available
at IUUQTHJUIVCDPN1BDLU1VCMJTIJOH . Check them out!
Preface
[ 4 ]Download the color images
We also provide a PDF file that has color images of the screenshots/diagrams used in this
book. You can download it from IUUQTXXXQBDLUQVCDPNTJUFTEFGBVMUGJMFT
EPXOMPBET.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE&EJUJPO@$PMPS*NBHFTQEG .
Conventions used
There are a number of text conventions used throughout this book.
$PEF*O5FYU : Indicates code words in text, database table names, folder names, filenames,
file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an
example: "Edit your script so that it reads like the following complete code block
for )0.&CJOIFMMPTI "
A block of code is set as follows:
JG<GJMF@DPNQSFTTJPO->UIFO
UBS@PQUUBS@M
FMJG<GJMF@DPNQSFTTJPO.>UIFO
UBS@PQUUBS@N
FMTF
UBS@PQUUBS@I
GJ
Any command-line input or output is written as follows:
$ type ls
ls is aliased to 'ls --color=auto'
Bold : Indicates a new term, an important word, or words that you see onscreen. For
example, words in menus or dialog boxes appear in the text like this. Here is an example:
"Another very useful feature is found on the Preferences  | Plugins  tab"
Warnings or important notes appear like this.
Tips and tricks appear like this.
Preface
[ 5 ]Get in touch
Feedback from our readers is always welcome.
General feedback : Email GFFECBDL!QBDLUQVCDPN  and mention the book title in the
subject of your message. If you have questions about any aspect of this book, please email
us at RVFTUJPOT!QBDLUQVCDPN .
Errata : Although we have taken every care to ensure the accuracy of our content, mistakes
do happen. If you have found a mistake in this book, we would be grateful if you would
report this to us. Please visit XXXQBDLUQVCDPNTVCNJUFSSBUB , selecting your book,
clicking on the Errata Submission Form link, and entering the details.
Piracy : If you come across any illegal copies of our works in any form on the Internet, we
would be grateful if you would provide us with the location address or website name.
Please contact us at DPQZSJHIU!QBDLUQVCDPN  with a link to the material.
If you are interested in becoming an author : If there is a topic that you have expertise in
and you are interested in either writing or contributing to a book, please visit
BVUIPSTQBDLUQVCDPN .
Reviews
Please leave a review. Once you have read and used this book, why not leave a review on
the site that you purchased it from? Potential readers can then see and use your unbiased
opinion to make purchase decisions, we at Packt can understand what you think about our
products, and our authors can see your feedback on their book. Thank you!
For more information about Packt, please visit QBDLUQVCDPN .
11
The What and Why of Scripting
with Bash
Welcome to the what and why of bash scripting. In this chapter, you will discover the types
of shells in Linux and why we chose bash. You will learn what bash is, how to write your
first bash script, and how to run it. Also, you will see how to configure Linux editors, such
as vim and nano, in order to type your code.
Like in any other scripting language, variables are the basic blocks of coding. You will learn
how to declare variables such as integers, strings, and arrays. Furthermore, you will learn
how to export these variables and extend their scope outside the running process.
Finally, you will see how to visually debug your code using Visual Studio Code.
We will cover the following topics in this chapter:
Types of Linux shells
What is bash scripting?
The bash command hierarchy
Preparing text editors for scripting
Creating and executing scripts
Declaring variables
Variable scope
Command substitution
Debugging your scripts
The What and Why of Scripting with Bash Chapter 1
[ 7 ]Technical requirements
You'll need a running Linux box. It doesn't matter which distribution you use, since all
Linux distributions are shipped nowadays with the bash shell.
Download and install Visual Studio Code, which is free from Microsoft. You can download
it from here: IUUQTDPEFWJTVBMTUVEJPDPN .
You can use VS Code as an editor instead of vim and nano; it's up to you.
We prefer to use VS Code because it has a lot of features such as code completion,
debugging, and many more besides.
Install CBTIEC , which is a required package for the bash debug plugin. If you are using a
Red Hat-based distribution, you can install it like this:
$ sudo yum install bashdb
If you are using a Debian-based distribution, you can install it like this:
$ sudo apt-get install bashdb
Install the plugin for VS Code, called bash debug, from
IUUQTNBSLFUQMBDFWJTVBMTUVEJPDPNJUFNTJUFN/BNFSPHBMNJDCBTIEFCVH . This
plugin will be used to debug bash scripts.
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Types of Linux shells
As you know, Linux consists of some major parts, such as the kernel, the shell, and the GUI
interface (Gnome, KDE, and so on).
The shell translates your commands and sends them to the system. Most Linux
distributions are shipped with many shells.
The What and Why of Scripting with Bash Chapter 1
[ 8 ]Every shell has its own features, and some of them are very popular among developers
today. These are some of the popular ones:
Sh shell : This is called the Bourne shell, this was developed at AT&T labs in the
70s by a guy named Stephen Bourne. This shell offers many features.
Bash shell : Also called the Bourne again shell, this is very popular and
compatible with sh shell scripts, so you can run your sh scripts without changing
them. We are going to use this shell in this book.
Ksh shell : Also called the Korn shell, this is compatible with sh and bash. Ksh
offers some enhancements over the Bourne shell.
Csh and tcsh : Linux was built using the C language and that drove developers at
Berkeley University to develop a C-style shell in which the syntax is similar to the
C language. Tcsh adds some minor enhancements to csh.
Now we know the types of shells and we know that we are going to use bash, so what is
bash scripting?
What is bash scripting?
The basic idea of bash scripting is to execute multiple commands to automate a specific job.
As you might know, you can run multiple commands from the shell by separating them
with semi colons ( ):
ls ; pwd
The previous line is a mini bash script.
The first command runs, followed by the result of the second command.
Every keyword you type in bash scripting is actually a Linux binary (program), even the JG
statement, or FMTF  or XIJMF  loops. All are Linux executables.
You can say that the shell is the glue that binds these commands together.
The What and Why of Scripting with Bash Chapter 1
[ 9 ]The bash command hierarchy
When working on the bash shell and when you are sitting comfortably at your prompt
eagerly waiting to type a command, you will most likely feel that it is a simple matter of
typing and hitting the Enter  key. You should know better than to think this, as things are
never quite as simple as we imagine.
Command type
For example, if we type and enter MT to list files, it is reasonable to think that we were
running the command. It is possible, but we often will be running an alias. Aliases exist in
memory as a shortcut to commands or commands with options; these aliases are used
before we even check for the file. Bash's built-in UZQF  command can come to our aid here.
The UZQF  command will display the type of command for a given word entered at the
command line. The types of command are listed as follows:
Alias
Function
Shell built-in
Keyword
File
This list is also representative of the order in which they are searched. As we can see, it is
not until the very end where we search for the executable file MT.
The following command demonstrates the simple use type:
$ type ls
ls is aliased to 'ls --color=auto'
We can extend this further to display all the matches for the given command:
$ type -a ls
ls is aliased to 'ls --color=auto'
ls is /bin/ls
The What and Why of Scripting with Bash Chapter 1
[ 10 ]If we need to just type in the output, we can use the U option. This is useful when we need
to test the command type from within a script and only need the type to be returned. This
excludes any superfluous information, and thus makes it easier for us humans to read.
Consider the following command and output:
$ type -t ls
alias
The output is clear and simple, and is just what a computer or script requires.
The built-in UZQF  can also be used to identify shell keywords such as JG, and DBTF . The
following command shows UZQF  being used against multiple arguments and types:
$ type ls quote pwd do id
The output of the command is shown in the following screenshot:
You can also see that the function definition is printed when we stumble across a function
when using UZQF .
Command PATH
Linux will check for executables in the 1"5)  environment only when the full or relative
path to the program is supplied. In general, the current directory is not searched unless it is
in the 1"5) . It is possible to include our current directory within the 1"5)  by adding the
directory to the 1"5)  variable. This is shown in the following command example:
$ export PATH=$PATH:.
The What and Why of Scripting with Bash Chapter 1
[ 11 ]This appends the current directory to the value of the 1"5)  variable; each item in the 1"5)
is separated using a colon. Now your 1"5)  has been updated to include the current
working directory and, each time you change directories, the scripts can be executed easily.
In general, organizing scripts into a structured directory hierarchy is probably a great idea.
Consider creating a subdirectory called CJO within your home directory and add the scripts
into that folder. Adding )0.&CJO  to your 1"5)  variable will enable you to find the
scripts by name and without the file path.
The following command-line list will only create the directory, if it does not already exist:
$ test -d $HOME/bin || mkdir $HOME/bin
Although the preceding command-line list is not strictly necessary, it does show that
scripting in bash is not limited to the actual script, and we can use conditional statements
and other syntax directly at the command line. From our viewpoint, we know that the
preceding command will work whether you have the CJO directory or not. The use of the
)0.&  variable ensures that the command will work without considering your current
filesystem context.
As we work through the book, we will add scripts into the )0.&CJO  directory so that
they can be executed regardless of our working directory.
Preparing text editors for scripting
Throughout the book, we will be working on Linux Mint, and this will include the creation
and editing of the scripts. You, of course, can choose the way you wish to edit your scripts
and may prefer to make use of a graphical editor, so we will show some settings in gedit.
We will make one excursion into a Red Hat system to show screenshots of gedit in this
chapter.
Also, we will use Visual Studio Code as a modern GUI editor to edit and debug our scripts.
To help make the command-line editor easier to use, we can enable options and we can
persist with these options through hidden configuration files. Gedit and other GUI editors,
and their menus, will provide similar functionality.
The What and Why of Scripting with Bash Chapter 1
[ 12 ]Configuring vim
Editing the command line is often a must and is part of a developer's everyday life. Setting
up common options that make life easier in the editor give us the reliability and consistency
we need, a little like scripting itself. We will set some useful options in the vi or vim editor
file, )0.&WJNSD .
The options we set are detailed in the following list:
TFUTIPXNPEF : Ensures we see when we are in insert mode
TFUOPIMTFBSDI : Does not highlight the words that we have searched for
TFUBVUPJOEFOU : We indent our code often; this allows us to return to the last
indent level rather than the start of a new line on each line break
TFUUBCTUPQ : Sets a tab to be four spaces
TFUFYQBOEUBC : Converts tabs to spaces, which is useful when the file moves to
other systems
TZOUBYPO : Note that this does not use the TFU command and is used to turn on
syntax highlighting
When these options are set, the )0.&WJNSD  file should look similar to this:
TFUTIPXNPEF
TFUOPIMTFBSDI
TFUBVUPJOEFOU
TFUUBCTUPQ
TFUFYQBOEUBC
TZOUBYPO
Configuring nano
The nano text editor is increasing in importance and it is the default editor in many systems.
Personally, I don't like the navigation or the lack of navigation features that it has. It can be
customized in the same way as vim. This time, we will edit the )0.&OBOPSD  file. Your
edited file should look something like the following:
TFUBVUPJOEFOU
TFUUBCTJ[F
JODMVEFVTSTIBSFOBOPTIOBOPSD
The last line enables syntax highlighting for shell scripts.
The What and Why of Scripting with Bash Chapter 1
[ 13 ]Configuring gedit
Graphical editors, such as gedit, can be configured using the preferences menu, and are
pretty straightforward.
Enabling tab spacing to be set to 4 spaces and expanding tabs to spaces can be done using
the Preferences  | Editor  tab, as shown in the following screenshot:
You can download the example code files from your account at
IUUQXXXQBDLUQVCDPN  for all the Packt Publishing books you have
purchased. If you purchased this book elsewhere, you can visit
IUUQXXXQBDLUQVCDPNTVQQPSU  and register to have the files e-mailed
directly to you.
The What and Why of Scripting with Bash Chapter 1
[ 14 ]Another very useful feature is found on the Preferences  | Plugins  tab. Here, we can enable
the Snippets  plugin, which can be used to insert code samples. This is shown in the
following screenshot:
For the rest of the book, we will be working on the command line and in vim; feel free to
use the editor that you work with best. We have now laid the foundations to create good
scripts, and, although whitespace, tabs, and spaces in bash scripts are not significant, a well-
laid-out file with consistent spacing is easy to read. When we look at Python later in the
book, you will realize that in some languages, the whitespace is significant to the language
and it is better to adopt good habits early on.
Creating and executing scripts
With our editors primed and ready, we can now move quickly to creating and executing
our scripts. If you are reading this book with some prior experience, we will warn you that
we are going to start with the basics, but we will also include looking at positional
parameters; feel free to move on at your own pace.
The What and Why of Scripting with Bash Chapter 1
[ 15 ]Hello World!
As you know, it is almost obligatory to begin with a )FMMP8PSME  script and we will not
disappoint as far as this is concerned. We will begin by creating a new
script, )0.&CJOIFMMPTI . The contents of the file should read as in the following
screenshot:
We hope that you haven't struggled with this too much; it is just three lines, after all. We
encourage you to run through the examples as you read to really help you instill the
information with good hands-on practice.
CJOCBTI : Normally, this is always the first line of the script and is known as
the shebang. The shebang starts with a comment, but the system still uses this
line. A comment in a shell script has the  symbol. The shebang instructs the
interpreter of the system to execute the script. We use bash for shell scripts, and
we may use PHP or Perl for other scripts, as required. If we do not add this line,
then the commands will be run within the current shell; it may cause issues if we
run another shell.
FDIP)FMMP8PSME : The FDIP  command will be picked up in a built-in shell
and can be used to write a standard output, 45%065 ; this defaults to the screen.
The information to print is enclosed in double quotes; there will be more on
quotes later.
FYJU : The FYJU  command is a built-in shell, and is used to leave or exit the
script. The FYJU  code is supplied as an integer argument. A value of anything
other than  will indicate some type of error in the script's execution.
Executing the script
With the script saved in our 1"5)  environment, it still will not execute as a standalone
script. We will have to assign and execute permissions for the file, as needed. For a simple
test, we can run the file directly with bash. The following command shows you how to do
this:
$ bash $HOME/bin/hello1.sh
The What and Why of Scripting with Bash Chapter 1
[ 16 ]We should be rewarded with the )FMMP8PSME  text being displayed on our screens. This is
not a long-term solution, as we need to have the script in the )0.&CJO  directory,
specifically, to make running the script easy from any location without typing the full path.
We need to add in the execute permissions as shown in the following code:
$ chmod +x $HOME/bin/hello1.sh
We should now be able to run the script simply, as shown in the following screenshot:
Checking the exit status
This script is simple, but we still need to know how to make use of the exit codes from
scripts and other applications. The command-line list that we generated earlier, while 
creating the )0.&CJO  directory, is a good example of how we can use the exit code:
$ command1 || command 2
In the preceding example, DPNNBOE  is executed only if DPNNBOE  fails in some way. To be
specific, DPNNBOE  will run if DPNNBOE  exits with a status code other than .
Similarly, in the following extract, we will only execute DPNNBOE  if DPNNBOE  succeeds
and issues an exit code of :
$ command1 && command2
To read the exit code from our script explicitly, we can view the  variable, as shown in the
following example:
$ hello1.sh
$ echo $?
The expected output is , as this is what we have added to the last line of the file and there
is precious little else that can go wrong to cause the failure to reach that line.
The What and Why of Scripting with Bash Chapter 1
[ 17 ]Ensuring a unique name
We can now create and execute a simple script, but we need to consider the name a little. In
this case, IFMMPTI  will be good enough and is unlikely to clash with anything else on the
system. We should avoid using names that may clash with existing aliases, functions,
keywords, and building commands, as well as avoiding names of programs already in use.
Adding the TI suffix to the file does not guarantee the name will be unique, but, in Linux,
where we do not use file extensions, the suffix is part of the filename. This helps you to
provide a unique identity to your script. Additionally, the suffix is used by the editor to
help you identify the file for syntax highlighting. If you recall, we specifically added the
syntax highlighting file TIOBOPSD  to the nano text editor. Each of these files is specific to a
suffix and subsequent language.
Referring back to the command hierarchy within this chapter, we can use a UZQF  to
determine the location and type of file IFMMPTI :
$ type hello1.sh  #To determine the type and path
$ type -a hello1.sh  #To print all commands found if the name is NOT unique
$ type -t hello1.sh ~To print the simple type of the command
These commands and output can be seen in the following screenshot:
Hello Dolly!
It is possible that we might need a little more substance in the script than a simple fixed
message. Static message content does have its place, but we can make this script much more
useful by building in some flexibility.
In this chapter, we will look at the positional parameters or arguments that we can supply
to the script and in the next chapter, we will see how we can make the script interactive and
also prompt the user for input at runtime.
The What and Why of Scripting with Bash Chapter 1
[ 18 ]Running the script with arguments
We can run the script with arguments; after all, it's a free world and Linux promotes your
freedom to do what you want to do with the code. However, if the script does not make use
of the arguments, then they will be silently ignored. The following command shows the
script running with a single argument:
$ hello1.sh fred
The script will still run and will not produce an error. The output will not change either and
will print )FMMP8PSME :
Argument
IdentifierDescription
 The name of the script itself, which is often used in usage statements.
 A positional argument, which is the first argument passed to the script.
\^Where two or more digits are needed to represent the argument
position. Brace brackets are used to delimit the variable name from any
other content. Single value digits are expected.
The argument count is especially useful when we need to set the amount
of arguments needed for correct script execution.
 Refers to all arguments.
For the script to make use of the argument, we can change its content a little. Let's first copy
the script, add in the execute permissions, and then edit the new IFMMPTI :
$ cp $HOME/bin/hello1.sh $HOME/bin/hello2.sh
$ chmod +x $HOME/bin/hello2.sh
We need to edit the IFMMPTI  file to make use of the argument as it is passed at the
command line. The following screenshot shows the simplest use of command-line
arguments, now allowing us to have a custom message:

The What and Why of Scripting with Bash Chapter 1
[ 19 ]Run the script now; we can provide an argument as shown in the following:
$ hello2.sh fred
The output should now say )FMMPGSFE . If we do not provide an argument, then the
variable will be empty and will just print )FMMP . You can refer to the following screenshot
to see the execution argument and output:
If we adjust the script to use , all the arguments will print. We will see )FMMP  and then a
list of all the supplied arguments. Edit the script and replace the FDIP  line as follows:
echo "Hello $*"
This will execute the script with the following arguments:
$ hello2.sh fred wilma betty barney
And this will result in the output shown in the following screenshot:
If we want to print )FMMPOBNF , with each name on a separate line, we will need to wait
a little until we cover looping structures. A GPS loop is a good way to achieve this.
The importance of correct quotes
So far, we have used a simple double-quoting mechanism to encase the strings that we want
to use with echo.
In the first script, it does not matter if we use single or double quotes. FDIP)FMMP
8PSME  will be exactly the same as FDIP	)FMMP8PSME	 .
The What and Why of Scripting with Bash Chapter 1
[ 20 ]However, this is not the case in the second script, so it is very important to understand the
quoting mechanisms available in bash.
As we have seen, using double quotes in FDIP)FMMP  will result in )FMMPGSFE  or
whatever the supplied value is. Whereas, if we use single quotes in FDIP	)FMMP	 , the
printed output on the screen will be )FMMP ; that is, we see the variable name and not its
value.
The idea of the quotes is to protect special characters, such as a space between the two
words; both quotes protect the space from being misinterpreted. The space is normally read
as a default field, separated by the shell. In other words, all characters are read by the shell
as literals with no special meaning. This has the knock-on effect of the  symbol printing its
literal format rather than allowing bash to expand its value. The bash shell is prevented
from expanding the variable's value as it is protected by the single quotes.
This is where the double quote comes to our rescue. The double quote will protect all the
characters except the , allowing bash to expand the stored value.
If we ever need to use a literal  within the quoted string, along with variables that need to
be expanded, we can use double quotes, but escape the desired  with the backslash ( =). For
example, FDIP64&3FBSOT=  would print as 'SFEFBSOT  if the current user
were Fred.
Try the following examples at the command line using all quoting mechanisms. Feel free to
up your hourly rate as required:
$ echo "$USER earns $4"
$ echo '$USER earns $4'
$ echo "$USER earns \$4"
The output is shown in the following screenshot:

The What and Why of Scripting with Bash Chapter 1
[ 21 ]Printing the script name
The  variable represents the script name, and this is often used in usage statements. As
we are not yet looking at conditional statements, we will get the script name printed above
the displayed name.
Edit your script so that it reads like the following complete code block for
)0.&CJOIFMMPTI :
CJOCBTI
FDIP:PVBSFVTJOH
FDIP)FMMP
FYJU
The output from the command is shown in the following screenshot:
If we prefer not to print the path and only want the name of the script to show, we can use
the CBTFOBNF  command, which extracts the name from the path. Adjust the script so that
the second line now reads as follows:
echo "You are using $(basename $0)"
The 
  syntax is used to evaluate the output of the inner command. We first run
CBTFOBNF  and feed the result into an unnamed variable represented by the .
The new output will appear as seen in the following screenshot:
It is possible to achieve the same results using back quotes; this is less easy to read, but we
have mentioned this as you might need to understand and modify the scripts that have
been written by others. The alternative to the 
  syntax is shown in the following
example:
echo "You are using 'basename $0'"
The What and Why of Scripting with Bash Chapter 1
[ 22 ]Please note that the characters used are back quotes and NOT  single quotes. On UK and US
keyboards, these are found in the top-left corner next to the number 1 key.
Declaring variables
Just like in any programming language, you can declare variables in bash scripts. So, what
are these variables and what are the benefits of using them?
Well, a variable is like a placeholder where you store some value for later use in your code.
There are two kinds of variables you can declare in your script:
User-defined variables
Environment variables
User-defined variables
To declare a variable, just type the name you want and set its value using the equals sign
().
Check out this example:
CJOCBTI
OBNF.PLIUBS
BHF
UPUBM
FDIPOBNFQSJOUT.PLIUBS
FDIPBHFQSJOUT
FDIPUPUBMQSJOUT
As you can see, to print the variable's value, you should use the dollar sign ( ) before it.
Note that there are no spaces  between the variable name and the equals sign, or between
the equals sign and the value.
If you forget and type a space in between, the shell will treat the variable as if it were a
command, and, since there is no such command, it will show an error.
The What and Why of Scripting with Bash Chapter 1
[ 23 ]All of the following examples are incorrect declarations:
%PO	UEFDMBSFWBSJBCMFTMJLFUIJT
OBNF.PLIUBS
BHF
UPUBM
Another useful type of user-defined variable is the array. An array can hold multiple
values. So, if you have tens of values you want to use, you should use arrays instead of
filling your script with variables.
To declare an array, just enclose its elements between brackets, like this:
CJOCBTI
NZBSS
POFUXPUISFFGPVSGJWF
To access a specific array element, you can specify its index like this:
CJOCBTI
NZBSS
POFUXPUISFFGPVSGJWF
FDIP\NZBSS<>^QSJOUTUXPXIJDIJTUIFTFDPOEFMFNFOU
The index is zero based.
To print the array elements, you can use an asterisk, like this:
CJOCBTI
NZBSS
POFUXPUISFFGPVSGJWF
FDIP\NZBSS<>^
To remove a specific element from the array, you can use the VOTFU  command:
CJOCBTI
NZBSS
POFUXPUISFFGPVSGJWF
VOTFUNZBSS<>5IJTXJMMSFNPWFUIFTFDPOEFMFNFOU
VOTFUNZBSS5IJTXJMMSFNPWFBMMFMFNFOUT
Environment variables
So far, we have used variables that we didn't define, such as #"4)@7&34*0/ , )0.& ,
1"5) , and 64&3 . You might wonder, as we didn't declare these variables, where did they
come from?
The What and Why of Scripting with Bash Chapter 1
[ 24 ]These variables are defined by the shell for your use and they are called environment
variables.
There are many environment variables. If you want to list them, you can use the QSJOUFOW
command.
Also, you can print a specific environment variable by specifying it to the QSJOUFOW
command:
$ printenv HOME
We can use any of these variables in our bash scripts.
Note that all environment variables are written in capital letters, so you can declare your
variables as lower case to make it easy to differentiate your variables from environment
variables. This is not required, but is preferable.
Variable scope
Once you have declared your variable, it will be available for use in your entire bash script
without any problems.
Let's assume this scenario: you have divided your code into two files and you will execute
one of them from inside the other, like this:
5IFGJSTUTDSJQU
CJOCBTI
OBNF.PLIUBS
TDSJQUTI5IJTXJMMSVOUIFTFDPOETDSJQU
The second script looks like this:
5IFTDSJQUTITDSJQU
CJOCBTI
FDIPOBNF
Suppose that you want to use the OBNF  variable in the second script. If you try to print it,
nothing will show up; this is because a variable's scope is only limited to the process that
creates it.
To use the OBNF  variable, you can export it using the FYQPSU  command.
The What and Why of Scripting with Bash Chapter 1
[ 25 ]So, our code will be like this:
5IFGJSTUTDSJQU
CJOCBTI
OBNF.PLIUBS
FYQPSUOBNF5IFWBSJBCMFXJMMCFBDDFTTJCMFUPPUIFSQSPDFTTFT
TDSJQUTI
Now if you run the first script, it will print the name that came from the first script file.
Keep in mind that the second process or TDSJQUTI  only makes a copy of the variable and
never touches the original one.
To prove this, try to change that variable from the second script and try to access that
variable value from the first script:
5IFGJSTUTDSJQU
CJOCBTI
OBNF.PLIUBS
FYQPSUOBNF
TDSJQUTI
FDIPOBNF
The second script will be like this:
5IFGJSTUTDSJQU
CJOCBTI
OBNF"OPUIFSOBNF
FDIPOBNF
If you run the first script, it will print the modified OBNF  from the second script and then it
will print the original OBNF  from the first script. So, the original variable remains as it is.
Command substitution
So far, we have seen how to declare variables. These variables can hold integers, strings,
arrays, or floats, as we have seen, but this is not everything.
A command substitution means storing the output of a command execution in a variable.
The What and Why of Scripting with Bash Chapter 1
[ 26 ]As you might know, the QXE command prints the current working directory. So, we will see
how to store its value in a variable.
There are two ways to perform a command substitution:
Using the backtick character ( 	)
Using the dollar sign format, like this: 

Using the first method, we just surround the command between two backticks:
CJOCBTI
DVS@EJS	QXE	
FDIPDVS@EJS
And the second way is written as follows:
CJOCBTI
DVS@EJS
QXE
FDIPDVS@EJS
The output coming from commands can be further processed and actions can be made
based on that output.
Debugging your scripts
With the scripts as simple as we have seen so far, there is little that can go wrong or need
debugging. As the script grows and decision paths are included with conditional
statements, we may need to use some level of debugging to analyze the scripts' progress
better.
Bash provides two options for us, W and Y.
If we want to look at the verbose output from our script and the detailed information about
the way the script is evaluated line by line, we can use the W option. This can be within the
shebang, but it is often easier to run the script directly with bash:
$ bash -v $HOME/bin/hello2.sh fred
The What and Why of Scripting with Bash Chapter 1
[ 27 ]This is especially useful in this example as we can see how each element of the embedded
CBTFOBNF  command is processed. The first step is removing the quotes and then the
parentheses. Take a look at the following output:
The Y option, which displays the commands as they are executed, is more commonly used.
It's useful to know the decision branch that has been chosen by the script. The following
shows this in action:
$ bash -x $HOME/bin/hello2.sh fred
We again see that the CBTFOBNF  is evaluated first, but we do not see the more detailed steps
involved in running that command. The screenshot that follows captures the command and
output:

The What and Why of Scripting with Bash Chapter 1
[ 28 ]The previous method might be hard for beginners or people who have a programming
background in which they debugged their code visually.
Another modern way of debugging shell scripts is by using Visual Studio Code.
There is a plugin called bash debug  that enables you to debug bash scripts visually, as is
the case for any other programming language.
You can step into, step over, add watches, and do all the other usual debugging stuff you
know.
After installing the plugin, from the File menu, open your TIFMMTDSJQUT  folder. Then
you can configure the debugging process by pressing Ctrl + Shift  + P and typing the
following:
%FCVHPQFOMBVODIKTPO
This will open an empty file; type in the following configurations:
\
WFSTJPO
DPOGJHVSBUJPOT<
\
OBNF1BDLU#BTI%FCVH
UZQFCBTIEC
SFRVFTUMBVODI
TDSJQU1BUI\DPNNBOE4FMFDU4DSJQU/BNF^
DPNNBOE-JOF"SHVNFOUT
MJOVY\
CBTI1BUICBTI
^
PTY\
CBTI1BUICBTI
^
^
>
^
The What and Why of Scripting with Bash Chapter 1
[ 29 ]This will create a debug configuration named 1BDLU#BTI%FCVH :

The What and Why of Scripting with Bash Chapter 1
[ 30 ]Now insert a breakpoint and press F5, or start debugging from the Debug  menu; it will
show you the list of TI files:

The What and Why of Scripting with Bash Chapter 1
[ 31 ]Select the one you want to debug, and set a breakpoint on any line to test it, as shown in the
following screenshot:

The What and Why of Scripting with Bash Chapter 1
[ 32 ]You can add watches to watch variables' values while stepping over your lines of code:
Note that your script MUST  start with the bash shebang, CJOCBTI .
Now you can enjoy the visual method of debugging. Happy coding!
The What and Why of Scripting with Bash Chapter 1
[ 33 ]Summary
This marks the end of this chapter and you have no doubt found this useful. Especially for
those making a start with bash scripting, this chapter will have established a firm
foundation on which you can build your knowledge.
We began by ensuring that bash is secure and not susceptible to shell-shock from embedded
functions. With bash secured, we considered the execution hierarchy where aliases,
functions, and so on are checked before the command; knowing this can help us plan a
good naming structure and a path to locate the scripts.
Then we went on to illustrate the types of Linux shells and we found out what bash
scripting is.
Soon, we were writing simple scripts with static content, but we saw how easy it was to add
flexibility using arguments. The exit code from the script can be read with the  variable
and we can create a command-line list using ]] and , which depends on the success or
failure of the preceding command in the list.
Then we saw how to declare variables and how to use environment variables. We identified
the variables' scope and saw how to export them to another process.
Also, we saw how to store commands' output in variables, which is called command
substitution.
Finally, we closed the chapter by looking at debugging the script using bash options and VS
Code. It's not really required when the script is trivial, but it will be useful later when
complexity is added.
In the next chapter, we will create interactive scripts that read the user's input during script
execution.
The What and Why of Scripting with Bash Chapter 1
[ 34 ]Questions
What is the problem with the following code? And how do we fix it?1.
CJOCBTI
WBS8FMDPNFUPCBTITDSJQUJOH
FDIPWBS
What is the result of the following code?2.
CJOCBTI
BSS
4BUVSEBZ4VOEBZ.POEBZ5VFTEBZ8FEOFTEBZ
FDIP\BSS<>^
What is the problem with the following code? And how do we fix it?3.
CJOCBTI
GJMFT	MTMB	
FDIPGJMFT
What is the value of the b and c variables in the following code?4.
CJOCBTI
B
C
DB
CD
Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH)0850#BTI1SPH*OUSP)0850IUNM
IUUQUMEQPSH-%1BCTIUNMWBSBTTJHONFOUIUNM
IUUQUMEQPSH-%1BCTIUNMEFDMBSFSFGIUNM
22
Creating Interactive Scripts
In $IBQUFS , The What and Why of Scripting with Bash , we learned how to create a script and
use some of its basic elements. These included optional parameters that we can pass
through to the script when it is executed. In this chapter, we will extend this by using
the shell's built-in SFBE  command to allow for interactive scripts. Interactive scripts are
scripts that prompt for information during the script's execution.
In this chapter, we will cover the following topics:
Using FDIP  with options
Basic script using SFBE
Script comments
Enhancing read scripts with SFBE  prompts
Limiting the number of entered characters
Controlling the visibility of the entered text
Passing options
Read options values
Try to be standard
Enhancing learning with simple scripts
Technical requirements
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Creating Interactive Scripts Chapter 2
[ 36 ]Using echo with options
So far, in this book we have been able to see that the FDIP  command is very useful and is
going to be used in many of our scripts, if not all of them. When running the FDIP
command, the built-in command will be used unless we state the full path to the file. We
can test this with the following command:
$ which echo
To gain help on the built-in command, we can use man bash and search for FDIP ; however,
the FDIP  command is identical to the internal command, so we recommend that you use
man echo in most cases in order to display the command options.
The basic use of FDIP  that we have seen so far will produce a text output and a new line.
This is often the desired response, so we don't need to be concerned that the next prompt
will append to the end of the echoed text. The new line separates the script output from the
next shell prompt. If we do not supply any text string to print, FDIP  will print only the new
line to 45%065 . We can test this with the following command, directly from the command
line. We do not need to run FDIP  or, in fact, any other command from a script. To run FDIP
from the command line, we simply enter the command as shown:
$ echo
The output will show a clear new line between the command we issued and the subsequent
prompt. We can see this in the following screenshot:
If we want to suppress the new line, which is especially useful if we are to prompt users, we
can do this in the following two ways, with the help of FDIP :
$ echo -n "Which directory do you want to use? "
$ echo -e "Which directory do you want to use? \c"
The result will be to suppress the line feed. In the initial example, the O option is used to
suppress the line feed. The second example uses the more generic F option, which allows
escape sequences to be added to the text string. To continue on the same line, we use =D as
the escape sequence.
Creating Interactive Scripts Chapter 2
[ 37 ]This does not look great as the final part of the script or when it is run from the command
line, as the command prompt will follow. This is illustrated in the following screenshot:
Basic script using read
When used as part of a script that prompts for user input, the suppression of the line feed is
exactly what we want. We will begin by copying the existing IFMMPTI  script to
IFMMPTI  and build an interactive script. Initially, we will use FDIP  as the prompt
mechanism, but, as we gradually enhance the script, we will generate the prompt directly
from the shell built-in SFBE  command:
$ cp $HOME/bin/hello2.sh $HOME/bin/hello3.sh
$ chmod +x $HOME/bin/hello3.sh
Edit the )0.&CJOIFMMPTI  script so that it reads as follows:
CJOCBTI
FDIPO)FMMP
CBTFOBNF.BZ*BTLZPVSOBNF
SFBE
FDIP)FMMP3&1-:
FYJU
As we execute the script, we will be greeted and prompted with whatever is typed. This is
echoed using the 3&1-:  variable in the echo statement. As we have not yet supplied a
variable name to the read built-in command, the default 3&1-:  variable is used. The script
execution and output are shown in the following screenshot. Take some time to practice the
script on your own system.

Creating Interactive Scripts Chapter 2
[ 38 ]This little step has taken us a long way and there are many uses for a script like this; we
have all used installation scripts that prompt for options and directories as we run through
the install. We accept that it is still quite trivial, but, as we delve into the chapter, we will get
closer to some more useful scripts.
Script comments
We should always introduce commenting scripts early in the piece. A script comment is
prefaced with a  symbol. Anything after the # symbol is a comment and is not evaluated
by the script. The shebang, CJOCBTI , is primarily a comment and, as such, is not
evaluated by the shell. The shell running the script reads the whole shebang, so it knows
which command interpreter to hand the script over to. A comment may be at the start of a
line or partway into the line. Shell scripting does not have the notion of multi-line
comments.
If you are not already familiar with comments, then please note that they are added to the
script to describe who wrote the script, when it was written and last updated, and what the
script does. They are the metadata of the script.
The following is an example of comments in scripts:
CJOCBTI
8FMDPNFUPCBTITDSJQUJOH
"VUIPS.PLIUBS
%BUF
It is good practice to comment, and add comments that explain what the code is doing and
why. This will help you and your colleagues who need to edit the script at a later date.
Enhancing scripts with read prompts
We have seen how we can use the built-in read to populate a variable. So far, we have used
echo to produce the prompt, but this can be passed to read itself using the Q option. The
SFBE  command will surpass the additional linefeed, so we reduce both the line count and
the complexity to some degree.
We can test this at the command line itself. Try typing the following command to see SFBE
in action:
$ read -p "Enter your name: " name
Creating Interactive Scripts Chapter 2
[ 39 ]We use the SFBE  command with the Q option. The argument that follows the option is the
text that appears in the prompt. Normally, we would make sure that there is a trailing space
at the end of the text to ensure that we can clearly see what we type. The last argument
supplied here is the variable we want to populate; we simply call it name. Variables are
case-sensitive too. Even if we do not supply the last argument, we can still store the user's
response, but this time in the 3&1-:  variable.
When we return the value of a variable, we use , but not when we write
it. In simple terms, when reading a variable we refer to 7"3  and when
setting a variable we refer to 7"3WBMVF .
The SFBE  command with syntax using the Q option is shown as follows:
SFBEQQSPNQU WBSJBCMFOBNF 
We can edit the script so that it appears similar to the following extract from IFMMPTI :
CJOCBTI
SFBEQ.BZ*BTLZPVSOBNFOBNF
FDIP)FMMPOBNF
FYJU
The SFBE  prompt cannot evaluate commands within the message string, such as those we
used before.
Limiting the number of entered characters
We have not needed this functionality in the scripts we have used so far, but we may need
to ask users to hit any key to continue. At the moment, we have set it up in such a way that
the variable is not populated until we hit the Enter  key. Users have to hit Enter  to continue.
If we use the O option followed by an integer, we can specify the number of characters to
accept before continuing; we will set  in this case. Take a look at the following code extract:
CJOCBTI
SFBEQ.BZ*BTLZPVSOBNFOBNF
FDIP)FMMPOBNF
SFBEOQ1SFTTBOZLFZUPFYJU
FDIP
FYJU
Creating Interactive Scripts Chapter 2
[ 40 ]Now the script will pause after displaying the name until we press any key; we can literally
press any key before continuing, as we accept just 1 key stroke, whereas earlier we were
required to leave the default behavior in place, as we could not know how long an entered
name would be. We have to wait for the user to hit Enter .
We add an additional echo here to ensure that a new line is issued before
the script ends. This ensures that the shell prompt starts on a new line.
Controlling the visibility of the entered text
Even though we have limited the input to a single character, we do get to see the text on the
screen. In the same way, if we type the name, we get to see the entered text before we hit
Enter . In this case, it is just untidy, but if we are entering sensitive data, such as a PIN or a
password, we should hide the text. We can use the silent option, or T, to achieve this. A
simple edit in the script will set this in place:
CJOCBTI
SFBEQ.BZ*BTLZPVSOBNFOBNF
FDIP)FMMPOBNF
SFBETOQ1SFTTBOZLFZUPFYJU
FDIP
FYJU
Now, when we use a key to continue, it will not be displayed on the screen. We can see the
behavior of the script in the following screenshot:
Passing options
So far, we have seen in the first chapter how to read parameters from the user. Also, you
can pass options. So, what are options? And how are they different from parameters?
Options are characters with a single dash before them.
Creating Interactive Scripts Chapter 2
[ 41 ]Check out this example:
$ ./script1.sh -a
The B is an option. You can check from your script if the user entered this option; if so,
then your script can behave in some manner.
You can pass multiple options:
$ ./script1.sh -a -b -c
To print these options, you can use the , , and  variables:
CJOCBTI
FDIP
FDIP
FDIP
We should check these options, but, since we haven't discussed conditional statements yet,
we will keep it simple for now.
Options can be passed with a value, like this:
$ ./script1.sh -a -b 20 -c
Here the C option is passed with a value of .

Creating Interactive Scripts Chapter 2
[ 42 ]As you can see, the variable  , which is the passed value.
This might not be acceptable to you. You need C  and D .
We will use some conditional statements to get these options correct.
CJOCBTI
XIJMF<O>
EP
DBTFJO
BFDIPBPQUJPOVTFE
CFDIPCPQUJPOVTFE
DFDIPDPQUJPOVTFE
FDIP0QUJPOOPUBOPQUJPO
FTBD
TIJGU
EPOF
If you don't know about the while loop, it's not a problem; we will discuss conditional
statements in detail in the coming chapters.
The TIJGU  command shifts the options one step to the left.
So, if we have three options or parameters and we use the TIJGU  command:
 becomes 
 becomes 
 is dropped
It's like an action to move forward while iterating over the options using the while loop.
So, in the first loop cycle,  will be the first option. After shifting the options,  will be the
second option and so on.
If you try the previous code, you will notice that it still doesn't identify the values of options
correctly. Don't worry, the solution is coming; just wait a little longer.
Passing parameters with options
To pass parameters along with options simultaneously, you must separate them with a
double dash, like this:
$ ./script1.sh -a -b -c -- p1 p2 p3
Creating Interactive Scripts Chapter 2
[ 43 ]Using the previous technique, we can iterate over the options till we reach the double dash,
then we will iterate over the parameters:
CJOCBTI
XIJMF<O>
EP
DBTFJO
BFDIPBPQUJPOGPVOE
CFDIPCPQUJPOGPVOE
DFDIPDPQUJPOGPVOE
TIJGU
CSFBL
FDIP0QUJPOOPUBOPQUJPO
FTBD
TIJGU
EPOF
JUFSBUJPOPWFSPQUJPOTJTGJOJTIFEIFSF
JUFSBUJPOPWFSQBSBNFUFSTTUBSUFE
OVN
GPSQBSBNJO!
EP
FDIPOVNQBSBN
OVN

OVN
EPOF
Now if we run it with parameters and options combined, we should see a list of options and
another list of parameters:
$ ./script1.sh -a -b -c -- p1 p2 p3
As you can see, anything passed after the double dash is treated as a parameter.
Creating Interactive Scripts Chapter 2
[ 44 ]Read options values
We have seen how to identify options and parameters, but we still need a way to read the
options values correctly.
You may need to pass a value for a specific option. How can this value be read?
We will check for the  variable while the iteration goes through the options that we
expect a value for.
Check the following code:
CJOCBTI
XIJMF<O>
EP
DBTFJO
BFDIPBPQUJPOQBTTFE
CQBSBN
FDIPCPQUJPOQBTTFEXJUIWBMVFQBSBN
TIJGU
DFDIPDPQUJPOQBTTFE
TIJGU
CSFBL
FDIP0QUJPOOPUBOPQUJPO
FTBD
TIJGU
EPOF
OVN
GPSQBSBNJO!
EP
FDIPOVNQBSBN
OVN

OVN
EPOF

Creating Interactive Scripts Chapter 2
[ 45 ]This looks good now; your script identifies the options and the passed value for the second
option.
There is a built-in option for getting options from the users, which is using the HFUPQU
function.
Unfortunately, HFUPQU  doesn't support options with more than one character.
There is a non-built-in program called HFUPQU , which supports options larger than one
character, but, again, the macOS X version doesn't support long options.
Anyway, if you would like to read more about HFUPQU  usage, refer to the further reading
resources given after this chapter.
Try to be standard
You may use bash scripts from GitHub, and you may notice that there is a standard option
scheme that is followed. It's not required, but it is preferable.
These are some of the commonly used options:
B: List all items
D: Get a count of all items
E: Output directory
F: Expand items
G: Specify a file
I: Show the help page
J: Ignore the character case
M: List a text
P: Send output to a file
R: Keep silent; don't ask the user
S: Process something recursively
T: Use stealth mode
W: Use verbose mode
Y: Specify an executable
Z: Accept without prompting me
Creating Interactive Scripts Chapter 2
[ 46 ]Enhancing learning with simple scripts
Our scripts are still a little trivial, and we have not looked at conditional statements so we
can test for correct input, but let's take a look at some simple scripts that we can build with
some functionality.
Backing-up with scripts
Now that we have created some scripts, we may want to back these up to a different
location. If we create a script to prompt us, we can choose the location and the type of files
that we want to backup.
Consider the following script for your first practice. Create the script and name
it )0.&CBDLVQTI :
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
4DSJQUUPQSPNQUUPCBDLVQGJMFTBOEMPDBUJPO
5IFGJMFTXJMMCFTFBSDIPOGSPNUIFVTFS	TIPNF
EJSFDUPSZBOEDBOPOMZCFCBDLFEVQUPBEJSFDUPSZ
XJUIJO)0.&
-BTU&EJUFE+VMZ
SFBEQ8IJDIGJMFUZQFTEPZPVXBOUUPCBDLVQGJMF@TVGGJY
SFBEQ8IJDIEJSFDUPSZEPZPVXBOUUPCBDLVQUPEJS@OBNF
5IFOFYUMJOFTDSFBUFTUIFEJSFDUPSZJGJUEPFTOPUFYJTU
UFTUE)0.&EJS@OBNF]]NLEJSN)0.&EJS@OBNF
5IFGJOEDPNNBOEXJMMDPQZGJMFTUIFNBUDIUIF
TFBSDIDSJUFSJBJFTI5IFQBUIQSVOFBOEP
PQUJPOTBSFUPFYDMVEFUIFCBDLEJSFDUPSZGSPNUIF
CBDLVQ
GJOE)0.&QBUI)0.&EJS@OBNFQSVOFP=
OBNFGJMF@TVGGJYFYFDDQ\^)0.&EJS@OBNF=
FYJU
Creating Interactive Scripts Chapter 2
[ 47 ]You will see that the file is commented; though, in black and white, the readability is a little
difficult. If you have an electronic copy of this book, you should see the colors in the
following screenshot:
As the script runs, you may choose TI for the files to backup and backup as the directory.
The script execution is shown in the following screenshot, along with a listing of the
directory:
Now you can see that we can start to create meaningful scripts with trivial scripting;
although we strongly urge adding error checking of the user input if this script is for
something other than personal use. As we progress into the book, we will cover this.
Creating Interactive Scripts Chapter 2
[ 48 ]Connecting to a server
Let's look at some practical scripts that we can use to connect to servers. Firstly, we will
look at ping, and in the second script we will look at prompting for SSH credentials.
Version 1 ` ping
This is something we can all do, as no special services are required. This will simplify the
QJOH  command for console users who may not know the details of the command. This will
ping the server for just three counts rather than the normal infinite amount. There is no
output if the server is alive, but a failed server reports TFWFSEFBE . Create the following
script as )0.&CJOQJOH@TFSWFSTI :
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
4DSJQUUPQJOHBTFSWFS
-BTU&EJUFE+VMZ
SFBEQ8IJDITFSWFSTIPVMECFQJOHFETFSWFS@BEES
QJOHDTFSWFS@BEES  EFWOVMM]]FDIP4FSWFS%FBE
The following screenshot shows successful and failed outputs:
Version 2 ` SSH
Often SSH is installed and running on servers, so you may be able to run this script if your
system is running SSH or you have access to an SSH server. In this script, we prompt for the
server address and username, and pass them through to the SSH client. Create the
following script as )0.&CJODPOOFDU@TFSWFSTI :
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
Creating Interactive Scripts Chapter 2
[ 49 ]4DSJQUUPQSPNQUGPTTIDPOOFDUJPO
-BTU&EJUFE+VMZ
SFBEQ8IJDITFSWFSEPZPVXBOUUPDPOOFDUUPTFSWFS@OBNF
SFBEQ8IJDIVTFSOBNFEPZPVXBOUUPVTFVTFS@OBNF
TTI\VTFS@OBNF^!TFSWFS@OBNF
Use of the brace bracket is to delimit the variable from the ! symbol in the
last line of the script.
Version 3 ` MySQL/MariaDB
In the next script, we will provide the detail for a database connection along with the SQL
query to execute. You will be able to run this if you have a MariaDB or MySQL database
server on your system, or one that you can connect to. For this demonstration, we will
use Linux Mint 18.3 and MariaDB version 10; however, this should work for any MySQL
server or MariaDB, from version 5 onwards. The script collects user and password
information as well as the SQL command to execute. Create the following script as
)0.&CJOSVO@NZTRMTI :
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
4DSJQUUPQSPNQUGPS.:42-VTFSQBTTXPSEBOEDPNNBOE
-BTU&EJUFE+VMZ
SFBEQ.Z42-6TFSVTFS@OBNF
SFBETQ.Z42-1BTTXPSENZTRM@QXE
FDIP
SFBEQ.Z42-$PNNBOENZTRM@DNE
SFBEQ.Z42-%BUBCBTFNZTRM@EC
NZTRMVVTFS@OBNFQNZTRM@QXENZTRM@EC#FNZTRM@DNE
In this script, we can see that we suppress the display of the MySQL password when we
input it into the SFBE  command using the T option. Again, we use FDIP  directly to ensure
that the next prompt starts on a new line.
Creating Interactive Scripts Chapter 2
[ 50 ]The script input is shown in the following screenshot:
Now we can easily see the password suppression working and the ease of adding to the
MySQL commands.
Reading files
The SFBE  command is not only used to read inputs from the user; you can use the SFBE
command to read files for further processing.
CJOCBTI
XIJMFSFBEMJOF
EP
FDIPMJOF
EPOFZPVSGJMFUYU
We redirect the file content to the XIJMF  command to read the content using the SFBE
command, line by line.
Finally, we print the line using the FDIP  command.
Creating Interactive Scripts Chapter 2
[ 51 ]Summary
Feel proud that you now have your I can read  badge for shell scripting. We have developed
our scripts to be interactive and to prompt users for input during the script execution. These
prompts can be used to simplify user operations on the command line. In this way, they do
not need to remember the command-line options or have passwords that end up stored in
the command-line history. When using passwords, we can simply store the value using the
read TQ options.
Also, we saw how to pass options with and without values, and how to identify values
correctly. We saw how to pass options and parameters at the same time, thanks to the
double dash.
In the next chapter, we will take our time to look at the conditional statements in bash.
Questions
How many comments are in the following code?1.
CJOCBTI
8FMDPNFUPTIFMMTDSJQUJOH
"VUIPS.PLIUBS
If we have the following code:2.
CJOCBTI
FDIP
FDIP
FDIP
And we run the script with these options:
$ ./script1.sh -a -b50 -c
What is the result of running this code?
Creating Interactive Scripts Chapter 2
[ 52 ]Check the following code:3.
CJOCBTI
TIJGU
FDIP
If we run it with these options:
$ ./script1.sh Mokhtar -n -a 35 -p
What is the result?1.
What is the dropped parameter?2.
Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMTFDU@@IUNM
IUUQTTTDPNCBTISFBEIUNM
IUUQXXXNBOQBHF[DPNNBOHFUPQU
IUUQTTTDPNCBTIHFUPQUTIUNM
33
Conditions Attached
Now you can make your scripts more interactive using the SFBE  command, and you know
how to read parameters and options to lighten your inputs.
We can say that we are now into the fine print of the script. These are the details that are
written into our scripts using conditions to test if a statement should run or not. We are now
ready to add some intelligence into our scripts, so our scripts become more robust, easier to
use, and more reliable. Conditional statements can be written with simple command-line
lists of "/% or 03 commands together, or, more often, within traditional JG statements.
In this chapter, we will cover the following topics:
Simple decision paths using command-line lists
Verifying user input with lists
Using the test shell built-in
Creating conditional statements using JG
Extending JG with FMTF
Using the UFTU  command with the JG command
More conditions with FMJG
Using case statements
Recipe-frontend with HSFQ
Conditions Attached Chapter 3
[ 54 ]Technical requirements
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Simple decision paths using command-line
lists
We have used command-line lists ( ]] and ), both in $IBQUFS , The What and Why of
Scripting with Bash , and in some of the scripts found in $IBQUFS , Creating Interactive
Scripts . Lists are one of the simplest conditional statements that we can create, and so we 
thought that it was appropriate to use them in the earlier examples before fully explaining
them here.
Command-line lists are two or more statements that are joined using either the "/% or 03
notations:
: "/%
]]: 03
Where the two statements are joined using the "/% notation, the second command only
runs if the first command succeeds. Whereas, with the 03 notation, the second command
will run only if the first command fails.
The decision on the success or failure of a command is taken by reading the exit code from
the application. A zero represents a successful application completion and anything other
than a zero represents a failure. We can test the success or failure of an application by
reading the exit status by means of the system variables . This is shown in the following
example:
$ echo $?
If we need to ensure that a script is run from a user's home directory, we can build this into
the script's logic. This can be tested from the command line, and it does not have to be in a
script. Consider the following command-line example:
$ test $PWD == $HOME || cd $HOME
Conditions Attached Chapter 3
[ 55 ]The double vertical bars denote an 03 Boolean. This ensures that the second statement is
only executed when the first statement is not true. In simple terms, if we are not currently in
the home directory, we will be by the end of the command-line list. We will see more on the
UFTU  command soon.
We can build this into almost any command that we want and not just test. For example, we
can query to see if a user is logged into the system, and if they are, then we can use the
XSJUF  command to directly message their console. Similar to before, we can test this in the
command line prior to adding it to the script. This is shown in the following command-line
example:
$ who | grep pi > /dev/null 2>&1 && write pi < message.txt
Note that you should change the user QJ to your username.
If we use this in a script, it is almost certain that we will replace the username with a
variable. In general, if we need to refer to the same value more than once, then using a
variable is a good idea. In this case, we are searching for the QJ user.
When we break the command-line list down, we first use the XIP command to list the users
who are logged on. We pipe the list to HSFQ  to search for the desired username. We are not 
interested in the output from the search, just its success or failure. Bearing this in mind, we
redirect all our output to EFWOVMM . The double ampersand indicates that the second
statement in the list runs only if the first returns true. If the QJ user is logged on, we use
XSJUF  to message the user. The following screenshot illustrates this command and the
output:

Conditions Attached Chapter 3
[ 56 ]Verifying user input with lists
In this script, we will ensure that a value has been supplied to the first positional parameter.
We can modify the IFMMPTI  script that we created in $IBQUFS , The What and Why of
Scripting with Bash , to check for user input before displaying the IFMMP  text.
You can copy the IFMMPTI  script to IFMMPTI , or simply create a new script from
scratch. There will not be a lot of typing and the script will be created as
)0.&CJOIFMMPTI , as shown:
We can ensure that the script is executable by using the following command:
$ chmod +x $HOME/bin/hello4.sh
We can then run the script with or without arguments. The UFTU  statement is looking for
the  variable to be zero bytes. If it is, then we will not see the IFMMP  statement; otherwise,
it will print the IFMMP  message. In simple terms, we will see the IFMMP  message if we
supply a name.
The following screenshot shows the output that you will see when you do not supply a
parameter to the script, followed by the supplied parameter:
Using the test shell built-in
It is probably time for us to pull over to the side of the scripting highway and look a little
more at the command UFTU . This is both a shell built-in and a file executable in its own
right. Of course, we will have to hit the built-in command first, unless we specify the full
path to the file.
Conditions Attached Chapter 3
[ 57 ]When the UFTU  command is run without any expressions to evaluate, then the test will
return false. So, if we run the UFTU  as shown in the following command, the exit status will
be , even though no error output is shown:
$ test
The UFTU  command will always return either 5SVF  or 'BMTF , or  or , respectively. The
basic syntax of UFTU  is as follows:
test EXPRESSION
Or, we can invert the UFTU  command with this:
test ! EXPRESSION
If we need to include multiple expressions, this can be done using "/% or 03 together, using
the B and P options, respectively:
test EXPRESSION -a EXPRESSION
test EXPRESSION -o EXPRESSION
We can also write this as a shorthand version, replacing the UFTU  with square brackets to
surround the expression, as shown in the following example:
[ EXPRESSION ]
Testing strings
We can test for the equality or inequality of two strings. For example, one of the ways to test
the root user is using the following command:
test $USER = root
We could also write this using the square bracket notation:
[ $USER = root ]
Note that you must put a space between each bracket and the inner testing condition as
previously shown.
Equally, we could test for a non-root account with the following two methods:
test ! $USER = root
[ ! $USER = root ]
Conditions Attached Chapter 3
[ 58 ]We can also test for the zero values or non-zero values of strings. We saw this in an earlier
example in this chapter.
To test if a string has a value, we can use the O option. We can check to see if the current
connection is made through SSH by checking for the existence of a variable in the user's
environment. We do this by using UFTU  and square brackets in the following two examples:
test -n $SSH_TTY
[ -n $SSH_TTY ]
If this is true, then the connection is made with SSH; if it is false, then the connection is not
through SSH.
As we saw earlier, testing for a zero string value is useful when deciding if a variable is set:
test -z $1
Or, more simply, we could use the following:
[ -z $1 ]
A true result for this query means that no input parameters have been supplied to the
script.
Testing integers
As well as testing string values of bash scripts, we can test for integer values and whole
numbers. Another way of testing the input of a script is to count the numbers of positional 
parameters and also test if the number is above :
UFTUHU
Or using the brackets, as follows:
<HU>
When in a relationship, the top positional parameters of the  variable represent the
number of parameters passed to the script.
There are many tests that can be done for numbers:
OVNCFSFROVNCFS : This checks if OVNCFS  is equal to OVNCFS
OVNCFSHFOVNCFS : This checks if OVNCFS  is greater than or equal to
OVNCFS .
Conditions Attached Chapter 3
[ 59 ]OVNCFSHUOVNCFS : This checks if OVNCFS  is greater than OVNCFS
OVNCFSMFOVNCFS : This checks if OVNCFS  is smaller than or equal to
OVNCFS
OVNCFSMUOVNCFS : This checks if OVNCFS  is smaller than OVNCFS
OVNCFSOFOVNCFS : This checks if OVNCFS  is not equal to OVNCFS
Testing file types
While testing for values, we can test for the existence of a file or file type. For example, we
may only want to delete a file if it is a symbolic link. We use this while compiling a kernel.
The VTSTSDMJOVY  directory should be a symbolic link to the latest kernel source code.
If we download a newer version before compiling the new kernel, we need to delete the
existing link and create a new link. Just in case someone has created the VTSTSDMJOVY
directory, we can test if it has a link before removing it:
# [ -h /usr/src/linux ] &&rm /usr/src/linux
The I option tests that the file has a link. Other options include the following:
E: This shows that it's a directory
F: This shows that the file exists in any form
Y: This shows that the file is executable
G: This shows that the file is a regular file
S: This shows that the file is readable
Q: This shows that the file is a named pipe
C: This shows that the file is a block device
GJMFOUGJMF : This checks if GJMF  is newer than GJMF
GJMFPUGJMF : This checks if GJMF  is older than GJMF
0GJMF : This checks if the logged-in user is the owner of the file
D: This shows that the file is a character device
More options do exist, so delve into the main pages as you need to. We will use different
options throughout the book, and thus giving you practical and useful examples.
Conditions Attached Chapter 3
[ 60 ]Creating conditional statements using if
As we have seen so far, it is possible to build simple conditions using command-line lists.
These conditional statements can be written both with and without a test. As the complexity
of the tasks increases, it becomes easier to create statements using JG. This will certainly
ease both the readability of the script and the logic layout. To a degree, it also matches the
way in which we think and speak; JG is a semantic in our spoken language in the same way
it is within the bash script.
Even though it will take up more than a single line in the script, with an JG statement we
can achieve more and make the script more legible. That being said, let's look at creating JG
conditions. The following is an example of a script using an JG statement:
CJOCBTI
8FMDPNFTDSJQUUPEJTQMBZBNFTTBHFUPVTFSTPOMPHJO
"VUIPS!UIFVSCBOQFOHVJO
%BUF
JG<MU>UIFO
FDIP6TBHFOBNF 
FYJU
GJ
FDIP)FMMP
FYJU
The code within the JG statement will run only when the condition evaluates to true, and
the end of the JG block is denoted with GJ - JG backward. The color coding in WJN can be
useful to aid readability, which you will see in the following screenshot:
Within the script, we can easily add in multiple statements to run when the condition is
USVF . In our case, this includes exiting the script with an error indicated, as well as
including the VTBHF  statement to assist the user. This ensures that we only display the
IFMMP  message if we have supplied a name of the person to be welcomed.
Conditions Attached Chapter 3
[ 61 ]We can view the script execution both with and without the argument in the following
screenshot:
The following pseudocode shows the syntax of the JG conditional statement:
JGDPOEJUJPOUIFO
TUBUFNFOU
TUBUFNFOU
GJ
Indenting the code is not required, but it helps readability and is highly recommended.
Adding the UIFO  statement to the same line as the JG statement, again assists with the
readability of the code, and the semicolon is required to separate the JG from the UIFO .
Extending if with else
When a script is required to continue regardless of the result of the JG condition, it is often
necessary to deal with both conditions of the evaluation, what to do when it is USVF  as well
as GBMTF . This is where we can make use of the FMTF  keyword. This allows the execution of
one block of code when the condition is true and another when the condition is evaluated as
false. The pseudocode for this is shown as follows:
JGDPOEJUJPOUIFO
TUBUFNFOU
FMTF
TUBUFNFOU
GJ
If we consider extending the IFMMPTI  script that we created earlier, it is easily possible to
allow for the correct execution, regardless of the parameter being present or not. We can
recreate this as IFMMPTI , as follows:
CJOCBTI
8FMDPNFTDSJQUUPEJTQMBZBNFTTBHFUPVTFST
"VUIPS!UIFVSCBOQFOHVJO
%BUF
JG<MU>UIFO
Conditions Attached Chapter 3
[ 62 ]SFBEQ&OUFSBOBNF
OBNF3&1-:
FMTF
OBNF
GJ
FDIP)FMMPOBNF
FYJU
The script now sets a named variable, which helps readability, and we can assign the
correct value to OBNF  from the input parameter or from the SFBE  prompt; either way the
script is working well and starting to take shape.
Test command with the if command
You have seen how to use the UFTU  command or the short version <>. This test returns
zero (true) or non-zero (false).
You will see how to check the returned result using the JG command.
Checking strings
You can use the JG command with the UFTU  command to check if the string matches a
specific criterion:
JG<TUSJOHTUSJOH> : This checks if TUSJOH  is identical to TUSJOH
JG<TUSJOHTUSJOH> : This checks if TUSJOH  is not identical to
TUSJOH
JG<TUSJOH=TUSJOH> : This checks if TUSJOH  is less than TUSJOH
JG<TUSJOH= TUSJOH> : This checks if TUSJOH  is greater than TUSJOH
The less than and greater than should be escaped with a backslash as if it shows a warning.
JG<OTUSJOH> : This checks if TUSJOH  is longer than zero
JG<[TUSJOH> : This checks if TUSJOH  has zero length
Let's see some examples to explain how JG statements work:
CJOCBTI
JG<NPLIUBS.PLIUBS>
UIFO
FDIP4USJOHTBSFJEFOUJDBM
Conditions Attached Chapter 3
[ 63 ]FMTF
FDIP4USJOHTBSFOPUJEFOUJDBM
GJ
This JG statement checks if strings are identical or not; since the strings are not identical,
because one of them has a capital letter, they are identified as not identical.
Note the space between the square brackets and the variables; without this
space it will show a warning in some cases.
The not-equal operator ( ) works the same way. Also, you can negate the JG statement
and it will work the same way, like this:
JG<NPLIUBS.PLIUBS>
The less-than and greater-than operators check if the first string is greater than or less than
the second string from the ASCII-order perspective:
CJOCBTI
JG<NPLIUBS= .PLIUBS>
UIFO
FDIP4USJOHJTHSFBUFSUIBOTUSJOH
FMTF
FDIP4USJOHJTMFTTUIBOUIFTUSJOH
GJ
In the ASCII order, the lower-case characters are higher than the upper case.
Conditions Attached Chapter 3
[ 64 ]Don't get confused if you use the TPSU  command to sort a file or similar, and find that the
sorting order works the opposite way to the UFTU  command. This is because the TPSU
command uses the numbering order from the system settings, which is the opposite to the
ASCII order.
To check the string length, you can use the O test:
CJOCBTI
JG<ONPLIUBS>
UIFO
FDIP4USJOHMFOHUIJTHSFBUFSUIBO[FSP
FMTF
FDIP4USJOHJT[FSPMFOHUI
GJ
To check for a length of zero, you can use the [ test:
CJOCBTI
JG<[NPLIUBS>
UIFO
FDIP4USJOHMFOHUIJT[FSP
FMTF
FDIP4USJOHMFOHUIJTOPU[FSP
GJ
We have used quotes around the tested strings, even though our string has no spaces.
In case you have a string with spaces, you MUST  use quotes.
Conditions Attached Chapter 3
[ 65 ]Checking files and directories
Similarly, you can check files and directories using the JG statement.
Let's look at an example:
CJOCBTI
NZEJSIPNFNZEJS
JG<ENZEJS>
UIFO
FDIP%JSFDUPSZNZEJSFYJTUT
FMTF
FDIP%JSFDUPSZNZEJSOPUGPVOE
GJ
We used the E test to check if the path is a directory.
The rest of the tests work the same way.
Checking numbers
Also, in the same way, we can check numbers using the UFTU  and the JG commands.
CJOCBTI
JG<HU>
UIFO
FDIPOVNCFSJTHSFBUFSUIBOOVNCFS
FMTF
FDIPOVNCFSJTMFTTUIBOOVNCFS
GJ
As expected,  is greater than .
All other numeric tests work the same way.
Conditions Attached Chapter 3
[ 66 ]Combining tests
You can combine multiple tests and check them using one JG statement.
This is done using the "/% () and 03 (]]) commands:
CJOCBTI
NZEJSIPNFNZEJS
OBNFNPLIUBS
JG<ENZEJS><OOBNF>UIFO
FDIP5IFOBNFJTOPU[FSPMFOHUIBOEUIFEJSFDUPSZFYJTUT
FMTF
FDIP0OFPGUIFUFTUTGBJMFE
GJ
The JG statement performs two checks, it checks if the directory exists and that the name is
not of zero length.
The two tests must return success (zero) to evaluate the next FDIP  command.
If one of them fails, the JG statement goes to the FMTF  clause.
Unlike the 03 (]]) command, if any of the tests returns success (zero), the JG statement
succeeds.
CJOCBTI
NZEJSIPNFNZEJS
OBNFNPLIUBS
JG<ENZEJS>]]<OOBNF>UIFO
FDIP0OFPGUFTUTPSCPUITVDDFTTFT
FMTF
FDIP#PUIGBJMFE
GJ
Conditions Attached Chapter 3
[ 67 ]
It is clear enough that if one of the tests returns true, the JG statement returns true for the
combined tests.
More conditions with elif
Moving on to where we require a greater degree of control, we can use the FMJG  keyword.
Unlike FMTF , FMJG  requires an additional condition to be tested for each FMJG . In this way,
we can provide for different circumstances. We can add in as many FMJG  conditions as
required. The following shows some pseudocode:
JGDPOEJUJPOUIFO
TUBUFNFOU
FMJGDPOEJUJPOUIFO
TUBUFNFOU
FMTF
TUBUFNFOU
GJ
FYJU
A script may make life easier for the operator by providing a simplified selection for a more
complex piece of code. Even though the script becomes gradually more complex to meet the
requirements, to the operator the execution is greatly simplified. It is our job to enable users
to run more complex operations easily from the command line when creating scripts. Often,
this will necessitate the addition of more complexity to our scripts; however, we will be
rewarded with the reliability of the scripted application.
Creating the backup2.sh using elif
We can revisit the script that we created to run the earlier backup. This script,
)0.&CJOCBDLVQTI , prompts the user for the file type and the directory in which to
store the backup. The tools used for the backup are GJOE  and DQ.
Conditions Attached Chapter 3
[ 68 ]With this new-found knowledge, we can now allow the script to run the backup using the
command UBS and the level of compression selected by the operator. There is no
requirement to select the file type, as the complete home directory will be backed up, with
the exclusion of the backup directory itself.
The operator can select the compression based on three letters: ), ., and -. The selection will
affect the options passed to the UBS command and the backup file created. The selection of
high uses C[JQ  compression, medium uses H[JQ  compression, and low creates an
uncompressed UBS archive. The logic exists in the extended JG statement that follows:
JG<GJMF@DPNQSFTTJPO->UIFO
UBS@PQUUBS@M
FMJG<GJMF@DPNQSFTTJPO.>UIFO
UBS@PQUUBS@N
FMTF
UBS@PQUUBS@I
GJ
Based on the user selection, we can configure the correct options for the UBS command. As
we have three conditions to evaluate, the JG, FMJG , and FMTF  statements are appropriate.
To see how the variables are configured we can look at the following extract from the script:
UBS@MDWGCBDLVQ@EJSCUBSFYDMVEFCBDLVQ@EJS)0.&
UBS@ND[WGCBDLVQ@EJSCUBSH[FYDMVEFCBDLVQ@EJS)0.&
UBS@IDKWGCBDLVQ@EJSCUBSC[JQFYDMVEFCBDLVQ@EJS)0.&
The complete script can be created as )0.&CJOCBDLVQTI  and should comprise the
following code:
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
SFBEQ$IPPTF).PS-DPNQSFTTJPOGJMF@DPNQSFTTJPO
SFBEQ8IJDIEJSFDUPSZEPZPVXBOUUPCBDLVQUPEJS@OBNF
5IFOFYUMJOFTDSFBUFTUIFEJSFDUPSZJGJUEPFTOPUFYJTU
UFTUE)0.&EJS@OBNF]]NLEJSN)0.&EJS@OBNF
CBDLVQ@EJS)0.&EJS@OBNF
UBS@MDWGCBDLVQ@EJSCUBSFYDMVEFCBDLVQ@EJS)0.&
UBS@ND[WGCBDLVQ@EJSCUBSH[FYDMVEFCBDLVQ@EJS)0.&
UBS@IDKWGCBDLVQ@EJSCUBSC[JQFYDMVEFCBDLVQ@EJS)0.&
JG<GJMF@DPNQSFTTJPO->UIFO
UBS@PQUUBS@M
FMJG<GJMF@DPNQSFTTJPO.>UIFO
UBS@PQUUBS@N
FMTF
UBS@PQUUBS@I
Conditions Attached Chapter 3
[ 69 ]GJ
UBSUBS@PQU
FYJU
When we execute the script, we need to select ), ., or - in upper case, as this is how the
selection is made within the script. The following screenshot shows the initial script
execution, where the selection for . has been made:
Using case statements
Rather than using multiple FMJG  statements, a DBTF  statement may provide a simpler
mechanism when evaluations are made on a single expression.
The basic layout of a DBTF  statement is listed as follows, using pseudocode:
DBTFFYQSFTTJPOJO
DBTF
TUBUFNFOU
TUBUFNFOU

DBTF
TUBUFNFOU
TUBUFNFOU


TUBUFNFOU

FTBD
The statement layout that we see is not dissimilar to the TXJUDI  statements that exist in
other languages. In bash, we can use the DBTF  statement to test for simple values, such as
strings or integers. Case statements can cater for a wide range of letters, such as <BG>  or B
through to G, but they cannot easily deal with integer ranges such as <> .
Conditions Attached Chapter 3
[ 70 ]The DBTF  statement will first expand the expression and then it will try to match it with
each item in turn. When a match is found, all the statements are executed until the . This
indicates the end of the code for that match. If there is no match, the case FMTF  statement
indicated by the  will be matched. This needs to be the last item in the list.
Consider the following script HSBEFTI , which is used to evaluate grades:
CJOCBTI
4DSJQUUPFWBMVBUFHSBEFT
6TBHFHSBEFTITUEVFOUHSBEF
"VUIPS!MJLFHFFLT
%BUF
JG<FR>UIFO
FDIP:PVNVTUQSPWJEFTUVEFOU HSBEF 
FYJU
GJ
DBTF\??^JO1BSBNFUFSFYQBOTJPOJTVTFEUPDBQJUBMJ[FJOQVU
<"$>FDIPJTBTUBSQVQJM

<%>FDIPOFFETUPUSZBMJUUMFIBSEFS

<&'>FDIPDPVMEEPBMPUCFUUFSOFYUZFBS

FDIP(SBEFDPVMEOPUCFFWBMVBUFEGPS

FTBD
The script first uses an JG statement to check that exactly two arguments have been
supplied to the script. If they are not supplied, the script will exit with an error state:
JG<FR>UIFO
FDIP:PVNVTUQSPWJEFTUVEFOU HSBEF 
FYJU
GJ
Then we use parameter expansion for the value of the  variable to capitalize the input
using ??. This represents the grade that we supply. Since we are capitalizing the input, we
first try to match against the letters " through to $.
We make similar tests for the other supplied grades, & through to '.
Conditions Attached Chapter 3
[ 71 ]The following screenshot shows the script execution with different grades:
Recipe ` building a frontend with grep
As a finale to this chapter, we will group a few features that we have learned together and
build a script that prompts the operator for a filename, a search string, and an operation to
carry out with the HSFQ  command. We will create the script as )0.&CJOTFBSDITI ,
and don't forget to make it executable:
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
VTBHF6TBHFTFBSDITIGJMFTUSJOHPQFSBUJPO
JG<FR>UIFO
FDIPVTBHF
FYJU
GJ
<G>FYJU
DBTFJO
<D$>
NFTH$PVOUJOHUIFNBUDIFTJOPG
PQUD

<Q1>
NFTH1SJOUUIFNBUDIFTPGJO
PQU

<E%>
NFTH1SJOUJOHBMMMJOFTCVUUIPTFNBUDIJOHGSPN
PQUW

FDIP$PVMEOPUFWBMVBUF
Conditions Attached Chapter 3
[ 72 ]FTBD
FDIPNFTH
HSFQPQU
We start by checking for exactly three input arguments using the following code:
JG<FR>UIFO
FDIPVTBHF
FYJU
GJ
The next check uses a command-line list to exit the script if the file argument is not a regular
file, using UFTUG :
<G>FYJU
The DBTF  statement allows for three operations:
Counting the matching lines
Printing the matching lines
Printing all but the matching lines
The following screenshot shows the search of the FUDOUQDPOG  file for lines beginning
with the string server. We choose the count option in this example:
Summary
One of the most important and time-consuming tasks in scripting is building all of the
conditional statements that we need to make the script usable and robust. There is an 80/20
rule that is often spoken of. This is where 20 percent of your time is spent in writing the
main script and 80 percent of the time is spent in ensuring that all of the possible
eventualities are correctly handled in the script. This is what we refer to as the procedural
integrity of the script, where we try to cover each scenario carefully and accurately.
We started by looking at a simple test with command-line lists. If the actions needed are
simple, then these provide great functionality and are easily added. Where more complexity
is required, we add JG statements.
Conditions Attached Chapter 3
[ 73 ]Using the JG statements, we can extend them as required using the FMTF  and FMJG
keywords. Don't forget that FMJG  keywords need their own conditions to evaluate.
We saw how to use JG statements with the UFTU  command, and check strings, files, and
numbers.
Finally, we saw how we can use DBTF  where a single expression needs to be evaluated.
In the next chapter, we will seek to understand the importance of reading in already
prepared code snippets. We will create a sample JG statement that can be saved as a code
snippet to be read into the script at the time of editing.
Questions
What is the result of the following code: 5SVF  or 'BMTF ? 1.
JG<-JLF(FFLT= MJLFHFFLT>
UIFO
FDIP5SVF
FMTF
FDIP'BMTF
GJ
Which one of the following scripts is correct?2.
CJOCBTI
JG<NPLIUBS.PLIUBS>
UIFO
FDIP4USJOHTBSFOPUJEFOUJDBM
FMTF
FDIP4USJOHTBSFJEFOUJDBM
GJ
Or
CJOCBTI
JG<NPLIUBS.PLIUBS>
UIFO
FDIP4USJOHTBSFOPUJEFOUJDBM
FMTF
FDIP4USJOHTBSFJEFOUJDBM
GJ
Conditions Attached Chapter 3
[ 74 ]How many commands can be used as an operator to return 5SVF  in the following 3.
example?
CJOCBTI
JG<>
UIFO
FDIP5SVF
FMTF
FDIP'BMTF
GJ
What is the result of the following code?4.
CJOCBTI
NZEJSIPNFNZEJS
OBNFNPLIUBS
JG<ENZEJS>]]<OOBNF>UIFO
FDIP5SVF
FMTF
FDIP'BMTF
GJ
Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH)0850#BTI1SPH*OUSP)0850IUNM
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMTFDU@@IUNM
IUUQXJLJCBTIIBDLFSTPSHDPNNBOETDMBTTJDUFTU
44
Creating Code Snippets
Now we can write our conditional tests to make decisions. After your hands become faster
in coding, you will need to save some code pieces for later use, so how to save time and
effort when writing scripts?
If you like using the command line, but also like some of the features associated with using
graphical integrated development environments  (IDEs ), then this chapter may reveal some
new ideas to you. We can create shortcuts for commonly used script elements using the vi
or vim text editors from the command line.
In this chapter, we will cover the following topics:
Abbreviations
Using code snippets
Creating snippets using VS Code
Technical requirements
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Creating Code Snippets Chapter 4
[ 76 ]Abbreviations
We have already taken one short sojourn into the _WJNSD  file and we will now revisit this
file to look at abbreviations or BCCS  controls. This file acts as the run control mechanism for
the vim text editor, which is likely to be installed on your Linux distribution. Older
distributions or Unix variants may have the original vi text editor and will make use of the
_FYSD  file. If you are uncertain of the identity of your version of vi and the correct run
control file to use, simply enter the WJ command. If a blank page opens, it is indeed vi.
However, if a new blank document opens with the vim splash screens, then you are using
the improved vim or vi.
Abbreviations allow for a shortcut string to be used in place of a longer string. These
abbreviations can be set during a vim session from the last line mode but are often set in the
control file. The shebang can be easily represented by an abbreviation, as follows:
abbr _sh #!/bin/bash
The basic syntax of an abbreviation is shown in the following command:
abbr <shortcut><string>
Using this abbreviation, we just need to type @TI while in the edit mode. On pressing the
Enter  key after the shortcut code, the full text for the shebang is printed. In reality, pressing
any key after the BCCS  code will expand the shortcut, not just pressing the Enter  key. Simple
elements like this can add a lot to the experience of using vim as our text editor. The
following screenshot shows the updated _WJNSD  file:
We are not limited to the single abbreviation code, as we can add more BCCS  entries, for
example, to support the shebang for Perl scripts at the line:
abbr _pl #!/usr/bin/perl
Creating Code Snippets Chapter 4
[ 77 ]The use of the underscore is not required, but the aim is to keep the shortcut code unique
and not to have a typed error. We are also not limited to a single line, although this is where
abbreviations are most used. Consider the following abbreviation for an JG statement:
abbr _if if [-z $1];then<CR>echo "> $0 <name><CR>exit 2<CR>fi
Although this does work, the formatting of the JG statement will not be perfect and
multiline abbreviations are far from ideal. This is where we may consider using code
snippets that we have prepared in advance.
Using code snippets
All we mean by the term code snippets  is a prepared code that we can read into our current
script. This is especially easy with vim being able to read the contents of other text files
during editing:
&4$
SQBUIBOEGJMFOBNF 
For example, if we need to read the contents of a file called JG located in )0.&TOJQQFUT ,
we will use the following key sequences in vim:
&4$
S)0.&TOJQQFUTJG
The contents of this file are read into the current document below the current cursor
position. In this way, we can make the code snippets as complex as we need and maintain
the correct indentations to aide readability and consistency.
So, we will make it our duty to always create a snippets directory in our IPNF  directory:
$ mkdir -m 700 $HOME/snippets
It is not required to share the directory, so it is good practice to set the mode to  or
private to the user when it is being created.
When creating snippets, it is your choice to use a pseudo-code or real examples. My
preference is to use real examples that are edited to reflect the requirements of the recipient
script. The contents of a simple JG snippet will be as follows:
JG<[>UIFO
FDIP6TBHFOBNF 
FYJU
GJ
Creating Code Snippets Chapter 4
[ 78 ]This gives us the layout to create an JG statement with a practical example. In this case, we
check to see whether  is unset and send an error to the user before exiting the script. The
key is in keeping the snippet short to limit the changes that need to be made but make it
easily understood and expandable as required.
Bringing color to the Terminal
If we are to display text messages to the users and operators executing the scripts, we can
provide colors to help in message interpretation. Using red as a synonym for errors and
green to indicate success makes it easier to add functionality to our scripts. Not all but
certainly a vast majority of Linux Terminals support color. The built-in command FDIP ,
when used with the F option, can display color to users.
To display a text in red, we can use the FDIP  command as follows:
$ echo -e "\033[31mError\033[0m"
The following screenshot shows both the code and the output:
The red text will bring immediate attention to the text and the potential failure of the script
execution. The use of color in this way adheres to the basic principles of application design.
If you find the code cumbersome, then simply use friendly variables to represent the colors
and the reset code.
In the previous code, we used red and the final reset code to set the text back to the shell
default. We could easily create variables for these color codes and others:
3&%=<N
(3&&/=<N
#-6&=<N
3&4&5=<N
The =  value is the escape character and <N  is the color code for red.
Creating Code Snippets Chapter 4
[ 79 ]We need to take care while using variables, to ensure that they are properly delimited from
the text. Modifying the earlier example, we can see how this is easily achieved:
$ echo -e ${RED}Error$RESET"
We use the brace brackets to ensure that the 3&% variable is identified and
separated from the &SSPS  word.
Saving the variable definitions to the )0.&TOJQQFUTDPMPS  file will allow them to be
used in other scripts. Interestingly, we don't need to edit this script; we can use the
command TPVSDF  to read these variables definitions into the script at runtime. Within the
recipient script, we need to add the following line:
source $HOME/snippets/color
Using the shell built-in TPVSDF  command will read the color variables into the script that is
executing at runtime. The following screenshot shows a modified version of the IFMMPTI
script that we now call IFMMPTI , which makes use of these colors:
We can see the effect this has when we execute the script. In the following screenshot, you
will see the execution and output both with and without a supplied parameter:

Creating Code Snippets Chapter 4
[ 80 ]We can easily identify the success and failure of the script through the color-coded output;
the green )FMMPGSFE  where we supply the parameter, and the red 6TBHF  statement where
we have not provided the required name.
Creating snippets using VS Code
For those who love graphical IDEs, you can use VS Code as an editor for your shell scripts.
We used it as a debugger in $IBQUFS , The What and Why of Scripting with Bash . Now we
will see one of its capabilities as an editor.
You can create your own snippets in VS Code as follows.
Navigate to File | Preferences  | User Snippets .
Then start to type TIFMM . This will open the TIFMMTDSJQUKTPO  file.
The file has two brackets ready to enter your snippets between them:

Creating Code Snippets Chapter 4
[ 81 ]To create a snippet, type the following between the brackets on the file:
1SJOUBXFMDPNFNFTTBHF\
QSFGJYXFMDPNF
CPEZ<
FDIP	8FMDPNFUPTIFMMTDSJQUJOH	
>
EFTDSJQUJPO1SJOUXFMDPNFNFTTBHF
^
You can use the following template and modify it based on your needs.
Try to use prefixes different to the shell scripting keywords to avoid confusion.
Creating Code Snippets Chapter 4
[ 82 ]When you open any TI file and start to type XFMDPNF , the autocompletion will show you
the snippet we have just created:
You can use any prefix you want; in our case, we used XFMDPNF  so the autocompletion
starts with it.
Creating Code Snippets Chapter 4
[ 83 ]You can add many lines to your snippet body:
1SJOUUPBXFMDPNFNFTTBHF\
QSFGJYXFMDPNF
CPEZ<
FDIP	8FMDPNFUPTIFMMTDSJQUJOH	
FDIP	5IJTJTBTFDPOENFTTBHF	
>
EFTDSJQUJPO1SJOUXFMDPNFNFTTBHF
^
You can use placeholders in your snippet body to simplify code editing.
Placeholders are written like this:
$1, $2, etc,
Modify the previous snippet and add a placeholder like this:
1SJOUBXFMDPNFNFTTBHF\
QSFGJYXFMDPNF
CPEZ<
FDIP	8FMDPNFUPTIFMMTDSJQUJOH	
>
EFTDSJQUJPO1SJOUXFMDPNFNFTTBHF
^
When you start to type XFMDPNF  and after you choose the snippet, you will notice that the
cursor will stop at the exact position of the placeholder waiting for your input.
You can use choices if you forget what to type in these editable places:
1SJOUUPBXFMDPNFNFTTBHF\
QSFGJYXFMDPNF
CPEZ<
FDIP	8FMDPNFUPTIFMMTDSJQUJOH\]GJSTUTFDPOEUIJSE]^	
>
EFTDSJQUJPO1SJOUXFMDPNFNFTTBHF
^
Creating Code Snippets Chapter 4
[ 84 ]After you choose this snippet in your code and hit Enter , you should see the cursor waiting
for your input with your choices:
That's very helpful!
Also, you can add a default value for the placeholder so this value will be written if you hit
Tab:
1SJOUBXFMDPNFNFTTBHF\
QSFGJYXFMDPNF
CPEZ<
FDIP	8FMDPNFUPTIFMMTDSJQUJOH\CPPL^	
>
EFTDSJQUJPO1SJOUXFMDPNFNFTTBHF
^
Creating Code Snippets Chapter 4
[ 85 ]Summary
To any administrator, script reuse will always be upmost in the quest for efficiency. Using
vim at the command line can make for very quick and effective editing of a script and we
can save typing in the use of abbreviations. These are best set within a user's personal
WJNSD  file and are defined with the BCCS  control. Beyond abbreviations, we can see the
sense in using code snippets. These are pre-prepared blocks of code that can be read into
the current script.
Also, we had a look at the value in using color at the command line where a script will
provide feedback. In the first look, these color codes are not the friendliest, but we can
simplify the process by using variables. We created variables with color codes and saved
them to a file and by using source command, these variables will be available to our current
environment.
Finally, we saw how to create code snippets using VS Code and how to add placeholders to
simplify our code editing.
In the next chapter, we will look at other mechanisms that we can use to write test
expressions simplifying the use of integers and variables.
Questions
The following code creates a snippet which prints one line. How do you make the1.
snippet with choices?
)FMMPNFTTBHF\
QSFGJYIFMMP
CPEZ<
FDIP	)FMMP	
>
EFTDSJQUJPO)FMMPNFTTBHF
^
Which command should you use to make your code snippets available for your2.
use in the shell?
Creating Code Snippets Chapter 4
[ 86 ]Further reading
Please see the following for further reading relating to this chapter:
IUUQTDPEFWJTVBMTUVEJPDPNEPDTFEJUPSVTFSEFGJOFETOJQQFUT
IUUQTCSJHBEFFOHJOFFSJOHTIBSQFOZPVSWJNXJUITOJQQFUT
CEC
55
Alternative Syntax
So far in the scripting journey, we have seen that we can use the UFTU  command to
determine a conditional status. We have taken this a little further and discovered that we
can also make use of the single square bracket. Here, we will recap the UFTU  command and
look at the single square bracket in more detail. After having learned more about the square
bracket, we will move onto more advanced variable or parameter management, thus
providing defaults and understating quoting issues.
Finally, we are going to see that within advanced shells such as bash, Korn, and Zsh, we can
go with double brackets! Making use of the double round parenthesis and double square
bracket can simplify the overall syntax and allow the standardization of the use of
mathematical symbols.
In this chapter, we will cover the following topics:
Recapping UFTU
Providing parameter defaults
When in doubt b quote!
Advanced tests using <<
Arithmetic operations using 

Technical requirement
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Alternative Syntax Chapter 5
[ 88 ]Recapping the test command
So far, we have used the built-in UFTU  command to drive our conditional statements. Using
other options with UFTU , we can look at the returned value to determine the status of files in
the filesystem. Running the UFTU  command without any options will return a false output:
$ test
Testing files
Commonly, we can use UFTU  to check the conditions based around files. For example, to
test whether a file is present or not, we can use the F option. The following command will
test the existence of the FUDIPTUT  file:
test -e /etc/hosts
We can run this UFTU  again, but this time check that the file not only exists but is a regular
file as opposed to having some special purpose. Specific file types can be directories, pipes,
and links, among others. The option for a regular file is G:
$ test -f /etc/hosts
Adding logic
If we need to open a file from within our script, we test that the file is both a regular file and
has the read permission set. To achieve this with UFTU , we can also include the B option to
"/% multiple conditions together. In the following example command, we will use the S
condition to check that the file is readable:
$ test -f /etc/hosts -a -r /etc/hosts
Similarly, the use of P is supported to 03 two conditions within an expression.
Square brackets as not seen before
As an alternative to the UFTU  command, we can implement the same conditional tests using
the single square bracket. Repeating the previous conditional UFTU  and omitting the
command itself. We will rewrite this, as shown in the following command:
 $ [ -f /etc/hosts -a -r /etc/hosts ]
Alternative Syntax Chapter 5
[ 89 ]Many times, even as experienced administrators, we are used to language elements and we
accept them as they are. I feel many Linux administrators will be surprised to learn that < is
a command for both a shell built-in and a standalone file. Using the UZQF  command, we can
verify this:
$ type -a [
We can see the output of this command in the following screenshot confirming its existence:
The built-in < command imitates the UFTU  command but it requires a closing bracket.
Now we know a little more about the < command, which is found in bash and the earlier
Bourne shell, we can now continue to add a little command-line list syntax. In addition to
the command-line list, we can see the desired functionality working in the following
command sample:
$ FILE=/etc/hosts
$ [ -f $FILE -a -r $FILE ] && cat $FILE
Having set the parameter '*-&  variable, we can test that it is both a regular file and is
readable by the user before attempting to list the file contents. In this way, the script
becomes more robust without the need for a complex script logic. We can see the code in
use in the following screenshot:

Alternative Syntax Chapter 5
[ 90 ]This type of abbreviation is quite common and is easily recognizable. We should always be
cautious of using abbreviations if they do not add readability. Our aim in scripting should
be to write clear and understandable code and avoid shortcuts if they do not add to this
goal.
Providing parameter defaults
Within bash parameters, there are named spaces in the memory that allow us access to
stored values. There are two types of parameters:
Variables
Special parameters
Variables
We already described what variables are and how to define them in $IBQUFS , The What
and Why of Scripting with Bash .  
Just to refresh your memory, you can define a variable by assigning a value with an equals
sign and without any spaces like this:
CJOCBTI
NZWBS
NZWBSXFMDPNF
So nothing new here.
Special parameters
Special parameters are the second parameter type and are managed by the shell itself and
are presented as read-only. We have come across these before in parameters such as  but
let's take a look at another . We can expand these parameters to gain an understanding of
their use, using the FDIP  command:
$ echo "My shell is $0 and the shell options are: $-"
Alternative Syntax Chapter 5
[ 91 ]From the annotated text that I have added, we can understand that the  option represents
the shell options that are configured. These can be displayed using the set P command but
it can be read programmatically using .
We can see this in the following screenshot:
The options set here are as follows:
I: This is short for hashall; it allows for programs to be found using the 1"5)
parameter
J: This shows that this is an interactive shell
N: This is short for monitor; it allows the use of the CH and GH commands to bring
commands in and out of the background
#: This allows the brace expansion or NLEJSEJS\^ , where we create EJS
and EJS
): This allows history expansion of running commands, such as   to repeat
commands from history
Setting defaults
Using either the UFTU  command or the brackets, we can provide default values for
variables, including command-line parameters. Taking the IFMMPTI  script we worked
with earlier, we can modify it and set the OBNF  parameter if it is zero bytes:
#!/bin/bash
name=$1
[ -z $name ] && name="Anonymous"
echo "Hello $name"
exit 0
This code is functional but it is our choice how we code in the default value. We can,
alternatively, assign a default value directly to the parameter. Consider the following
command, where a default assignment is made directly:
name=${1-"Anonymous"}
Alternative Syntax Chapter 5
[ 92 ]In bash, this is known as parameter substitution  and can be written in the following
pseudo-code:
${parameter-default}
Wherever a variable ( QBSBNFUFS ) has not been declared and has a null value, the default
value will be used. If the parameter has been explicitly declared with a null value, we will
use the  syntax, as shown in the following example:
parameter=
${parameter:-default}
By editing the script now, we can create IFMMPTI  to make use of bash parameter
substitution to provide the default value:
#!/bin/bash
#Use parameter substitution to provide default value
name=${1-"Anonymous"}
echo "Hello $name"
exit 0
This script and its output, both with and without a supplied value, are shown in the
following screenshot:
The IFMMPTI  script provides the functionality that we need, with the logic built directly
into the parameter assignment. The logic and assignment are now a single line of code
within the script and this is a major step in keeping the script simple and maintaining the
readability.
Alternative Syntax Chapter 5
[ 93 ]When in doubt ` quote!
Having established that variables are a type of parameter, we should always keep this in
mind, especially when reading manuals and HOWTOs . Often the documentation refers to 
parameters and, in doing so, they include variables, as well as the bash special parameters,
such as  and so on. In keeping with this, we will look at why it is advisable to quote the
parameters when we use them on the command line or within scripts. Learning this now
can save us a lot of pain and heartache later, especially when we start looking at loops.
First, the correct term that we should use for reading the value of variables is parameter
expansion . To you and me, this is reading a variable, but to bash this would be too simple.
The assignment of a correct name, such as parameter expansion, reduces any ambiguity to
its meaning but adds complexity at the same time. In the following example, the first line of
command assigns the value of GSFE  to the OBNF  parameter. The second line of command
uses parameter expansion to print the stored value from memory. The  symbol is used to
allow the expansion of the parameter:
$ name=fred
$ echo "The value is: $name"
In the example, we have used the double quotes to allow FDIP  to print the single string as
we have used spaces. Without the use of quotes, FDIP  might have seen this as multiple
arguments, the space being the default field separator in most shells, including bash. Often,
when we do not think to use quotes, we do not see the spaces directly. Consider the
following extract of command-line code that we made use of earlier:
$ FILE=/etc/hosts
$ [ -f $FILE -a -r $FILE ] && cat $FILE
Even though this worked, we may have been a little fortunate, especially if we were
populating the '*-&  parameter from a list of files that we had not created ourselves. It is
quite conceivable that a file can have spaces within its name. Let's now replay this
command using a different file. Consider the following command:
$ FILE="my file"
$ [ -f $FILE -a -r $FILE ] && cat $FILE
Even though, structurally, there has been no change to the code, it now fails. This is because
we are providing too many arguments to the < command. The failing result will be the
same even if we use the UFTU  command.
Alternative Syntax Chapter 5
[ 94 ]Even though we have correctly quoted the assignment of the filename to the parameter
'*-& , we have not protected the spaces when the parameter is expanded. We can see the
code failing, as it is captured in the following screenshot:
We can see that this will not be ready for our scripts. Alas, what we once thought of as
robust is now in tatters and, like the Titanic, our code has sunk.
However, a simple solution is to revert to quoting parameter expansion unless specifically
not desired. We can make this ship unsinkable with a simple edit to the code:
$ FILE="my file"
$ [ -f "$FILE" -a -r "$FILE" ] && cat "$FILE"
We can now proudly stand on the White Star Line dock, as we see the Titanic II get
launched in the following code example, which is captured in the following screenshot:
It is truly amazing and sometimes just a little unbelievable what effect these tiny quotes can
have. We should never ignore the quotes when expanding variables. To ensure that we drill
home this point, we can highlight this phenomenon in another, even simpler, example. Let's
take the scenario where we now just want to remove the file. In the first example, we do not
use quotes:
$ rm $FILE
This code will produce failures as the parameter expansion will lead to the following
perceived command:
$ rm my file
Alternative Syntax Chapter 5
[ 95 ]The code will fail because it is unable to find the NZ file or the GJMF  file. Even worse, we
could potentially be deleting incorrect files if any of the names could be resolved
accidentally. However, quoting the parameter expansion will save the day, as we see in the
second example:
$ rm "$FILE"
This is correctly expanded to the desired command that we illustrate in the following
command example:
$ rm "my file"
I certainly hope that these examples demonstrate the need for care when expanding 
parameters and make you aware of the pitfalls.
Advanced tests using [[
The use of the double brackets <<DPOEJUJPO>>  allows us to do more advanced
condition testing but it is not compatible with the Bourne shell. The double brackets were
first introduced as a defined keyword in the KornShell and are also available in bash and
Zsh. Unlike the single bracket, this is not a command but a keyword. The use of the UZQF
command can confirm this:
$ type [[
White space
The fact that << is not a command is significant where white space is concerned. As a
keyword, << parses its arguments before bash expands them. As such, a single parameter
will always be represented as a single argument. Even though it goes against best practice,
<< can alleviate some of the issues associated with white space within parameter values.
Reconsidering the condition we tested earlier, we can omit the quotes when using <<, as
shown in the following example:
$ echo "The File Contents">"my file"
$ FILE="my file"
$ [[ -f $FILE && -r $FILE ]] && cat "$FILE"
We still need to quote the parameter when using DBU, as you can see, and we can use quotes
within the double brackets but they become optional. Note that we can also use the more
traditional  and ]] to represent B and P respectively.
Alternative Syntax Chapter 5
[ 96 ]Other advanced features
These are some of the extra features that we can include with the double brackets. Even if
we lose portability in using them, there are some great features that overcome the loss.
Remember that if we only use bash, then we can use the double brackets but can't run our
scripts in the Bourne shell. The advanced features that we gain, which are covered in the
following sections, include pattern matching and regular expressions.
Pattern matching
Using the double brackets, we can do more than just match strings, we can use pattern
matching. For example, we may need to work exclusively with Perl scripts, files that end
with QM. We will be able to implement this easily within a condition by including the
pattern as a match, as shown in the following example:
$ [[ $FILE = *.pl ]] && cp"$FILE" scripts/
Regular expressions
We will talk in dept about regular expressions in a $IBQUFS , Regular Expressions , but let's
take a small glimpse now.
We could rewrite the last example using a regular expression:
$ [[ $FILE =~ \.pl$ ]] && cp "$FILE" scripts/
As the single dot or period has a special meaning in regular expressions,
we need to escape it with =.
The following screenshot shows the regular expression matching working with a file called
NZQM  and another called NZBQM . The match correctly shows for the file that ends in QM:

Alternative Syntax Chapter 5
[ 97 ]Regular expression script
Another simple demonstration of conditional testing using regular expressions will be to
expose the US and UK spelling of color , being color  and colour  respectively. We may prompt
the user if they want a color or mono output for the script but at the same time cater for
both spellings. The line that will do the work in the script is as follows:
if [[ $REPLY =~ colou?r ]] ; then
The regular expression caters to both spellings of color  by making the V optional: V.
Furthermore, we can disable case sensitivity allowing for COLOR  and color  by setting a shell
option:
shopt -s nocasematch
This option can be disabled again at the end of the script with the following command:
shopt -u nocasematch
When we use the variable parameters that we have named (3&&/  and 3&4&5 , we affect
the color of the output. The color green will only be shown where we have sourced the
color definition file. This is set when we choose the color display. Selecting mono will
ensure that the variable parameters are null and have no effect.
The complete script is shown in the following screenshot:

Alternative Syntax Chapter 5
[ 98 ]Arithmetic operations using ((
When using bash and some other advanced shells, we can make use of the 

  notation
to simplify mathematical operations with scripts.
Simple math
The double parenthesis construct in bash allows for arithmetic expansion. Using this in the
simplest format, we can easily carry out integer arithmetic. This becomes a replacement for
the MFU built-in. The following examples show the use of the MFU command and the double
parenthesis to achieve the same result:
$ a=(( 2 + 3 ))
$ let a=2+3
In both cases, the B parameter is populated with the sum of  . If you want to write it
on a shell script, you need to add a dollar sign before the parentheses:
#!/bin/bash
echo $(( 2 + 3 ))
Parameter manipulation
Perhaps a little more useful to us in scripting is the C-style parameter manipulation that we
can include using the double parenthesis. We can often use this to increment a counter
within a loop and also put a limit on the number of times the loop iterates. Consider the
following command:
$ COUNT=1
$ (( COUNT++ ))
echo $COUNT
Within this example, we first set $06/5  to  and then we increment it with the  operator.
When it is echoed in the final line, the parameter will have a value of . We can see the
results in the following screenshot:

Alternative Syntax Chapter 5
[ 99 ]We can achieve the same result in longhand by using the following syntax:
$ COUNT=1
$ (( COUNT=COUNT+1 ))
echo $COUNT
This of course allows for any increment of the $06/5  parameter and not just a single unit
increase. Similarly, we can count down using the  operator, as shown in the following
example:
$ COUNT=10
$ (( COUNT-- ))
echo $COUNT
We start using a value of , reducing the value by  within the double parentheses.
Note that we do not use the  to expand the parameters within the
parentheses. They are used for parameter manipulation and, as such, we
do not need to expand parameters explicitly.
Standard arithmetic tests
Another advantage that we can gain from these double parentheses is with the tests. Rather
than having to use HU for greater than, we can simply use  . We can demonstrate this in
the following code:
$(( COUNT > 1 )) && echo "Count is greater than 1"
The following screenshot demonstrates this:
It is this standardization, both in the C-style manipulation and tests, that makes the double
parenthesis so useful to us. This use extends to both the command line and scripts. We will
use this feature extensively when we look at looping constructs.
Alternative Syntax Chapter 5
[ 100 ]Summary
Within this chapter, I really hope that we have introduced many new and interesting
choices to you. This was an area with a wide range where we began by recapping the use of
UFTU  and discovered that the < is a command not a syntax construct. The main effect that it
is a command is on white space and we looked at the need to quote variables.
Even though we may commonly call variables variables, we have also seen that their correct
name, especially in documentation, is parameters. Reading a variable is a parameter
expansion. Understanding parameter expansion can help us understand the use of the
keyword <<. The double square brackets are not commands and do not expand the
parameters. This means that we do not need to quote variables even if they do contain
white space. Moreover, we can use advanced tests with double square brackets, such as
pattern matching or regular expressions.
Finally, we looked at arithmetic expansion and parameter manipulation using the double
parenthesis notation. The biggest feature this delivers is the possibility to easily increment
and decrement counters.
In the next chapter, we will move onto the looping constructs found in bash and make use
of some of our new-found skills from this chapter.
Questions
How do you subtract 8 from 25 using shell scripting?1.
What is wrong with the following code? And how can you fix it?2.
$ rm my file
What is the problem with the following code?3.
#!/bin/bash
a=(( 8 + 4 ))
echo $a
Alternative Syntax Chapter 5
[ 101 ]Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH-%1BCTIUNMBSJUIFYQIUNM
IUUQXJLJCBTIIBDLFSTPSHDPNNBOETDMBTTJDUFTU
66
Iterating with Loops
Now we can perform arithmetic operations and tests and our scripts have more control.
Sometimes, you will find that you need to perform some tasks repeatedly, such as going
through log file entries and performing an action, or maybe running a piece of code
continuously. We are busy people who have better things to do than repeat a task 100 times
or more; loops are our friends.
Looping structures are the lifeblood of scripts. These loops are workhorse engines that can
iterate many times, repeating the same task reliably and consistently. Imagine having
100,000 lines of text within a CSV file that has to be checked for incorrect entries. A script
can do this easily and accurately once developed but, in the case of a human, the reliability
factor and accuracy will fail very quickly.
So let's see how we can save our time and sanity by covering the following topics in this
chapter:
GPS loops
Advanced GPS loops
The internal field separator (IFS)
Counting directories and files
C-style for loops 
Nested loops
Redirecting loop output
XIJMF  loops and VOUJM  loops
Reading input from files
Creating operator menus
Iterating with Loops Chapter 6
[ 103 ]Technical requirement
The source code for this chapter can be can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
for loops
All our looping controls are simple and we will begin by looking at GPS loops. The word
GPS is a keyword in bash and in terms of its working, it is similar to JG. We can use the 
command type to verify this, as shown in the following example:
$ type for
for is a shell keyword
As a reserved shell keyword, we can use a GPS loop both in scripts and directly at the
command line. In this way, we can utilize loops within and without the scripts, optimizing
the use of the command line. A simple GPS loop is shown in the following example code:
GPSVJOCPCKPFEP
VTFSBEEV
FDIP	V1BTTXPSE	]DIQBTTXEQJQFUIFDSFBUFEVTFSUPDIQBTTXE
QBTTXEFV
EPOF
The VTFSBEE  command is used to create users and the DIQBTTXE  command is used to
update passwords in batch mode.
Within a GPS loop, we read from the list on the right to populate the variable parameter on
the left; in this case, we will read from the list containing CPC and KPF into the parameter
variable V. Each item from the list is inserted into the variable, one item at a time. In this
way, as long as there are items to be processed in the list, the loop will execute until the list
is exhausted.
Practically, for us, the execution of this loop means that we will do the following:
Create the user CPC 1.
Set the password for CPC 2.
Expire the password so it will need to be reset on the first login for the user CPC 3.
We then loop back and repeat the process for the user KPF.
Iterating with Loops Chapter 6
[ 104 ]We can view the preceding example in the following screenshot. After having gained root
access through TVEPJ , we proceeded to run the loop and create the users:
The list that is read in the GPS loop can be generated dynamically or statically, as shown in
the previous example. To create dynamic lists, we could use various globbing techniques to
populate the list. As an example, to work with all files in a directory, we could use , as
shown in the following example:
for f in * ; do
stat "$f"
done
When a list is generated, such as with file globbing, we should quote the
expansion of the variable parameter. Without the quotes, it is possible that
a space will be included that will cause the command to fail. This is what
we have seen here in the TUBU  command.
In the following examples, we isolate the filenames that begin with CB. We then use the
TUBU  command to print the inode metadata. The code and output are shown in the
following screenshot:

Iterating with Loops Chapter 6
[ 105 ]This list can also be generated from the output of another command or a pipeline of
commands. For example, if we need to print the current working directory of all logged-in
users, we could try something similar to the following:
$ for user in $(who | cut -f1 -d" ") ; do
lsof -u "$user" -a -c bash | grep cwd
done
In the previous example, we can see that the choice of name for the parameter is down to
us; we are not limited to a single character and we can use the VTFS  name in this example.
By using lowercase, we will not overwrite the system variable 64&3 . The following
screenshot demonstrates the loop and the subsequent output:
The MTPG  command will list open files; we can search for the files opened by each user in
turn and with the CBTI  command as the current working directory.
Working with the scripts that we have created so far, we can create a new script called
IFMMPTI . If we copy the )0.&CJOIFMMPTI  script to the new script, we can edit it
to make use of a GPS loop:
CJOCBTI
FDIP:PVBSFVTJOH
CBTFOBNF
GPSOJO
EP
FDIP)FMMPO
EPOF
FYJU
The loop is used to iterate through each command-line argument supplied and greet each
user individually. When we execute the script, we can see that we can now display the
)FMMP  message for each user. This is shown in the following screenshot:
Iterating with Loops Chapter 6
[ 106 ]
Although what we have seen here is still relatively trivial, we should now realize a little of
what we can do with scripts and loops. The arguments of this script can be the usernames
that we have already used or anything else. If we stick with the usernames, then it will be
very easy to create user accounts and set passwords, as we saw earlier.
Advanced for loops
In the previous examples, we used the GPS loop to iterate over simple values where each
value has no space.
As you know, if your values contain a space, you should use double quotes:
CJOCBTI
GPSWBSJOPOF5IJTJTUXP/PXUISFF8F	MMDIFDLGPVS
EP
FDIP7BMVFWBS
EPOF
As you can see, each value is printed as expected thanks to double quotes.
This example contains values in one line and we quote the values because they have spaces
and commas. What if the values were on multiple lines, as in a file?
Iterating with Loops Chapter 6
[ 107 ]What if the separator between the values we want to iterate over is something other than a
space such as a comma or a semicolon?
Here comes the IFS.
The IFS
By default, the IFS variable has the value of one of (space, newline, or tab).
Suppose that you have a file like the following and you want to iterate over its lines:
)FMMPUIJTJTBUFTU
5IJTJTUIFTFDPOEMJOF
"OEUIJTJTUIFMBTUMJOF
Let's write the GPS loop that will iterate over these lines:
CJOCBTI
GJMFGJMFUYU
GPSWBSJO
DBUGJMF
EP
FDIPWBS
EPOF
If you check the result, it's something that we don't need:

Iterating with Loops Chapter 6
[ 108 ]Since the first separator the shell found is the space, the shell treats every word as a field,
but we need every line to be printed as a field.
Here we need to change the IFS variable to be newline instead.
Let's modify our script to iterate over lines correctly:
CJOCBTI
GJMFGJMFUYU
*'4	=O	)FSFXFDIBOHFUIFEFGBVMU*'4UPCFBOFXMJOF
GPSWBSJO
DBUGJMF
EP
FDIPWBS
EPOF
We changed the IFS variable to newline and it works as expected.
Look at the dollar sign in the IFS definition in the preceding section, *'4=O . By default,
bash doesn't interpret escape characters such as =S, =O, and =U. So, in our example, it will be
treated as an O character, so to interpret escape characters, you have to use a dollar sign ( )
before it to make it work properly.
But if your IFS is a normal character, you don't have to use the dollar sign ( ) at all.
Counting directories and files
We can use a simple GPS loop to iterate over folder content and use an JG statement to
check whether the path is a directory or a file:
CJOCBTI
GPSQBUIJOIPNFMJLFHFFLT
EP
JG<EQBUI>
UIFO
FDIPQBUIJTBEJSFDUPSZ
Iterating with Loops Chapter 6
[ 109 ]FMJG<GQBUI>
UIFO
FDIPQBUIJTBGJMF
GJ
EPOF
This is pretty straightforward script. We iterate over directory content and then we use an
JG statement to check whether the path is a directory or a file. Finally, we print beside each
path whether it's a file or a directory.
We used quotes for the path variable because the file could contain a
space.
C-style for loops 
If you have a C language background, you will be happy to know that you can write your
GPS loops in C-style. This feature was taken from KornShell. The shell GPS loop can be
written like this:
GPS
WWW
\
QSJOUG
7BMVFJTE=OW
^
It is easy for C developers to use this syntax in GPS loops.
Iterating with Loops Chapter 6
[ 110 ]Check out this example:
CJOCBTI
GPS

WWW
EP
FDIPWBMVFJTW
EPOF
The choice is yours; you have a lot of syntax styles for the GPS loop.
Nested loops
Nested loops means loops inside loops. Check out the following example:
CJOCBTI
GPS

WWW
EP
FDIP'JSTUMPPQW
GPS

WWW
EP
FDIP4FDPOEMPPQW
EPOF
EPOF
The first loop hits first, then the second loop, and this happens three times.
Iterating with Loops Chapter 6
[ 111 ]Redirecting loop output
You can redirect the loop output to a file using the EPOF  command:
CJOCBTI
GPS

WWW
EP
FDIPW
EPOF GJMF
If there is no file, it will be created and filled with the loop output.
This redirection is helpful when you don't need to show the loop output on the screen and
save it to a file instead.
Controlling the loop
Having entered our loop, we may need to either exit the loop prematurely or perhaps
exclude certain items from processing. If we want to process only directories in a listing,
rather than every file of any type, then to implement this, we have loop control keywords,
such as CSFBL  and DPOUJOVF .
The CSFBL  keyword is used to exit the loop, processing no more entries, whereas the
DPOUJOVF  keyword is used to stop the processing of the current entry in the loop and
resume the processing with the next entry.
Assuming we only want to process directories, we could implement a test within the loop
and determine the file type:
$ for f in * ; do
[ -d "$f" ] || continue
chmod 3777 "$f"
done
Within the loop, we want to set permissions, including the SGID and sticky bits, but for the
directories only. The  search will return all files; the first statement within the loop will
ensure that we only process directories. If the test is done for the current loop, the target
fails the test and is not a directory; the DPOUJOVF  keyword retrieves the next loop-list item.
If the UFTU  returns USVF  and we are working with a directory, then we will process the
subsequent statements and execute the DINPE  command.
Iterating with Loops Chapter 6
[ 112 ]If we need to run the loop until we find a directory and then exit the loop, we can adjust the
code so that we can iterate though each file. If the file is a directory, then we exit the loop
with the CSFBL  keyword:
$ for f in * ; do
[ -d "$f" ] && break
done
echo "We have found a directory $f"
Within the following screenshot, we can see the code in action:
By working with the same theme, we can print each directory found in the listing using the
following code:
for f in * ; do
[ -d "$f" ] || continue
dir_name="$dir_name $f"
done
echo "$dir_name"
We can achieve a result by processing the loop item only if it is a directory and within the
loop. We can work with regular files only using the JG test. In this example, we append the
directory name to the EJS@OBNF  variable. Once we exit the loop, we print the complete list
of directories. We can see this in the following screenshot:
Using these examples and your own ideas, you should now be able to see how you can
control loops using the DPOUJOVF  and CSFBL  keywords.
Iterating with Loops Chapter 6
[ 113 ]while loops and until loops
When using the GPS loop, we iterate through a list; it's either the one that we create or the
one that is dynamically generated. Using the XIJMF  or VOUJM  loops, we loop based on the
fact that the condition becomes either USVF  or GBMTF .
A XIJMF  loop loops while the condition is true and, conversely, an VOUJM  loop will loop
while the condition is false. The following command will count from 10 through to zero,
each iteration of the loop printing the variable and then reducing the value by one:
$ COUNT=10
$ while (( COUNT >= 0 )) ; do
echo -e "$COUNT \c"
(( COUNT-- ))
done ; echo
We can see the output of this command in the following screenshot, thus confirming the
countdown to zero:
The use of the =D escape sequence used here allows the suppression of the
line feed normally used with FDIP . In this way, we can keep the
countdown on the single line of output. I think you will agree that it's a
nice effect.
The functionality of this loop can be gained using the VOUJM  loop; just a quick rethink of the
logic is required, as we will want to loop until the condition becomes true. Generally, it is a
personal choice and the way the logic works best for you about which loop to use. The
following example shows the loop written with the VOUJM  loop:
$ COUNT=10
$ until (( COUNT < 0 )) ; do
echo -e "$COUNT \c"
(( COUNT-- ))
done ; echo
Iterating with Loops Chapter 6
[ 114 ]Reading input from files
Now, it may seem that these loops can do a little more than just count down numbers. We
may want to read data in from a text file and process each line. The shell built-in SFBE
command that we saw earlier in this book can be used to read a file line by line. In this way,
we can use a loop to process each line of a file.
To demonstrate some of these functionalities, we will use a file that contains the server
addresses. These could be hostnames or IP addresses. In the following example, we will
make use of the IP addresses of Google DNS servers. The following command shows the
contents of the TFSWFSTUYU  file:
$ cat servers.txt
8.8.8.8
8.8.4.4
Using the SFBE  command in the condition of the XIJMF  loop, we can loop as long as we
have more lines to read from the file. We specify the input file directly after the EPOF
keyword. For each line that we read from the file, we can test whether the server is up with
the QJOH  command, and, if the server is responding, we append it to a list of available
servers. This list is printed once the loop closes. In the following example, we can see that
we begin to add in as many elements of scripting as we have covered in this book:
$ while read server ; do
ping -c1 $server && servers_up="$servers_up $server"
done < servers.txt
echo "The following servers are up: $servers_up"
Iterating with Loops Chapter 6
[ 115 ]We can verify the operation in the following screenshot, which captures the output:
Using this kind of loop, we can start to build extremely practical scripts to process
information either fed from the command line or from scripts. It will be very easy to replace
the filename that we read with , representing a positional parameter passed into the
script. Let's return to the QJOH@TFSWFSTI  script and adjust it to accept the input
parameter. We can copy the script to the new )0.&CJOQJOH@TFSWFS@GSPN@GJMFTI
file. Within the script, we first test whether the input parameter is a file. We then create an
output file with a tile that includes the date. As we enter the loop, we append available
servers to this file and list the file at the end of the script:
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
4DSJQUUPQJOHTFSWFSTGSPNGJMF
-BTU&EJUFE"VHVTU
JG<G>UIFO
FDIP5IFJOQVUUPTIPVMECFBGJMFOBNF
FYJU
GJ
FDIP5IFGPMMPXJOHTFSWFSTBSFVQPO
EBUFY TFSWFSPVU
EPOF
XIJMFSFBETFSWFS
EP
Iterating with Loops Chapter 6
[ 116 ]QJOHDTFSWFSFDIP4FSWFSVQTFSWFS  TFSWFSPVU
EPOF
DBUTFSWFSPVU
We can execute the script now in the following manner:
$ ping_server_from_file.sh servers.txt
The output from the script execution should be similar to the following screenshot:
Creating operator menus
We can provide a menu to the Linux operators who need limited functionality from the
shell and do not want to learn the details of command-line use. We can use their login script
to launch a menu for them. This menu will provide a list of command selections to choose
from. The menu will loop until the user chooses to exit from the menu. We can create a new
)0.&CJONFOVTI  script; the basis of the menu loop will be the following:
XIJMFUSVF
EP

EPOF
Iterating with Loops Chapter 6
[ 117 ]The loop we have created here is infinite. The USVF  command will always return true and
loop continuously; however, we can provide a loop control mechanism to allow the user to
leave the menu. To start building the structure of the menu, we will need to echo some text
within the loop asking the user for their choice of command. We will clear the screen before
the menu is loaded each time and an additional read prompt will appear after the execution
of the desired command.
This allows the user to read the output from the command before the screen is cleared and
the menu is reloaded. The script will look like the following code at this stage:
CJOCBTI
"VUIPS!UIFVSCBOQFOHVJO
8FCXXXUIFVSCBQFOHVJODPN
4BNQMFNFOV
-BTU&EJUFE"VHVTU
XIJMFUSVF
EP
DMFBS
FDIP$IPPTFBOJUFNBCPSD
FDIPB#BDLVQ
FDIPC%JTQMBZ$BMFOEBS
FDIPD&YJU
SFBETO
SFBEOQ1SFTTBOZLFZUPDPOUJOVF
EPOF
If you execute the script at this stage, there will be no mechanism to leave the script. We
have not added any code to the menu selections; however, you can test functionality and
exit using the Ctrl + C keys.
At this stage, the menu should look similar to the output shown in the following screenshot:
To build the code behind the menu selection, we will implement a DBTF  statement. This will
be added in between the two SFBE  commands, as follows:
read -sn1
case "$REPLY" in
a) tar -czvf $HOME/backup.tgz ${HOME}/bin;;
Iterating with Loops Chapter 6
[ 118 ]b) cal;;
c) exit 0;;
esac
read -n1 -p "Press any key to continue"
We can see the three options that we have added to the DBTF  statement, B, C, and D:
Option B: This runs the UBS command to back up the scripts
Option C: This runs the DBM command to display the current month
Option D: This exits the script
To ensure that the user is logged out when exiting from their login script, we will run the
following:
exec menu.sh
The FYFD  command is used to ensure that the shell is left after the NFOVTI  file is complete.
In this way, the user never needs to experience the Linux shell. The complete script is
shown in the following screenshot:

Iterating with Loops Chapter 6
[ 119 ]Summary
We have begun to make progress within this chapter. We have been able to join many of the
elements that we have previously used into cohesive and functional scripts. Although the
focus of this chapter has been on loops, we have used command-line lists, JG statements,
DBTF  statements, and arithmetic calculations.
We opened this chapter by describing loops as the workhorse of our scripts and we have
been able to demonstrate this with GPS, XIJMF , and VOUJM  loops. The GPS loop is used to
iterate through elements of a list. The list can be either static or dynamic; with an emphasis
on dynamic lists, we showed how simply these are created through file globbing or
command expansion.
Also, we saw how to iterate over complex values and how to set the IFS to iterate over fields
correctly.
We learned how to write nested loops and how to redirect loop output to files.
The XIJMF  and VOUJM  loops are controlled using conditions. The XIJMF  loop will loop
while the supplied condition is true. The VOUJM  loop will loop until the supplied condition
returns true or while it returns false. The DPOUJOVF  and CSFBL  keywords are specific to
loops and, using them along with FYJU , we can control the loop flow.
In the next chapter, we will look at modularizing scripts using functions.
Questions
How many lines will be printed on screen from the following script?1.
#!/bin/bash
for (( v1 = 12; v1 <= 34; v1++ ))
do
echo "$v1"
done > output
Iterating with Loops Chapter 6
[ 120 ]How many lines will be printed on the screen from the following script?2.
#!/bin/bash
for (( v=8; v <= 12; v++ ))
do
if [ $v -ge 12 ]
then
break
fi
echo "$v"
done
What is wrong with the following script? And how can you fix it?3.
#!/bin/bash
for (( v=1, v <= 10, v++ ))
do
echo "value is $v"
done
How many lines will be printed on the screen from the following script?4.
#!/bin/bash
count=10
while (( count >= 0 )) ; do
echo $count
done
$((count--))
exit 0
Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH-%1BCTIUNMJOUFSOBMWBSJBCMFTIUNM
IUUQUMEQPSH)0850#BTI1SPH*OUSP)0850IUNM
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMTFDU@@IUNM
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMTFDU@@IUNM
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMTFDU@@IUNM
77
Creating Building Blocks with
Functions
In this chapter, we will dive into the wonderful world of functions. We can look at these as
modular building blocks creating powerful and adaptive scripts. By creating functions, we
add the code in a single building block isolated from the rest of the script. Focusing on
improvements of a single function is a lot easier than trying to improve the script as a single
object. Without functions, it is difficult to hone in on problem areas and the code is often
repeated, which means that updates need to happen in many locations. Functions are
named as blocks of code or scripts within scripts and they can overcome many problems
associated with more complex code.
As we make our way through the chapter, we will cover the following topics:
Introducing functions
Passing parameters to functions
Variable scope
Returning values from functions
Recursive functions
Using functions in menus
Technical requirements
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Creating Building Blocks with Functions Chapter 7
[ 122 ]Introducing functions
Functions are blocks of code that exist in memory as named elements . These elements can
be created within the shell environment, as well as within the script execution. When a
command is issued at the command line, aliases are checked first and, following this, we
check for a matching function name. To display the functions residing in your shell
environment, you can use the following command:
$ declare -F
The output will vary depending on the distribution you are using and the number of
functions you have created. On my Linux Mint, the partial output is shown in the following
screenshot:
Using the small G option, you can display the function and the associated definition.
However, if we want to see just a single function definition, we can use the UZQF  command:
$ type quote
Creating Building Blocks with Functions Chapter 7
[ 123 ]The previous code example will display the code block for the RVPUF  function, if it exists
within your shell. We can see the output of this command in the following screenshot:
The RVPUF  function in bash inserts single quotes around a supplied input parameter. For
example, we can expand the 64&3  variable and display the value as a string literal; this is
shown in the following screenshot. The screenshot captures the command and output:
Most code can be represented by a pseudo-code which shows an example layout. Functions
are no different and the code to create a function is listed in the following example:
GVODUJPOOBNF
\
DPEFUPFYFDVUF 
^
Also, there is another way of defining functions, like this:
GVODUJPOGVODUJPOOBNF \
DPEFUPFYFDVUF 
^
The LFZXPSE  function is deprecated for portability with the Portable Operating System
Interface  (POSIX ) specification, but it is still used by some developers.
Note that the 
 are not necessary when using the LFZXPSE  function, but
they are a must if you define the function without the LFZXPSE  function.
Creating Building Blocks with Functions Chapter 7
[ 124 ]The function is created without a EP and EPOF  block, as we have used in the previous loops.
It is the purpose of the curly brackets to define the code block boundaries.
A simple function to display aggregated system information is shown in the following code.
This can be created at the command line and will be resident in your shell. This will not
persist the logins and will be lost when the shell is closed or the function is unset. To allow
the function to persist, we need to add this to the login script of our user account. The
sample code is as follows:
$ show_system() {
echo "The uptime is:"
uptime
echo
echo "CPU Detail"
lscpu
echo
echo "User list"
who
}
We can print the detail of the function similar to the prior instance using the UZQF
command; this is shown in the following screenshot:
To execute the function, we simply need to type TIPX@TZTUFN  and we will see the static
text and output from the three commands: VQUJNF , MTDQV , and XIP. This is, of course, a
very simple function but we can start to add more functionality by allowing parameters to
be passed at runtime.
Creating Building Blocks with Functions Chapter 7
[ 125 ]Passing parameters to functions
Earlier within this chapter, we referred to functions as scripts within scripts and we will still
maintain that analogy. Similar to how a script can have input parameters, we can create
functions that also accept parameters that can make their operation less static. Before we
work on a script, we can look at a useful function in the command line.
One of my pet peeves is overcommented configuration files, especially
where documentation exists to detail the options available.
The GNU's Not Unix  (GNU ) Linux TFE command can easily edit the file for us and remove
commented lines and empty lines. We are introducing the stream editor, TFE, here but we
will look at it in more detail in the following chapter.
The TFE command line that runs the in-place edit will be as follows:
$ sed -i.bak '/^\s*#/d;/^$/d' <filename>
We can run out forensics in the command line by breaking it down element by element.
Let's take a deeper look:
TFEJCBL : This edits the file and creates a backup with the extension CBL .
The original file will then be accessible as GJMFOBNF CBL .
?: This caret character ( ?) means edit the lines that start with what after the
caret. So the caret matches the beginning of a line.
=T: This means any amount of white space, including no spaces or tabs.
: This is a normal  sign. So the total expression ?=T  means we are looking
for lines that begin with comment or spaces and a comment.
E: This is the delete action to remove matching lines.
?E : The semicolon is used to separate expressions and the second expression
is similar to the first but this time we are preparing to delete empty lines.
To move this into a function, we will simply need to think of a great name. I like to build
verbs into function names; it helps with the uniqueness and identifies the purpose of the
function. We will create the DMFBO@GJMF  function as follows:
$ function clean_file {
sed -i.bak '/^\s*#/d;/^$/d' "$1"
}
Creating Building Blocks with Functions Chapter 7
[ 126 ]As within scripts, we use positional parameters to accept command-line arguments. We can
replace the hardcoded filename that we used previously with  within the function. We
will quote this variable to protect against spaces within the filename. To test the
DMFBO@GJMF  function, we will make a copy of a system file and work with the copy. In this
way, we can be sure that no harm comes to any system file. We can assure all readers that
no system files were harmed during the making of this book. The following are the detailed
steps we need to follow to perform the test on the new function:
Create the DMFBO@GJMF  function as described 1.
Move to your IPNF  directory using the DE command without arguments 2.
Copy the time configuration file to your IPNF  directory DQFUDOUQDPOG 3.
)0.&
Count the number of lines in the file with the following command: XDM 4.
)0.&OUQDPOG
Now remove the commented and empty lines with DMFBO@GJMF 5.
)0.&OUQDPOG
Now recount the lines using XDM)0.&OUQDPOG 6.
Also, check the count the backup of the original file that we created: XDM 7.
)0.&OUQDPOGCBL
The sequence of commands is shown in the following screenshot:
We can direct the attention of the function to the required file using the argument that was
supplied while executing the function. If we need to persist this function, then we should
add it to a login script. However, if we want to test this within a shell script, we can create
the following file to do this and practice some of the other elements we have learned. We
will need to take notice that the functions should always be created at the start of the script
as they need to be stored in memory by the time they are called. Just think that your
function needs to be unlocked and loaded before you pull the trigger.
Creating Building Blocks with Functions Chapter 7
[ 127 ]We will create a new shell script, )0.&CJODMFBOTI , and the execute permission, as
always, will need to be set. The code of the script is as follows:
CJOCBTI
4DSJQUXJMMQSPNQUGPSGJMFOBNF
UIFOSFNPWFDPNNFOUFEBOECMBOLMJOFT
JT@GJMF
\
JG<G>UIFO
FDIPEPFTOPUTFFNUPCFBGJMF
FYJU
GJ
^
DMFBO@GJMF
\
JT@GJMF
#&'03&
XDM
FDIP5IFGJMFTUBSUTXJUI#&'03&
TFEJCBL	?=TE?E	
"'5&3
XDM
FDIP5IFGJMFJTOPX"'5&3
^
SFBEQ&OUFSBGJMFUPDMFBO
DMFBO@GJMF3&1-:
FYJU
We have provided two functions within the script. The first, JT@GJMF , simply tests to
ensure that the filename we have entered is a regular file. Then we declare the DMFBO@GJMF
function with a little added functionality, displaying the line count of the file before and
after the operation. We can also see that functions can be nested and we call the JT@GJMF
function with DMFBO@GJMF .
Without the function definitions, we have only three lines of code at the end of the file,
which we can see in the example code laid out in the previous code block that has been
saved as )0.&CJODMFBOTI . We first prompt for the filename and then run the
DMFBO@GJMF  function, which in turn calls the JT@GJMF  function. The simplicity of the main
code is important here. The complexity is in the functions, as each function can be worked
on as a standalone unit.
Creating Building Blocks with Functions Chapter 7
[ 128 ]We can now test the script operation, first using a wrong filename, as we can see in the
following screenshot:
Now that we have seen the operation with an incorrect file, we can try again using an actual
file! We can use the same system file we worked on before. We need to first return the files
to their original state:
$ cd $HOME
$ rm $HOME/ntp.conf
$ mv ntp.conf.bak ntp.conf
With the file now ready, we can execute the script from the )0.&  directory as shown in the
following screenshot:
Passing arrays
Not all your passed values will be single values; you may need to pass an array to the
function. Let's see how to pass an array as a parameter:
CJOCBTI
NZGVOD
\
BSS!
FDIP5IFBSSBZGSPNJOTJEFUIFGVODUJPO\BSS<>^
^
UFTU@BSS

FDIP5IFPSJHJOBMBSSBZJT\UFTU@BSS<>^
NZGVOD\UFTU@BSS<>^
Creating Building Blocks with Functions Chapter 7
[ 129 ]
From the result, you can see that the used array is returned the way it is from the function.
Note that we used ! to get the array inside the function. If you use , it will return the
first array element only:
CJOCBTI
NZGVOD
\
BSS
FDIP5IFBSSBZGSPNJOTJEFUIFGVODUJPO\BSS<>^
^
NZ@BSS

FDIP5IFPSJHJOBMBSSBZ\NZ@BSS<>^
NZGVOD\NZ@BSS<>^
Because we used , it returns only the first array element.
Variable scope
By default, any variable you declare inside a function is a global variable. That means this
variable can be used outside and inside the function without problems.
Check out this example:
CJOCBTI
NZWBS
NZGVOD
\
NZWBS
^
Creating Building Blocks with Functions Chapter 7
[ 130 ]NZGVOD
FDIPNZWBS
If you run this script, it will return , which is the value changed inside the function.
What if you want to declare a variable that is exclusive to the function? This is called a local
variable.
You can declare local variables by using the MPDBM  command like this:
NZGVOD
\
MPDBMNZWBS
^
To ensure that the variable is used only inside the function, let's check out the following
example:
CJOCBTI
NZWBS
NZGVOD
\
MPDBMNZWBS
^
NZGVOD
FDIPNZWBS
If you run this script, it will print , which means that the local version of the variable is
different than the global version.
Returning values from functions
Whenever we have statements that are printed on the screen within the function, we can see
their result. However, lots of times we will want the function to populate a variable within
the script and not display anything. In this case, we use SFUVSO  in the function. This is
especially important when we are gaining input from users. We may prefer the case to
translate the input to a known case to make the condition testing easier. Embedding the
code in a function allows it to be used many times within a script.
Creating Building Blocks with Functions Chapter 7
[ 131 ]The following code shows how we can achieve this by creating the UP@MPXFS  function:
UP@MPXFS

\
JOQVU
PVUQVU
FDIPJOQVU]US<";><B[>
SFUVSOPVUQVU
^
Stepping through the code, we can begin to understand the operation of this function:
JOQVU : This is more for ease than anything else; we assign the first input
parameter to a named variable input.
PVUQVU
FDIPJOQVU]US<";><B[> : This is the main engine of the
function, where the translation from uppercase to lowercase occurs. We pipe the
input to the US command to convert uppercase to lowercase.
SFUVSOPVUQVU : This is how we create the return value.
One use of this function will be within a script that reads the user's input and simplifies the
test to see whether they choose 2 or R. This can be seen in the following extract of code:
UP@MPXFS

\
JOQVU
PVUQVU
FDIPJOQVU]US<";><B[>
SFUVSOPVUQVU
^
XIJMFUSVF
EP
SFBEQ&OUFSDUPDPOUJOVFPSRUPFYJU
3&1-:
UP@MPXFS3&1-:
JG<3&1-:R>UIFO
CSFBL
GJ
EPOF
FDIP'JOJTIFE
Creating Building Blocks with Functions Chapter 7
[ 132 ]Recursive functions
A recursive function is a function that calls itself from inside itself. This function is very
useful when you need to call the function to do something again from inside of it. The most
famous example for that is calculating factorials.
To calculate the factorial of 4, you multiply the number by the descending numbers. You
can do it like this:

The  sign means factorial.
Let's write a recursive function that calculates the factorial of any given number:
CJOCBTI
DBMD@GBDUPSJBM
\
JG<FR>
UIFO
FDIP
FMTF
MPDBMWBS


MPDBMSFT
DBMD@GBDUPSJBMWBS
FDIP

SFT
GJ
^
SFBEQ&OUFSBOVNCFSWBM
GBDUPSJBM
DBMD@GBDUPSJBMWBM
FDIP5IFGBDUPSJBMPGWBMJTGBDUPSJBM

Creating Building Blocks with Functions Chapter 7
[ 133 ]First, we define the function which is called DBMD@GBDUPSJBM  and inside it we check if the
number equals 1 and if so, the function will return 1 because the factorial of 1 equals 1.
Then we decrement the number by one and call the function from inside it and that will call
the function again.
This will continue to happen until it reaches 1 and then the function will exit.
Using functions in menus
In $IBQUFS , Iterating with Loops , we created the NFOVTI  file. Menus are great targets to
use functions, as the DBTF  statement is maintained very simply with single-line entries,
while the complexity can still be stored in each function. We should consider creating a 
function for each menu item. If we copy the previous )0.&CJONFOVTI  to
)0.&CJONFOVTI , we can improve the functionality. The new menu should look like
the following code:
CJOCBTI
"VUIPS!MJLFHFFLT
8FCMJLFHFFLTDPN
4BNQMFNFOVXJUIGVODUJPOT
-BTU&EJUFE"QSJM
UP@MPXFS
\
JOQVU
PVUQVU
FDIPJOQVU]US<";><B[>
SFUVSOPVUQVU
^
EP@CBDLVQ
\
UBSD[WG)0.&CBDLVQUH[\)0.&^CJO
^
TIPX@DBM
\
JG<YVTSCJOODBM>UIFO
DPNNBOEVTSCJOODBMX
FMTF
DPNNBOEVTSCJODBM
GJ
DPNNBOE
^
XIJMFUSVF
EP
Creating Building Blocks with Functions Chapter 7
[ 134 ]DMFBS
FDIP$IPPTFBOJUFNBCPSD
FDIPB#BDLVQ
FDIPC%JTQMBZ$BMFOEBS
FDIPD&YJU
SFBETO
3&1-:
UP@MPXFS3&1-:
DBTF3&1-:JO
BEP@CBDLVQ
CTIPX@DBM
DFYJU
FTBD
SFBEOQ1SFTTBOZLFZUPDPOUJOVF
EPOF
As we can see, we still maintain the simplicity of the DBTF  statement; however, we can
develop the script to add in more complexity through the functions. For example, when
choosing option C for the calendar, we now check to see whether the ODBM  command is
available. If it is, we use ODBM  and use the X option to print the week number. We can see
this in the following screenshot, where we have chosen to display the calendar and install
ODBM :
We can also not be concerned about the Caps Lock  key as the UP@MPXFS  function converts
our selection to lowercase. Over time, it would be very easy to add additional elements to
the functions, knowing that we only affect that single function.
Creating Building Blocks with Functions Chapter 7
[ 135 ]Summary
We are still making progress in leaps and bounds in script writing. I hope these ideas stay
with you and you find the code examples useful. Functions are very important for the ease
of maintenance of your scripts and their ultimate functionality. The easier the scripts are to
maintain, the more likely you are to add improvements over time. We can define functions
at the command line or within scripts but they need to be included in the script before they
are used.
The functions themselves are loaded into memory while the script is running, but as long as
the script is forked and not sourced, they will be released from memory once the script is
finished. We have touched a little upon TFE in this chapter and we will look more at using
the stream editor ( TFE) in the next chapter. The TFE command is very powerful and we can
make good use of it within scripts.
Questions
What is the printed value of the following code?1.
CJOCBTI
NZGVOD
\
BSS
FDIP5IFBSSBZ\BSS<>^
^
NZ@BSS

NZGVOD\NZ@BSS<>^
What is the output of the following code?2.
CJOCBTI
NZWBS
NZGVOD
\
NZWBS
^
FDIPNZWBS
NZGVOD
Creating Building Blocks with Functions Chapter 7
[ 136 ]What is the problem with the following code? And how can you fix it?3.
DMFBO@GJMF\
JT@GJMF
#&'03&
XDM
FDIP5IFGJMFTUBSUTXJUI#&'03&
TFEJCBL	?=TE?E	
"'5&3
XDM
FDIP5IFGJMFJTOPX"'5&3
^
What is the problem with the following code? And how can you fix it?4.
CJOCBTI
NZGVOD
\
BSS!
FDIP5IFBSSBZGSPNJOTJEFUIFGVODUJPO\BSS<>^
^
UFTU@BSS

FDIP5IFPSJHJBOMBSSBZJT\UFTU@BSS<>^
NZGVOD
\UFTU@BSS<>^
Further reading
Please see the following for further reading relating to this chapter:
IUUQUMEQPSH)0850#BTI1SPH*OUSP)0850IUNM
IUUQUMEQPSH-%1BCTIUNMGVODUJPOTIUNM
IUUQTMJLFHFFLTDPNCBTIGVODUJPOT
88
Introducing the Stream Editor
In the previous chapter, we saw that we could make use of TFE to edit files from within our
scripts. The TFE command is the stream editor  (sed) and opens the file line by line to search
or edit the file content. Historically, this goes way back to Unix, where systems may not
have had enough RAM to open very large files. Using TFE was absolutely required to carry
out edits. Even today, we will use TFE to make changes and display data from files with
hundreds and thousands of entries. It is simpler and easier and more reliable than a human
trying to do the same thing. Most importantly, as we have seen, we can use TFE in scripts to
edit the files automatically; no human interaction is required.
We will start by looking at HSFQ  and searching the files for text. The SF in the HSFQ
command is short for regular expression . Even though we are not looking at scripting in
this chapter, we will be covering some very important tools that we can use with scripts. In
the next chapter, we will see the practical implementation of TFE in scripts.
For the moment though, we have enough to deal with and we will cover the following
topics in this chapter:
Using HSFQ  to display text
Understanding the basics of TFE
Other TFE commands
Multiple TFE commands
Introducing the Stream Editor Chapter 8
[ 138 ]Technical requirements
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Using grep to display text
We will be beginning this journey by looking at the HSFQ  command. This will enable us to
grasp some simple concepts of searching through the text before moving onto more
complex regular expressions and editing files with TFE.
Global regular expression print  (grep ), or what we more commonly call the
HSFQ  command, is a command-line tool used to search globally (across all the lines in a file)
and print the result to 45%065 . The search string is a regular expression.
The HSFQ  command is such a common tool that it has many simple examples and numerous
occasions where we can use it each day. In the following sections, we have included some
simple and useful examples with explanations.
Displaying received data on an interface
In this example, we will print just the received data from the FUI  interface.
This is the interface that is my primary network connection. If you are
uncertain of your interface name, you can use the JGDPOGJHB  command
to display all the interfaces and choose the correct interface name on your
system. If JGDPOGJH  is not found, try typing the full
path, TCJOJGDPOGJH .
Using just the JGDPOGJHFUI  command, a heap of data can be printed to the screen. To
show just the packets received, we can isolate the lines that contain 39QBDLFUT  (39 for
received). This is where HSFQ  comes in:
$ ifconfig eth0 | grep "RX packets"
Introducing the Stream Editor Chapter 8
[ 139 ]Using the pipe or vertical bars, we can take the output of the JGDPOGJH  command and send
it to the input of the HSFQ  command. In this case, HSFQ  is searching for a very simple string,
39QBDLFUT . The search string is case sensitive, so we need to get this right or use the J
option with HSFQ  to run the search as case insensitive, as shown in the following example:
$ ifconfig eth0 | grep -i "rx packets"
A case-insensitive search is especially useful when searching for options in
a configuration file, which often have mixed cases.
We can see the result of the initial command in the following screenshot, confirming that we
have been able to isolate just the single line of output, as shown:
Displaying user account data
The local user account database in Linux is the FUDQBTTXE  file and this is readable by all
user accounts. If we want to search for the line that contains our own data, we can use either
our own login name in the search or use parameter expansion and the 64&3  variable. We
can see this in the following command example:
$ grep "$USER" /etc/passwd
In this example, the input to HSFQ  comes from the FUDQBTTXE  file and we search for the
value of the 64&3  variable. Again, in this case, it is a simple text, but it is still the regular
expression, just without any operators.
For completeness, we include the output in the following screenshot:

Introducing the Stream Editor Chapter 8
[ 140 ]We can extend this a little using this type of query as a condition within a script. We can use
this to check whether a user account exists before trying to create a new account. To keep
the script as simple as possible and to ensure that administrative rights are not required,
creating the account will display just the prompt and conditional test in the following
command-line example:
$ bash
$ read -p "Enter a user name: "
$ if (grep "$REPLY" /etc/passwd > /dev/null) ; then
> echo "The user $REPLY exists"
> exit 1
> fi
The HSFQ  search now makes use of the 3&1-:  variable populated by SFBE . If I enter the
name QJ, a message will be displayed and we will exit because my user account is also
called QJ. There is no need to display the result from HSFQ ; we are just looking for a return
code that is either USVF  or GBMTF . To ensure that we do not see any unnecessary output if
the user is in the file, we redirect the output from HSFQ  to the special device file EFWOVMM .
If you want to run this from the command line, you should start a new bash shell first. You
can do this by simply typing CBTI . In this way, when the FYJU  command runs, it will not
log you out but close the newly opened shell. We can see this happening and the results
when specifying an existing user in the following screenshot:
Listing the number of CPUs in a system
Another really useful feature is that HSFQ  can count the matching lines and not display
them. We can use this to count the number of CPUs or CPU cores we have on a system.
Each core or CPU is listed with a name in the QSPDDQVJOGP  file. We can then search for
the text OBNF  and count the output; the D option used is shown in the following example:
$ grep -c name /proc/cpuinfo
Introducing the Stream Editor Chapter 8
[ 141 ]My CPU has four cores, as shown in the following output:
If we use the same code on another PC Model B that has a single core, we will see the
following output:
We can again make use of this in a script to verify that enough cores are available before
running a CPU-intensive task. To test this from the command line, we can use the following
code, which we execute on a PC with just a single core:
$ bash
$ CPU_CORES=$(grep -c name /proc/cpuinfo)
$ if (( CPU_CORES < 4 )) ; then
> echo "A minimum of 4 cores are required"
> exit 1
> fi
We only run CBTI  at the start to ensure that we are not logged out of the system with the
FYJU  command. If this was in a script, this would not be required, as we would exit the
script and not our shell session.
By running this on the Model B that has a single core, we can see the results of the script
and also the indication that we do not have the required number of cores:

Introducing the Stream Editor Chapter 8
[ 142 ]If you had a requirement to run this check in more than one script, then you could create a
function in a shared script and source the script holding the shared functions within the
script that needs to be checked:
GVODUJPODIFDL@DPSFT\
<[>3&2@$03&4
$16@$03&4
HSFQDOBNFQSPDDQVJOGP
JG

$16@$03&43&2@$03&4UIFO
FDIP"NJOJNVNPG3&2@$03&4DPSFTBSFSFRVJSFE
FYJU
GJ
^
If a parameter is passed to the function, then it is used as the required number of cores;
otherwise, we set the value to  as the default. If we define this as a function in the shell on
the Model B PC and display the details with the UZQF  command, we should see this as
shown in the following screenshot:
If we run this on a single-core system and specify the requirement of just a single core, we
will see that there is no output when we meet the requirement. If we do not specify the
requirement, then it will default to  cores and we will fail to meet the requirement and we
will exit the shell.
We can see the output of the function when run with the argument of , and then without
arguments, in the following screenshot:

Introducing the Stream Editor Chapter 8
[ 143 ]We can see how useful even the basics of HSFQ  can be within the scripts and how we can
use what we have learned to start creating usable modules to add to our scripts.
Parsing CSV files
We will now look at creating a script to parse or format a CSV file. The formatting of the file
will add new lines, tabs, and color to the output, so that it is more readable. We can then use
HSFQ  to display single items from the CSV file. The practical application here is a catalog
system based on the CSV files.
The CSV file
The CSV file, or list of comma-separated values, will come from the file named UPPMT  that
we have in a current directory. This is a catalog of products that we sell. The file content is
shown in the following output:
drill,99,5
hammer,10,50
brush,5,100
lamp,25,30
screwdriver,5,23
table-saw,1099,3
This is just a simple demonstration, so we don't expect too much data, but each item in the
catalog consists of the following:
Name
Price
Units in stock
We can see that we have a drill that costs $99 and we have five units in stock. If we list the
file with DBU, it is not very friendly; however, we can write a script to display the data in a
more appealing way. We can create a new script called )0.&CJOQBSTFDTWTI :
CJOCBTI
0-%*'4*'4
*'4
XIJMFSFBEQSPEVDUQSJDFRVBOUJUZ
EP
FDIPF=<NQSPEVDU=
=<N=O=
1SJDF=UQSJDF=O=
Introducing the Stream Editor Chapter 8
[ 144 ]2VBOUJUZ=URVBOUJUZ=O
EPOF
*'40-%*'4
Let's work through this file and look at the pertinent elements:
Element Meaning
0-%*'4*'4The *'4 variable stores the file separator and this is normally a
white space character. We can store the old *'4 so that we can
restore it later at the end of the script, ensuring that we return
the same environment once the script is complete, no matter
how the script is run.
*'4We set the separator to a comma to match what we need with a
CSV file.
XIJMFSFBEQSPEVDU
QSJDFRVBOUJUZWe enter a XIJMF  loop to populate three variables that we need:
QSPEVDU , QSJDF , and RVBOUJUZ . The XIJMF  loop will read the
input file, line by line, and populate each of the variables.
FDIPThe FDIP  command displays the product name in blue with
double underscores underneath. The other variables are printed
on new lines and tabbed in.
EPOFThis is where we read the input file, which we pass as an
argument to the script.
The script is shown in the following screenshot:

Introducing the Stream Editor Chapter 8
[ 145 ]We can execute the script with the UPPMT  catalog file located in the current directory using
the following command:
$ parsecsv.sh tools
To look at how this will display, we can view the partial output in the following screenshot:
We are now starting to get the idea that we have a lot of power at the command line to
format files in a more readable way and a plain text file does not need to be plain.
Isolating catalog entries
If we need to search for one entry, then we need more than just one line. The entry is in
three lines. So, if we search for the hammer, we need to go to the hammer line and the two
lines that follow. We do this by using the " option to HSFQ , which is short for after. We
need to display the matching line and two lines after. This will be expressed by the
following code:
$ parsecsv.sh tool | grep -A2 hammer
This is displayed in the following screenshot:

Introducing the Stream Editor Chapter 8
[ 146 ]Understanding the basics of sed
Having built a little foundation, we can now start to look at some of the operations of TFE.
The commands will be supplied with most Linux systems and are core commands.
We will dive directly into some simple examples:
$ sed 'p' /etc/passwd
The Q operator will print the matched pattern. In this case, we have not specified a pattern
so we will match everything. Printing the matched lines without suppressing 45%065  will
duplicate lines. The result of this operation is to print all the lines in the QBTTXE  file twice.
To print the modified lines only, we use the O option:
$ sed -n 'p' /etc/passwd
Brilliant!! We have just reinvented the DBU command. We can now specifically work with
just a range of lines:
$ sed -n '1,3 p ' /etc/passwd
Now we have reinvented the IFBE  command, but we can also specify the range in a
regex pattern to recreate the HSFQ  command:
$ sed -n '/^root/ p' /etc/passwd
We can see this demonstrated in the following screenshot:
Note that the caret character ( ?) means the beginning of the line, which means the line must
start with the word SPPU . Don't worry; we will explain all these regex characters in a
separate chapter.
Introducing the Stream Editor Chapter 8
[ 147 ]The substitute command
We have seen the Q command for printing the pattern space. The Q is actually a flag for the
TVCTUJUVUF  command T.
The TVCTUJUVUF  command is written like this:
$ sed s/pattern/replacement/flags
There are three common flags used with the TVCTUJUVUF  command:
Q: Print the original content
H: Global replacement for all occurrences
X: Filename: send results to a file
We will now look at the TVCTUJUVUF  command or T. With this command, we can replace
one string with another. Again, by default, we send the output to the 45%065  and do not
edit the file.
To replace the default shell of the user QJ, we can use the following command:
sed -n ' /^pi/ s/bash/sh/p ' /etc/passwd
We continue the earlier instance using the Q command to print the matched pattern and use
the O option to suppress 45%065 . We search for lines beginning with QJ. This represents
the username. We then issue the T command to substitute text in those matched lines. This
takes two arguments: the first is the text to search for and the second represents the text
used to replace the original. In this case, we look for CBTI  and replace it with TI. This is
simple and does work but it may not be reliable in the long term. We can see the output in
the following screenshot:

Introducing the Stream Editor Chapter 8
[ 148 ]We must emphasize that, currently, we are not editing the file and are just displaying it to
the screen. The original QBTTXE  file remains untouched and we can run this as a standard
user. I mentioned in the previous example that the search may be less than reliable as the
string we are searching for is CBTI . This is very short and perhaps it can be included
elsewhere on a matched line. Potentially, someone's last name may be 5BCBTI , which
includes the string CBTI . We can extend the search to look for CJOCBTI  and replace it
with CJOTI . However, this introduces another problem: the default delimiter is the
forward slash, so we will have to escape each forward slash we use in the search and
replace strings, which is as follows:
sed -n ' /^pi/ s/\/bin\/bash/\/usr\/bin\/sh/p ' /etc/passwd
This is an option but it is not a tidy option. A better solution is to know that the first
delimiter we use defines the delimiters. In other words, you can use any character as a
delimiter. Using the ! symbol may be a good idea in this scenario, as it does not appear in
either the search or the replace string:
sed -n ' /^pi/ s@/bin/bash@/usr/bin/sh@p ' /etc/passwd
We now have a more reliable search and a readable command line to work with, which is
always a good thing. We replace just the first occurrence on each line of CJOCBTI  with
CJOTI . If we need to replace more than the first occurrence, we add the H command, for
global, at the end:
sed -n ' /^pi/ s@bash@sh@pg ' /etc/passwd
In our case, it is not required but it is good to know.
Global replacement
Let's assume that we have the following sample file:
)FMMPTFEJTBQPXFSGVMFEJUJOHUPPM*MPWFXPSLJOHXJUITFE
*GZPVNBTUFSTFEZPVXJMMCFBQSPGFTTJPOBMPOF
Let's try to use TFE against this file:
$ sed 's/sed/Linux sed/' myfile
Introducing the Stream Editor Chapter 8
[ 149 ]Here, we use TFE to replace the word TFE with -JOVYTFE :
If you check the result carefully, you will notice that TFE modified the first word of each
line only.
This may not be what you want if you want to replace all occurrences.
Here comes the H flag.
Let's use it and see the results again:
$ sed 's/sed/Linux sed/g' myfile
Now all occurrences are modified.
You can port these modifications to a file using the X flag:
$ sed 's/sed/Linux sed/w outputfile' myfile
Also, you can limit the number of occurrences from the same line, so we can modify the first
two occurrences from each line only like this:
$ sed 's/sed/Linux sed/2' myfile
So, if there is a third occurrence, it will be neglected.
Introducing the Stream Editor Chapter 8
[ 150 ]Limiting substitution
We saw how the H flag modifies all occurrences in the same line and this goes for the entire
file lines.
What if we want to limit our edits to a specific line? Or a specific line range?
We can specify the ending line or the line range like this:
$ sed '2s/old text/new text/' myfile
The preceding command will only modify the second line of the file. The following
command will modify only the third to the fifth lines:
$ sed '3,5s/old text/new text/' myfile
The following command will modify from the second line to the end of the file:
$ sed '2,$s/old text/new text/' myfile
Editing the file
Using the X flag, we can write our edits to a file, but what if we want to edit the file itself?
We can use the J option. We will need permissions to work with the file but we can make
a copy of the file to work with, so we don't harm any system file or require additional
access.
We can copy the QBTTXE  file locally:
$ cp /etc/passwd "$HOME"
$ cd
We finish with the DE command to ensure that we are working in the IPNF  directory and
the local QBTTXE  file.
The J option is used to run an in-place update. We will not need the O option or the Q
command when editing the file. As such, the command is as simple as the following
example:
$ sed -i ' /^pi/ s@/bin/bash@/bin/sh/ ' $HOME/passwd
Introducing the Stream Editor Chapter 8
[ 151 ]There will be no output to the command but the file will now reflect the change. The
following screenshot shows the command usage:
We should make a backup before we make the change by appending a string directly after
the J option and without any spaces. This is shown in the following example:
$ sed -i.bak ' /^pi/ s@/bin/bash@/bin/sh/ ' $HOME/passwd
If we want to see this, we can reverse the search and replace strings:
$ sed -i.bak ' /^pi/ s@/bin/sh@/bin/bash/ ' $HOME/passwd
This will set the local QBTTXE  file to be the same as it was before and we will have a
QBTTXECBL  with the previous set of changes. This keeps us safe with a rollback option if
we need it.
Other sed commands
TFE offers a lot of commands that can be used to insert, change, delete, and transform text
with ease. Let's see some examples of how to use these commands with TFE.
The delete command
You can use the EFMFUF  command E to delete lines or a range of lines from your
stream. The following command will delete the third line from the stream: 
$ sed '3d' myfile
The following command will delete the third to the fifth line from the stream:
$ sed '3,5d' myfile
 This command will delete from the fourth line to the end of the file:
$ sed '4,$d' myfile
Introducing the Stream Editor Chapter 8
[ 152 ]Note that the deletion happens only to the stream, not the actual file. So if you want to
delete from the actual file, you can use the J option:
$ sed -i '2d' myfile #Permenantly delete the second line from the file
The insert and append commands
The insert, J, and append, B, commands work the same way with just a slight difference.
The JOTFSU  command inserts the specified text before the specified line or pattern.
The BQQFOE  command inserts the specified text after the specified line or pattern.
Let's see some examples.
Our sample 02 file will be like this:
'JSTUMJOF
4FDPOEMJOF
5IJSEMJOF
'PVSUIMJOF
To insert a line, you need to use the insert command J like this:
$ sed '2i\inserted text' myfile
To append a line, you need to use the append command B like this:
$ sed '2a\inserted text' myfile
Look at the result and check the inserted line position:

Introducing the Stream Editor Chapter 8
[ 153 ]The change command
We saw how to substitute occurrences using the TVCTUJUVUF  command T. So what is the
DIBOHF  command and how is it different?
The DIBOHF  command, D, is used for changing the entire line.
To change a line, you can use the DIBOHF  command like this:
$ sed '2c\modified the second line' myfile
We replaced the second line with a new line.
The transform command
The USBOTGPSN  command is used to replace any letter or a number with another, for
example, capitalizing letters or transforming numbers into different numbers.
It works like the US command.
You can use it like this:
$ sed 'y/abc/ABC/' myfile
The transformation applies to the entire stream and can't be limited.
Introducing the Stream Editor Chapter 8
[ 154 ]Multiple sed commands
In all the previous examples, we only applied one TFE command to our stream. What about
running multiple TFE commands?
You can do that by using the F option and separating the commands with a semicolon like
this:
$ sed -e 's/First/XFirst/; s/Second/XSecond/' myfile
Also, you can enter every command on a separate line and you will achieve the same result:
$ sed -e '
> s/First/XFirst/
> s/Second/XSecond/' myfile
The TFE command offers great flexibility; if you use it well, you will gain a lot of power.
Summary
Another great chapter that you have firmly under your belt and I hope it was really useful
to you. Although we wanted to concentrate on using TFE, we started with how powerful
HSFQ  can be, both inside and outside our scripts. Although we have only just touched
on TFE, we will start extending this in the next chapter, where we will expand upon what
we have learned.
Also, we learned how to substitute text and how to limit and globalize the substitution and
how to save the editing stream using J.
Introducing the Stream Editor Chapter 8
[ 155 ]We learned how to insert, append, delete, and transform text using TFE.
Finally, we learned how to run multiple TFE commands using the F option.
In the next chapter, we will learn how to automate Apache Virtual Hosts, how to create new
virtual hosts automatically, and other cool stuff. The workhorse of all these operations will
be TFE and TFE scripts.
Questions
Suppose you have a file with the following content:1.
)FMMPTFEJTBQPXFSGVMFEJUJOHUPPM*MPWFXPSLJOHXJUITFE
*GZPVNBTUFSTFEZPVXJMMCFBQSPGFTTJPOBMPOF
And suppose you use the following command:
$ sed 's/Sed/Linux sed/g' myfile
How many lines will be substituted?
Suppose you have the same file that was used in the previous question and you2.
use the following command:
$ sed '2d' myfile
How many lines will be deleted from the file?
What is the location of the inserted line in the following example?3.
$ sed '3a\Example text' myfile
Suppose you have the same previous sample file and you run the following4.
command:
$ sed '2i\inserted text/w outputfile' myfile
How many lines will be saved to the output file?
Introducing the Stream Editor Chapter 8
[ 156 ]Further reading
Please see the following for further reading relating to this chapter:
IUUQTXXXHOVPSHTPGUXBSFTFENBOVBMTFEIUNM
IUUQTMJOVYEJFOFUNBOTFE
99
Automating Apache Virtual
Hosts
Now that we have seen a little of the stream editor  (sed), we can put this knowledge into
practice. In $IBQUFS , Introducing the Stream Editor , we familiarized ourselves with some of
the capabilities of TFE; however, this represented just a small amount of the power enclosed
in the editor. In this chapter, we are going to exercise TFE a little more and expose ourselves
to some practical uses of the tool, especially when using our bash scripts.
In this journey, we will use TFE to help us automate the creation of Apache name-based
Virtual Hosts. The Apache hosts are practical users of the TFE that we demonstrated but,
more importantly, we will use TFE to search for selected lines in the main configuration. We
will then uncomment those lines and save them as a template. Having created the template,
we will create new configurations from it. The concept that we demonstrate with Apache
can be applied in many different situations.
We will find that using TFE in our shell scripts will allow us to easily extract template data
from the main configuration and adjust to the needs of the virtual host. In this way, we will
be able to extend the knowledge of both TFE and shell scripting. In this chapter, we are
going to cover the following topics:
Apache name-based Virtual Hosts
Automating virtual host creation
Automating Apache Virtual Hosts Chapter 9
[ 158 ]Technical requirements
You will need the following:
CentOS 7.x machine
Apache 2.4.x web server installed
You can install Apache as follows:
$ sudo yum install httpd
Then you can start the web server:
$ systemctl start httpd
You can ensure that the service is already running by checking the status as follows:
$ systemctl status httpd
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Apache name-based Virtual Hosts
For this demonstration, we will be working with the IUUQEDPOG  file from an Apache 2.4
HTTPD server taken from a CentOS 7.x host. To be perfectly honest, we are far more
interested in the configuration file, as Red Hat or CentOS supply it, than the actual
configuration changes that we will make. The file will be available for download from the
code bundle of the chapter. Our purpose is to learn how we can extract data from the
system-supplied file and create a template from it. We can apply this to Apache
configuration files or any other text data file. It is the methodology we are looking at, not
the actual result.
Automating Apache Virtual Hosts Chapter 9
[ 159 ]To have some understanding of what we are trying to do, we must first look at the
FUDIUUQEDPOGIUUQEDPOG  file, that is, CentOS, Red Hat Enterprise Linux, or
Scientific Linux. The following screenshot shows the virtual host section of the file that we
are interested in:
Looking at these lines, we can see that they are commented and this is all a part of a
monolithic IUUQEDPOG . While creating virtual hosts, we normally prefer separate
configurations for each of our potential virtual hosts. We need to be able to extract this data
from the main file and at the same time uncomment it. We can then save this uncommented
data as a template.
Using this template, we will create new configuration files that represent different named
IPTUT  that we need to have running on one instance of Apache. This enables us to host
TBMFTFYBNQMFDPN  and NBSLFUJOHFYBNQMFDPN  on a single server. Both sales and
marketing will have their own configuration and websites, independent from each other.
Additionally, it will also be very easy to add additional sites that we need with the template
we create. It becomes the task of the main web server to read the incoming HTTP header
requests to the server and direct them to the correct site based on the domain name used.
Our first task then will be to extract the data present between the opening and closing
7JSUVBM)PTU  tags, uncomment it, and save it to a template. This will only need to be done
once and will not be a part of our main script to create the virtual hosts.
Creating the virtual host template
As we are not going to test the virtual hosts we create, we will make a copy of the
IUUQEDPOG  file and work with that locally in our IPNF  directory. This is good practice
while developing the scripts so as not to impact the working configuration. The
IUUQEDPOG  file that I am working with should be able to be downloaded with other script
resources referred to in the script from the publisher. Alternatively, you can copy it from
your CentOS host with Apache installed. Make sure that the IUUQEDPOG  file is copied to
your IPNF  directory and that you are working in your IPNF  directory.
Automating Apache Virtual Hosts Chapter 9
[ 160 ]First steps
The very first step in creating the template is to isolate the lines that we need. In our case,
this will be the lines included in the sample virtual host definition that we saw in the earlier
screenshot. This includes the opening and closing tag for the 7JSUVBM)PTU  and everything
in between. We can use line numbers for this; however, this will probably not be reliable, as
we will need to assume that nothing has changed in the file for the line numbers to be
consistent. For completeness, we will show this before moving onto a more reliable
mechanism.
First, we will remind ourselves of how we can print the whole file with TFE. This is
important, as in the next step we will filter the display and show only the lines that we
want:
$ sed -n ' p ' httpd.conf
The O option is used to suppress the standard output and the TFE command within the
quotes is Q; it is used to display the pattern match. As we have not filtered anything here,
the matched pattern is the complete file. If we were to use line numbers to filter, we could
add line numbers easily with TFE, as shown in the following command:
$ sed = httpd.conf
From the following screenshot, we can see that, in this system, we need to work with just
lines  to ; however, I do stress again that these numbers may vary from file to file:
Isolating lines
To display these lines encased with the tags, we can add a number range to TFE. This is
easily achieved by adding those numbers to TFE, as shown in the following command:
$ sed -n '355,361 p ' httpd.conf
Automating Apache Virtual Hosts Chapter 9
[ 161 ]With the range of lines specified, we have been able to easily isolate the lines that we
required, and the only lines that are now displayed are those of the virtual host definition.
We can see this in the following screenshot, which displays both the command and the
output:
The issue that we face while hardcoding in the line numbers is that we lose flexibility. These
line numbers relate to this file and maybe only this file. We will always need to check the
correct line numbers in the file that relate to the file we are working with. This could be an
issue if the lines are not conveniently at the end of the file and we have to scroll back to try
and locate the correct line numbers. To overcome these issues, instead of using line
numbers, we can implement a search for the opening and closing tags directly:
$ sed -n '/^#<VirtualHost/,/^#<\/VirtualHost/p' httpd.conf
We are no longer using the starting number and ending number but the more reliable
starting regular expression and closing regular expression. The opening regular expression
looks for the line that begins with 7JSUVBM)PTU . The ending regular expression is
searching for the closing tag. However, we need to protect the 7JSUVBM)PTU  with an
escape character. By looking at the end of the regular expression, we see that it translates to
lines that begin with =7JSUVBM)PTU , with the escaped forward slash.
If you recall from $IBQUFS , Introducing the Stream Editor , we specify the
lines that begin with a specified character by using the carat ( ?).
Automating Apache Virtual Hosts Chapter 9
[ 162 ]By looking at the following screenshot, we can now isolate the required lines more reliably
and without knowing the line numbers. This is more desirable across edited files, which
will differ in their line numbering:
sed script files
Isolating the lines was only the first step! We still have to uncomment the lines and then
save the result as a template. Although we can write this as one single TFE command string,
we can already see that it will be awkwardly long and difficult to read and edit. Thankfully,
the TFE command does have the option to read its commands from an input file, often
known as a script. We use the G option with TFE to specify the file we want to read as our
control.
We have already seen that we can isolate the correct lines from the file. So, the first line of
the script configures the lines that we will work with. We implement the brace brackets \^
to define a code block immediately after the selected lines.
A code block is one or more commands that we want to run on a given selection.
In our case, the first command will be to remove the comment and the second command
will be to write the pattern space to a new file. The TFE script should look like the following
example:
?7JSUVBM)PTU?=7JSUVBM)PTU\
T?
XUFNQMBUFUYU
^
We can save this file as )0.&WITFE .
Automating Apache Virtual Hosts Chapter 9
[ 163 ]In the first line, we select the lines to work with, as we have previously seen, and then open
the code block with the left brace. In line 2, we use the TVCTUJUVUF  command, T. This looks
for lines that start with a comment or . We replace the comment with an empty string.
There are no characters or spaces between the middle and end forward slash. In English, we
are uncommenting the line but, to the code, this is replacing the  with an empty string. The
final line of code uses the write command, X, to save this to UFNQMBUFUYU . To help you see
this, we have included the following screenshot of the WITFE  file:
We can see all of our efforts come to fruition now by ensuring that we are in the same
directory as the IUUQEDPOG  and WITFE  files that are executing the following command:
$ sed -nf vh.sed httpd.conf
We have now created the UFNQMBUFUYU  file within our working directory. This is the
isolated uncommented text from the IUUQEDPOG  file. In simple terms, we have extracted
the seven correct lines from over 350 lines of text in milliseconds, removed the comment,
and saved the result as a new file. The UFNQMBUFUYU  file is displayed in the following
screenshot:
Now we have a template file that we can begin to work with to create virtual host
definitions. Even though it's Apache that we have been looking at, the same idea of
uncommenting the text or removing the first character of selected lines can apply to many
situations, so take this as an idea of what TFE can do.
Automating Apache Virtual Hosts Chapter 9
[ 164 ]Automating virtual host creation
After having created the template, we can now use this to create virtual host configurations.
In the simplest terms, we need to replace the EVNNZIPTUFYBNQMFDPN  URL with
the TBMFTFYBNQMFDPN  or NBSLFUJOHFYBNQMFDPN  URL. Of course, we have to also
create the %PDVNFOU3PPU  directory, the directory where the web pages will be, and also
add some basic content. When we use a script to run through the process, nothing will be
forgotten and the edits will be accurate every time. The basics of the script will be as
follows:
CJOCBTI
8&#%*3XXXEPDT
$0/'%*3FUDIUUQEDPOGE
5&.1-"5&)0.&UFNQMBUFUYU
<E$0/'%*3>]]NLEJSQ$0/'%*3
TFETEVNNZIPTUFYBNQMFDPN5&.1-"5& $0/'%*3DPOG
NLEJSQ8&#%*3
FDIP/FXTJUFGPS 8&#%*3JOEFYIUNM
We can ignore the shebang in the first line; we should know this by now. We can start our
explanation on line 2 of the script:
Line Meaning
8&#%*3XXXEPDTWe initialize the 8&%*3  variable that we
store in the path to the directory that will
hold the different websites.
$0/'%*3FUDIUUQEDPOGEWe initialize the $0/'%*3  variable that we
will use to store the newly created virtual
host configuration file.
5&.1-"5&)0.&UFNQMBUFUYUWe initialize the variable that we will use
for the template. This should point to the
path of our template.
<E$0/'%*3>]]NLEJSQ
$0/'%*3On a working &- host, this directory will
exist and is included in the main
configuration. If we are running this as a
pure test, then we can create a directory to
prove that we can create the correct
configuration within the target directory.
Automating Apache Virtual Hosts Chapter 9
[ 165 ]TFETEVNNZIPTUFYBNQMFDPN
5&.1-"5& $0/'%*3DPOGThe TFE command works as an engine in
the script, running the search and replace
operations. Using the substitute command
in TFE, we search for the dummy text and
replace it with the argument passed to the
script.
NLEJSQ8&#%*3Here, we create the correct subdirectory to
house the websites for the new virtual
host.
FDIP/FXTJUFGPS 
8&#%*3JOEFYIUNMIn this final step, we create a basic holding
page for the website.
We can create this script as )0.&CJOWIPTUTI . Don't forget to add the execute
permission. This is illustrated in the following screenshot:
To create the sales virtual host and web page, we can run the script as shown in the
following example. We will be running the script directly as the root user. Alternatively,
you may choose to make use of the TVEP  command within the script:
# vhost.sh sales.example.com
Automating Apache Virtual Hosts Chapter 9
[ 166 ]We can now see how easily we can create virtual hosts using a well-crafted script. The
configuration file for the virtual host will be created in the FUDIUUQEDPOGE  directory
and will be named TBMFTFYBNQMFDPNDPOG . The file will look similar to the following
screenshot:
The website content must have been created in the XXXEPDTTBMFTFYBNQMFDPN
directory. This will be a simple holding page that proves the point that we can do this from
the script. Using the following command, we can list the content or the base directory that
we use to house each site:
$ ls -R /www/docs
The 3 option allows for the recursive listing. We have used the XXXEPDT  directory
purely as this is set in the original virtual host definition that we extracted. You may prefer
to use WBSXXX  or something similar if working in a live environment rather than creating
the new directory at the root of your filesystem. It would be a simple matter of editing the
template that we created and that too could be done with TFE at the time of template
creation.
Prompting for data during site creation
We can now use the script to create the virtual hosts and the content but we have not
allowed for any customization other than the virtual hostname. Of course, this is important.
After all, it is this virtual hostname that is used in the configuration itself as well as in
setting the website directory and the configuration file name.
It is possible that we could allow additional options to be specified during the virtual host
creation. We will use TFE to insert the data as required. The TFE command J is used to
insert data before the selection and B to append after the selection.
Automating Apache Virtual Hosts Chapter 9
[ 167 ]For our example, we will add a host restriction to allow only the local network access to the
website. We are more interested in inserting data into the file rather than what we are doing
with the specific HTTP configuration file. Within the script, we will be adding SFBE
prompts and inserting a %JSFDUPSZ  block into the configuration.
To try and explain what we are trying to do, we should see something similar to the
following when executing the script. You can see from the text that we are creating this for
the marketing site and adding in restrictions as to who can access the site:
As you can see, we can ask two questions but, if needed, more of them can be added to
support customization, the idea being that the additional customization should be accurate
and reliable in the same way as the script creation was. You may also choose to elaborate
the questions with sample answers, so that the user knows how the network address should
be formatted.
To aide script creation, we will copy the original WIPTUTI  to WIPTUTI . We can tidy up a
few items in the script to allow for easier expansion and then add in the additional prompts.
The new script will look similar to the following code:
CJOCBTI
8&#%*3XXXEPDT
$0/'%*3FUDIUUQEDPOGE
$0/''*-&$0/'%*3DPOG
5&.1-"5&)0.&UFNQMBUFUYU
<E$0/'%*3>]]NLEJSQ$0/'%*3
TFETEVNNZIPTUFYBNQMFDPN5&.1-"5& $0/''*-&
NLEJSQ8&#%*3
FDIP/FXTJUFGPS 8&#%*3JOEFYIUNM
SFBEQ%PZPVXBOUUPSFTUSJDUBDDFTTUPUIJTTJUFZO
<\3&1-:??^	O	>FYJU
SFBEQ8IJDIOFUXPSLTIPVMEXFSFTUSJDUBDDFTTUP/&5803,
TFEJ=7JSUVBM)PTU J%JSFDUPSZ8&#%*3 =
=O0SEFSBMMPXEFOZ=
=O"MMPXGSPN=
=O"MMPXGSPN/&5803,=
=O%JSFDUPSZ $0/''*-&
Automating Apache Virtual Hosts Chapter 9
[ 168 ]Please note that we are not running too many checks in the script. This is
to keep our focus on the elements that we are adding rather than a robust
script. In your own environment, once you have the script working the
way you want, you may need to implement more checks to ensure script
reliability.
As you can see, we have a few more lines. The 8&#%*3  variable has been adjusted to contain
the full path to the directory and, in a similar way, we have added a new variable
$0/''*-& , so that we can make a reference to the file directly. If the answer to the first
prompt is O and the user wants no additional customization, the script will exit. If they
answer anything other than O for no, the script will continue and prompt the network to
grant access. We can then use TFE to edit the existing configuration and insert the new
EJSFDUPSZ  block. This will default to deny access but allow access from the MPDBMIPTU
and /&5803,  variables. We refer to the MPDBMIPTU  as   in the code.
To simplify the code for better understanding, the pseudo-code will look like the following
example:
$ sed -i "/SearchText/i NewText <filename>
Here 4FBSDI5FYU  represents the line in the file before which we want to insert our text.
Also, /FX5FYU  represents the new line or lines that will be added before the 4FBSDI5FYU .
The J command directly following the 4FBSDI5FYU  dictates that we are inserting text.
Using the B command to append will mean that the text we add will be added after the
4FBSDI5FYU .
We can see the resulting configuration file for NBSLFUJOHFYBNQMFDPN , as we have
created it with the additional %JSFDUPSZ  block added in the following screenshot:

Automating Apache Virtual Hosts Chapter 9
[ 169 ]We can see that we have added the new block above the closing 7JSUVBM)PTU  tag. In the
script, this is the 4FBSDI5FYU  that we use. The %JSFDUPSZ  block we add replaces the
/FX5FYU  in the pseudo-code. When we look at it, it appears more complex as we have
embedded the new lines with =O and formatted the file for easier reading with the line
continuation character =. Again, we have to emphasize that this edit is easy and accurate
once the script is created.
For completeness, we include the following screenshot of the script WIPTUTI :
Summary
In this chapter, we have seen how we can extend TFE into some very cool scripts that have
allowed us to extract data from files, uncomment selected lines, and write new
configurations. Not stopping at that, we also saw how we could use TFE with script that
inserts new lines into existing files. I think that TFE will very quickly become your friend
and we have created some powerful scripts to support the learning experience.
You may already know this but TFE has a big brother, BXL. In the next chapter, we are
going to see how we can use BXL to extract data from files.
Automating Apache Virtual Hosts Chapter 9
[ 170 ]Questions
How can you print line number  from an Apache configuration file? 1.
How can you change the Apache default port  to   using TFE? 2.
Further reading
Please see the following for further reading relating to this chapter:
IUUQTIUUQEBQBDIFPSHEPDT
IUUQTIUUQEBQBDIFPSHEPDTWIPTUTFYBNQMFTIUNM
110
AWK Fundamentals
The stream editor is not alone in its family and has a big brother, AWK. In this chapter, we
will run through the basics of AWK and explore the power of the AWK programming
language. We will learn why we need and love AWK and how we can make use of some of
the basic features before we start putting AWK to practical use in the next two chapters. As
we work our way through this, we will cover the following topics:
The history behind AWK
Displaying and filtering content from files
AWK variables
Conditional statements
Formatting output
Further filtering to display users by UID
AWK control files
Technical requirements
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
AWK Fundamentals Chapter 10
[ 172 ]The history behind AWK
The BXL command is a command suite mainstay in both UNIX and Linux. The UNIX
BXL command was first developed by Bell Labs in the 1970s and is named after the
surnames of the main authors: Alfred Aho, Peter Weinberger, and Brian Kernighan. The
BXL command allows access to the AWK programming language, which is designed to
process data within text streams.
There are many implementations of AWK:
gawk : Also known as GNU AWK, it is a free version of AWK and used by many
developers; we will use it in this book.
mawk : Another implementation made by a guy named Mike Brennan. This
implementation only includes a few gawk features; it was designed for speed and
performance.
tawk : Or Thompson AWK, is an implementation that works on Solaris, DOS, and
Windows.
BWK awk : Also known as nawk, it is used by OpenBSD and macOS.
Note that the BXL interpreter that we will use in this book is HBXL  but there is a symbolic
link for it with the name BXL. So BXL and HBXL  are the same command.
You can ensure this by listing the BXL binary to see where it points to:
To demonstrate the programming language that is provided with BXL, we should create a
)FMMP8PSME  program. We know this is compulsory for all languages:
$ awk 'BEGIN { print "Hello World!" }'
Not only can we see that this code will print the ubiquitous hello message, we can also
generate header information with the #&(*/  block. Later, we will see that we
can create summary information with an &/% code block by allowing for a main
code block.
AWK Fundamentals Chapter 10
[ 173 ]We can see the output of this basic command in the following screenshot:
Displaying and filtering content from files
Now, of course we all want to be able to print a little more than just )FMMP8PSME . The BXL
command can be used to filter content from files and, if needed, very large files. We should 
begin by printing the complete file before filtering it. In this way, we will get a feel for the
syntax of the command. Later, we will see how we can add this control information into
BXL files to ease the command line. Using the following command, we will print all the lines
from the FUDQBTTXE  file:
$ awk ' { print } ' /etc/passwd
This is equivalent to using the  variable with the QSJOU  statement:
$ awk ' { print $0 }' /etc/passwd
AWK provides us with some ready-to-use variables to extract data such as:
 for the entire line
 for the first field
 for the second field
 for the third field and so on
However, we will need to specify that in this file the field separator used is a colon, since it's
the field separator in FUDQBTTXE  file. The BXL default delimiter is a space or any amount
of spaces or tabs and newlines. There are two ways to specify the input delimiter; these are
displayed in the following examples.
The first example is easy and simple to use. The ' option works well, especially where we
do not need any additional header information:
$ awk -F":" '{ print $1 }' /etc/passwd
AWK Fundamentals Chapter 10
[ 174 ]We could also do this within the #&(*/  block; this is useful when we want to use the #&(*/
block to display header information:
$ awk ' BEGIN { FS=":" } { print $1 } ' /etc/passwd
We can see this clearly in the preceding example, in which we named the #&(*/  block and
all of the code within it is corralled by the brace brackets. The main block has no name and
is enclosed within the brace brackets.
After seeing the #&(*/  block and the main code blocks, we will now look at the &/% code
block. This is often used to display summary data. For example, if we want to print the total
lines in the QBTTXE  file, we can make use of the &/% block. The code with the #&(*/  and
&/% blocks is processed just once, whereas the main block is processed for each line. The
following example adds to the code we have written so far to include the total line count:
$ awk ' BEGIN { FS=":" } { print $1 } END { print NR } ' /etc/passwd
The BXL internal variable /3 maintains the number of processed lines. If we want, we can
add some additional text to this. This can be used to annotate the summary data. We can
also make use of the single quotes that are used with the AWK language; they will allow us
to spread the code across multiple lines. Once we have opened the single quotes, we can
add newlines to the command line right until we close the quote. This is demonstrated in
the next example where we have extended the summary information:
$ awk ' BEGIN { FS=":" }
> { print $1 }
> END { print "Total:",NR } ' /etc/passwd
If we do not wish to end our AWK experience here, we can easily display a running line
count with each line, as well as the final total. This is shown in the following example:
$ awk ' BEGIN { FS=":" }
> { print NR,$1 }
> END { print "Total:",NR } ' /etc/passwd
The following screenshot captures this command and shows a partial output:

AWK Fundamentals Chapter 10
[ 175 ]In the first example with #&(*/ , we saw that there is no reason why we cannot use the &/%
code block in isolation without a main code block. If we need to emulate the XDM
command, we can use the following BXL statement:
$ awk ' END { print NR }' /etc/passwd
The output will be the line count from the file. The following screenshot shows both the use
of the BXL command and the XD command to count the lines in the FUDQBTTXE  file:
As we can see, the output does tally with  lines and our code has worked.
Another feature that we can practice with is working on selected lines only. For example, if
we want to print only the first five lines, we will use the following statement:
$ awk ' NR < 6 ' /etc/passwd
If we want to print lines  through to , we can use the following code:
$ awk ' NR==8,NR==12 ' /etc/passwd
We can also use regular expressions to match the text in the lines. Take a look at the
following example where we look at the lines that end in the word CBTI :
$ awk ' /bash$/ ' /etc/passwd
The example and the output it produces are shown in the following screenshot:
So if you want to use a regex pattern, you should use two slashes and write the pattern
between them, CBTI .
AWK Fundamentals Chapter 10
[ 176 ]AWK variables
We saw how to use data fields such as  and . Also, we saw the /3 field, which holds
the number of processed lines, but there are more built-in variables that AWK offers to 
simplify work more and more.
'*&-%8*%5)4 : Specifies the field width
34: Specifies the record separator
'4: Specifies the field separator
0'4: Specifies the output separator, which is a space by default
034: Specifies the output separator
'*-&/".& : Holds the processed file name
/': Holds the line being processed
'/3: Holds the record which is processed
*(/03&$"4& : Ignores character case
These variables can help you a lot in many cases. Let's assume that we have the following
file:
+PIO%PF
CBDLTUSFFU


.PLIUBS&CSBIJN
.VTFVNTUSFFU


We can say that we have two records for two persons and each record contains three fields.
Let's assume that we need to print the name and the phone number. So how do we make
AWK process them correctly?
In this case, the fields are separated by a newline ( =O) and the records are separated by
empty lines.
So if we set the '4 to (=O) and the 34 to empty text, the fields will be identified correctly:
$ awk 'BEGIN{FS="\n"; RS=""} {print $1,$3}' myfile
AWK Fundamentals Chapter 10
[ 177 ]
The result appears valid and appropriate.
In the same way, you can use the 0'4 and 034 for the output report:
$ awk 'BEGIN{FS="\n"; RS=""; OFS="*"} {print $1,$3}' myfile
You can use any text that fits your needs.
We know that /3 holds the number of the processed line and '/3 looks the same from the
definition, but let's explore the following example to see the difference:
Assume that we have the following file:
8FMDPNFUP"8,QSPHSBNNJOH
5IJTJTBUFTUMJOF
"OEUIJTJTPOFNPSF
Let's process this file using AWK:
$ awk 'BEGIN{FS="\n"}{print $1,"FNR="FNR}' myfile myfile
Here we processed the file twice for testing purposes only to see what the value of the FNR
variable is.
AWK Fundamentals Chapter 10
[ 178 ]As you can see, the value starts from 1 for every processing cycle.
Let's see the whether /3 variable is used in the same way:
$ awk 'BEGIN {FS="\n"} {print $1,"FNR="FNR,"NR="NR} END{print "Total lines:
",NR}' myfile myfile
The /3 variable preserves its value during the entire processing while '/3 started from 1.
User-defined variables
You can define your own variables to use in AWK programming, as with any programming
language.
You can define the variable using any text, but it MUST  not start with numbers:
$ awk '
BEGIN{
var="Welcome to AWk programming"
print var
}'
You can define any type of variables and use it the same way.
AWK Fundamentals Chapter 10
[ 179 ]You can define numbers like this:
$ awk '
BEGIN{
var1=2
var2=3
var3=var1+var2
print var3
}'
Or perform string concatenation like this:
$ awk '
BEGIN{
str1="Welcome "
str2=" To shell scripting"
str3=str1 str2
print str3
}'
As you can see, AWK is a powerful scripting language.
AWK Fundamentals Chapter 10
[ 180 ]Conditional statements
AWK supports conditional statements such as JG and XIJMF  loops.
The if command
Assume you have the following file:
50
30
80
70
20
90
Now, let's filter the values:
$ awk '{if ($1 > 50) print $1}' myfile
The JG statement checks every value and, if it's greater than , it will print it.
You can use FMTF  clauses like this:
$ awk '{
if ($1 > 50)
{
x = $1 * 2
print x
} else
{
x = $1 * 3
print x
}}' myfile
AWK Fundamentals Chapter 10
[ 181 ]
If you don't use brackets \^ to enclose your statements, you can type them on the same line
with a semicolon:
$ awk '{if ($1 > 50) print $1 * 2;  else print $1 * 3}' myfile
Note that you can save this code into a file and assign it to the BXL
command using the G option, as we will see later on this chapter.
while loops
AWK processes every line of your file, but what if you want to iterate over the fields of each
line itself?
You can iterate over fields using a XIJMF  loop when using AWK.
Assume we have the following file:



AWK Fundamentals Chapter 10
[ 182 ]Now let's iterate over the fields using a XIJMF  loop.
$ awk '{
total = 0
i = 1
while (i < 4)
{
total += $i
i++
}
mean = total / 3
print "Mean value:",mean
}' myfile
The XIJMF  loop iterates over the fields; we get the mean value for every row and print it.
for loops
You can use GPS loops to iterate over values when using AWK like this:
$ awk '{
total = 0
for (var = 1; var < 4; var++)
{
total += $var
}
mean = total / 3
print "Mean value:",mean
}' myfile
AWK Fundamentals Chapter 10
[ 183 ]
We achieved the same result but using the GPS loop this time.
Formatting output
We have remained faithful to the QSJOU  command so far, as we have been limited in what
we require from the output. If we want to print out, say, the username, UID, and default
shell, we need to start formatting the output just a little. In this case, we can organize the
output in well-shaped columns. Without formatting, the command we use will look similar
to the following example, where we use commas to separate the field that we want to print:
$ awk ' BEGIN { FS=":" } { print $1,$3,$7 } ' /etc/passwd
We use the #&(*/  block here, as we can make use of it to print column headers later.
To understand the problem a little better, take a look at the following screenshot, which
illustrates uneven column widths:

AWK Fundamentals Chapter 10
[ 184 ]The issue that we have in the output is that the columns do not align, as the username is an
inconsistent length. To improve on this, we can use the QSJOUG  function where we can
specify the column width. The syntax for the BXL statements will be similar to the following
example:
$ awk ' BEGIN { FS=":" }
> { printf "%10s %4d %17s\n",$1,$3,$7 } ' /etc/passwd
The QSJOUG  formatting is included within double quotes. We also need to include the
newline with the =O. The QSJOUG  function does not add a newline automatically, whereas
the QSJOU  function does. We print the three fields; the first accepts string values and is set
to  characters wide. The middle field accepts up to 4 numbers and we finish with the
default shell field where we allow up to  string characters.
The following screenshot shows how the output can be improved:
We can further enhance this by adding header information. Although the code starts to look
untidy at this stage, we will later see how we can resolve this with AWK control files. The
following example shows the header information being added to the #FHJO  block. The
semicolon is used to separate the two statements in the #&(*/  block:
$ awk 'BEGIN {FS=":" ;printf "%10s %4s %17s\n","Name","UID","Shell" }
> { printf "%10s %4d %17s\n",$1,$3,$7 } ' /etc/passwd
In the following screenshot, we can see how this improves the output even further:

AWK Fundamentals Chapter 10
[ 185 ]In the previous chapter, we saw how we can augment the output with the use of colors in
the shell. We may also use color from within AWK by adding our own functions. In the
next code example, you will see that AWK allows us to define our own functions to
facilitate more complex operations and isolate the code. We will now modify the previous
code to include green output in the header:
$ awk 'function green(s) {
> printf "\033[1;32m" s "\033[0m\n"
> }
> BEGIN {FS=":";
green("   Name:  UID:    Shell:") }
> { printf "%10s %4d %17s\n",$1,$3,$7 } ' /etc/passwd
Creating the function within BXL allows color to be added where we require, in this case,
green text. It is easy to create functions to define other colors. The code and output are
included in the following screenshot:
Further filtering to display users by UID
We have been able to build our skills with AWK, piece by piece, and what we have learned
has been useful. We can take these tiny steps and add them to start creating something a
little more usable. Perhaps we want to print just standard users; these are usually users
higher than 500 or 1,000 depending on your particular distribution.
On the Linux Mint distribution that I am using for this book, standard users start with UID
 . The UID is the third field. This is really a simple matter of using the value of the third
field as the range operator. We can see this in the following example:
$ awk -F":" '$3 > 999 ' /etc/passwd
AWK Fundamentals Chapter 10
[ 186 ]We can show users whose UID is  or lower with the following command:
$ awk -F":" '$3 < 101 ' /etc/passwd
These just give you an idea of some of the possibilities available with AWK. The reality is
that we can play all day with our arithmetic comparison operators.
We have also seen that, with some of these examples, the BXL statements become a little
long. This is where we can implement the BXL control files. Let's take a look at these
straightaway before we get lost in a morass of syntax.
AWK control files
Just as with TFE, we can simplify the command line by creating and including control files.
This also makes editing the command later more easily achievable. The control files contain
all the statements that we want BXL to execute. The main thing that we must consider with
TFE, BXL, and shell scripts is modularization; creating reusable elements that can be used to
isolate and reuse the codes. This saves us time and work and we get more time for the tasks
that we enjoy.
To see an example of an BXL control file, we should revisit the formatting of the QBTTXE  file.
Creating the following file will encapsulate the BXL statements:
GVODUJPOHSFFO
T\
QSJOUG=<NT=<N=O
^
#&(*/\
'4
HSFFO
/BNF6*%4IFMM
^
\
QSJOUGTET=O
^
We can save this file as QBTTXEBXL .
Being able to encompass all BXL statements in the one file is very convenient and the
execution becomes clean and tidy:
$ awk -f passwd.awk /etc/passwd
This certainly encourages more complex BXL statements and allows you to extend more
functionality to your code.
AWK Fundamentals Chapter 10
[ 187 ]Built-in functions
In the previous example, we defined a function called HSFFO . This leads into taking about
some built-in functions that come with BXL.
AWK comes with many built-in functions such as mathematical functions:
TJO
Y
DPT
Y
TRSU
Y
FYQ
Y
MPH
Y
SBOE

You can use them like this:
$ awk 'BEGIN{x=sqrt(5); print x}'
Also, there are built-in functions that can be used in string manipulation:
$ awk 'BEGIN{x = "welcome"; print toupper(x)}'
Summary
I hope that you have a better and clearer understanding of what you can use the AWK tool
for. This is a data-processing tool that runs through text files, line by line, and processes the
code you add. The main block runs for each line that matches the row criteria, whereas the
#&(*/  and &/% block code is executed just once.
You've learned how to use AWK built-in variables and how to define your own variables
and use them.
Also, you have learned how to use the JG, XIJMF  , and GPS loop to iterate over data fields.
In the next chapter, we will discuss regular expressions and how to use them in TFE and
AWK to gain a lot of power.
AWK Fundamentals Chapter 10
[ 188 ]Questions
What is the output of the following command?1.
$ awk '
BEGIN{
var="I love AWK tool"
print $var
}'
Assume you have the following file:2.
13
15
22
18
35
27
Then you run the following command against this file:
$ awk '{if ($1 > 30) print $2}' myfile
How many numbers will be printed?
Assume that you have the following file:3.



AWK Fundamentals Chapter 10
[ 189 ]And you run the following command:
$ awk '{
total = 0
i = 1
while (i < 3)
{
total += $i
i++
}
mean = total / 3
print "Mean value:",mean
}' myfile
What is wrong with the previous code?
How many lines will be printed from the following command?4.
$ awk -F":" '$3 < 1 ' /etc/passwd
Further reading
Please see the following for further reading relating to this chapter:
IUUQTMJLFHFFLTDPNBXLDPNNBOE
IUUQTXXXHOVPSHTPGUXBSFHBXLNBOVBMHBXLIUNM
111
Regular Expressions
In this chapter, we will talk about the most mysterious part of using stream editor  (sed) and
AWK. They are regular expressions, or regexes for short. In the previous chapters, we
discussed some regular expressions shyly and that's because we don't need to dig into them
without a good understanding.
If you understand how regular expressions are written, you will save a lot of time and
effort. With regular expressions, you will unleash the real power behind sed and AWK and
will use them professionally. 
This chapter will cover the following aspects:
Regular expression engines
Defining BRE patterns
Defining ERE patterns
Using HSFQ
Technical requirements
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Regular Expressions Chapter 11
[ 191 ]Regular expression engines
To start with, what are regular expressions?
Regular expressions are strings that the regex engine interprets to match a specific text. It's
like an advanced way of searching.
Assume that you want to search a file for lines starting with any small letters, or you want
to search for lines that contain a number, or maybe search for lines starting with specific
text. The normal search can't be generic: the only way to do that is to use regular
expressions.
And what is the regex engine?
The regex engine is the piece of software that understands these strings and translates them
to find the matched text.
There are many regex engines out there; for example, the engines that are shipped with
programming languages such as Java, Perl, and Python. Also, the engines that Linux tools
use are sed and AWK, and the important thing for us now is to learn the types of regex
engine in Linux.
There are two types of regex engine in Linux:
The Basic Regular Expression  (BRE ) engine
The Extended Regular Expression  (ERE ) engine
Most Linux binaries understand both engines, such as sed and AWK.
HSFQ  also can understand ERE, but you have to use the & option, which is equivalent to
using FHSFQ .   
We will see how to define a regex pattern for sed and AWK. We will start by defining BRE
patterns, so let's get started.
Regular Expressions Chapter 11
[ 192 ]Defining BRE patterns
To define a regex pattern, you can type the following:
$ echo "Welcome to shell scripting" | sed -n '/shell/p'
$ echo "Welcome to shell scripting" | awk '/shell/{print $0}'
A very important thing you need to know about regex patterns in general is they are case
sensitive:
$ echo "Welcome to shell scripting" | awk '/shell/{print $0}'
$ echo "Welcome to SHELL scripting" | awk '/shell/{print $0}'
Say you want to match any of the following characters:
<>?\^=]

You must escape them with a backslash because these characters are
special characters for the regex engines.
Now you know how to define a BRE pattern. Let's use the common BRE characters.
Regular Expressions Chapter 11
[ 193 ]Anchor characters
Anchor characters are used to match the beginning or the end of a line. There are two
anchor characters: the caret ( ?) and the dollar sign ( ).
The caret character is used to match the beginning of a line:
$ echo "Welcome to shell scripting" | awk '/^Welcome/{print $0}'
$ echo "SHELL scripting" | awk '/^Welcome/{print $0}'
$ echo "Welcome to shell scripting" | sed -n '/^Welcome/p'
So, the caret character is used to check whether the specified text is at the beginning of the
line.
If you want to search for the caret as a character, you should escape it with a backslash if
you use AWK.
However, if you use TFE, you don't need to escape it:
$ echo "Welcome ^ is a test" | awk '/\^/{print $0}'
$ echo "Welcome ^ to shell scripting" | sed -n '/^/p'
To match the end of the text, you can use the dollar sign character ( ):
$ echo "Welcome to shell scripting" | awk '/scripting$/{print $0}'
$ echo "Welcome to shell scripting" | sed -n '/scripting$/p'
Regular Expressions Chapter 11
[ 194 ]
You can use both characters ( ?) and () in the same pattern to specify text.
You can use these characters to do something useful, such as search for empty lines and
trim them:
$ awk '!/^$/{print $0}' myfile
The exclamation mark ( ) is called the negation character, which negates what's after it.
The pattern searches for ? where the caret ( ?) refers to the beginning of a line and the
dollar sign ( ) refers to the end of a line, which means search for lines that have nothing
between the beginning and the end which means empty lines. Then we negate that with the
exclamation mark ( ) to get the other lines that are not empty. 
Let's apply it to the following file:
-PSFN*QTVNJTTJNQMZEVNNZUFYU
-PSFN*QTVNIBTCFFOUIFJOEVTUSZ	TTUBOEBSEEVNNZ
*UIBTTVSWJWFEOPUPOMZGJWFDFOUVSJFT
*UJTBMPOHFTUBCMJTIFEGBDUUIBUBSFBEFSXJMMCFEJTUSBDUFE
Now, let's see the magic:
$ awk '!/^$/{print $0}' myfile
The lines are printed without the empty lines.
Regular Expressions Chapter 11
[ 195 ]The dot character
The dot character matches any character except the new line ( =O). Let's use it against the
following file:
8FMDPNFUPTIFMMTDSJQUJOH
*MPWFTIFMMTDSJQUJOH
TIFMMTDSJQUJOHJTBXFTPNF
Say we use the following commands:
$ awk '/.sh/{print $0}' myfile
$ sed -n '/.sh/p' myfile
This pattern matches any line containing TI and any text before it:
As you can see, it matches the first two lines only because the third line starts with TI, so no
match for the third line.  
The character class
We saw how to match any character using the dot character. What if you want to match a
specific set of characters only?
You can pass the characters you want to match between square brackets <> to match them,
and this is the character class.
Let's take the following file as an example:
*MPWFCBTITDSJQUJOH
*IPQFJUXPSLTXJUIPVUBDSBTI
0S*	MMTNBTIJU
Regular Expressions Chapter 11
[ 196 ]Let's see how the character class works:
$ awk '/[mbr]ash/{print $0}' myfile
$ sed -n '/[mbr]ash/p' myfile
The character class <NCS>  matches any of the included characters followed by ash, so this
matches the three lines.
You can employ it in something useful, such as matching an uppercase or a lower case
character:
$ echo "Welcome to shell scripting" | awk '/^[Ww]elcome/{print $0}'
$ echo "welcome to shell scripting" | awk '/^[Ww]elcome/{print $0}'
The character class is negated using the caret character like this:
$ awk '/[^br]ash/{print $0}' myfile
Here, we match any line that contains ash and starts neither with C nor S.
Remember that using the caret character ( ?) outside the square brackets
means the beginning of a line.
Using character class, you specify your characters. What if you have a long range of
characters?
Regular Expressions Chapter 11
[ 197 ]Ranges of characters
You can specify a range of characters to match between square brackets as follows:
<BE>
This means the range of characters from B to E, so B, C, D, and E are included.
Let's use the same previous example file:
$ awk '/[a-m]ash/{print $0}' myfile
$ sed -n '/[a-m]ash/p' myfile
The character range from B to N is selected. The third line contains S before ash, which is not
in our range, so only the second line doesn't match.
You can use numbers ranges as well:
$ awk '/[0-9]/'
This pattern means from  to  is matched.
You can write multiple ranges in the same bracket:
$ awk '/[d-hm-z]ash/{print $0}' myfile
$ sed -n '/[d-hm-z]ash/p' myfile

Regular Expressions Chapter 11
[ 198 ]In this pattern, from E to I and from N to [ are selected and since the first line contains C
before ash, only the first line doesn't match.
You can use the ranges to select all uppercase and lowercase characters as follows:
$ awk '/[a-zA-Z]/'
Special character classes
We saw how to match a set of characters using the character class, then we saw how to
match a range of characters using character ranges.
Actually, the ERE engine offers ready-to-use classes to match some common sets of
characters as follows:
<<BMQIB>> Matches any alphabetical character
<<VQQFS>>         Matches A bZ uppercase only
<<MPXFS>> Matches a bz lowercase only
<<BMOVN>>        Matches 0 b9, AbZ, or abz
<<CMBOL>>       Matches space or Tab only
<<TQBDF>>        Matches any whitespace character: space, Tab, CR
<<EJHJU>>       Matches from  to 
<<QSJOU>>         Matches any printable character
<<QVODU>>        Matches any punctuation character
So, if you want to match uppercase characters, you can use <<VQQFS>>  and it will work
exactly as the character range [A-Z].
Let's test one of them against the following example file:
DIFDLJOHTQFDJBMDIBSBDUFSDMBTTFT
5IJT-*/&DPOUBJOTVQQFSDBTF
"-40UIJTPOF
Regular Expressions Chapter 11
[ 199 ]We will match the uppercase characters to see how it works:
$ awk '/[[:upper:]]/{print $0}' myfile
$ sed -n '/[[:upper:]]/p' myfile
The uppercase special class makes it easy to match any line that contains uppercase letters.
The asterisk
The asterisk is used to match the existence of a character or a character class zero or more
times.
This can be useful when searching for a word with multiple variations or that has been
misspelled:
$ echo "Checking colors" | awk '/colou*rs/{print $0}'
$ echo "Checking colours" | awk '/colou*rs/{print $0}'
If the character V doesn't exist at all or exists, that will match the pattern.
We can benefit from the asterisk character by using it with the dot character to match any
number of characters.
Regular Expressions Chapter 11
[ 200 ]Let's see how to use them against the following example file:
5IJTJTBTBNQMFMJOF
"OEUIJTJTBOPUIFSPOF
5IJTJTPOFNPSF
'JOBMMZUIFMBTUMJOFJTUIJT
Let's write a pattern that matches any line that contains the word UIJT  and anything after
it:
$ awk '/this.*/{print $0}' myfile
$ sed -n '/ this.*/p' myfile
The fourth line contains the word UIJT , but the first and third lines contain a capital 5, so
that it doesn't match.
The second line contains the word and text after it, whereas the fourth line contains the
word and nothing after it, and in both cases, the asterisk matches zero or more instances.
You can use the asterisk with the character class to match the existence of any character
inside the character class for one time or none at all.
$ echo "toot" | awk '/t[aeor]*t/{print $0}'
$ echo "tent" | awk '/t[aeor]*t/{print $0}'
$ echo "tart" | awk '/t[aeor]*t/{print $0}'

Regular Expressions Chapter 11
[ 201 ]The first line contains the character P two times, so it matches.
The second line contains the O character, which doesn't exist in the character class, so there
is no match.
The third line contains the characters B and S, once for each, and they exist in the character
class, so that line matches the pattern too.
Defining ERE patterns
We saw how easy it is to define BRE patterns. Now, we will see some ERE patterns, which
are more powerful.
ERE engines understand the following patterns besides BRE patterns:
Question marks
Plus signs
Curly braces
Pipe characters
Expression grouping
By default, AWK supports ERE patterns, and sed needs S to understand these patterns.
The question mark
The question mark matches the existence of the preceding character or character class zero
or one time only:
$ echo "tt" | awk '/to?t/{print $0}'
$ echo "tot" | awk '/to?t/{print $0}'
$ echo "toot" | awk '/to?t/{print $0}'
$ echo "tt" | sed -r -n '/to?t/p'
$ echo "tot" | sed -r -n '/to?t/p'
$ echo "toot" | sed -r -n '/to?t/p'
Regular Expressions Chapter 11
[ 202 ]
In the first two examples, the character P exists zero and one time, whereas in the third
example, it exists two times, which doesn't match the pattern
In the same way, you can use the question mark with the character class:
$ echo "tt" | awk '/t[oa]?t/{print $0}'
$ echo "tot" | awk '/t[oa]?t/{print $0}'
$ echo "toot" | awk '/t[oa]?t/{print $0}'
$ echo "tt" | sed -r -n '/t[oa]?t/p'
$ echo "tot" | sed -r -n '/t[oa]?t/p'
$ echo "toot" | sed -r -n '/t[oa]?t/p'
The third example only doesn't match because it contains the P character two times.
Note that when using the question mark with the character class, it doesn't need to have all
of character class in the text; one is enough to pass the pattern
Regular Expressions Chapter 11
[ 203 ]The plus sign
The plus sign matches the existence of the preceding character or character class one time or
more, so it must exist at least once:
$ echo "tt" | awk '/to+t/{print $0}'
$ echo "tot" | awk '/to+t/{print $0}'
$ echo "toot" | awk '/to+t/{print $0}'
$ echo "tt" | sed -r -n '/to+t/p'
$ echo "tot" | sed -r -n '/to+t/p'
$ echo "toot" | sed -r -n '/to+t/p'
The first example doesn't have an P character, and that's why it's the only example that has
no match.
Also, we can use the plus sign with the character class:
$ echo "tt" | awk '/t[oa]+t/{print $0}'
$ echo "tot" | awk '/t[oa]+t/{print $0}'
$ echo "toot" | awk '/t[oa]+t/{print $0}
$ echo "tt" | sed -r -n '/t[oa]+t/p'
$ echo "tot" | sed -r -n '/t[oa]+t/p'
$ echo "toot" | sed -r -n '/t[oa]+t/p'
Regular Expressions Chapter 11
[ 204 ]
The first example only doesn't match because it contains no P character at all. 
Curly braces
The curly braces define the number of existence of the preceding character or character
class:
$ echo "tt" | awk '/to{1}t/{print $0}'
$ echo "tot" | awk '/to{1}t/{print $0}'
$ echo "toot" | awk '/to{1}t/{print $0}'
$ echo "tt" | sed -r -n '/to{1}t/p'
$ echo "tot" | sed -r -n '/to{1}t/p'
$ echo "toot" | sed -r -n '/to{1}t/p'

Regular Expressions Chapter 11
[ 205 ]The third example doesn't contain any matches because the P character exists two times. So,
what if you want to specify a more flexible number?
You can specify a range inside the curly braces:
$ echo "toot" | awk '/to{1,2}t/{print $0}'
$ echo "toot" | sed -r -n '/to{1,2}t/p'
Here, we match the P character if it exists one or two times.
Also, you can use the curly braces with the character class:
$ echo "tt" | awk '/t[oa]{1}t/{print $0}'
$ echo "tot" | awk '/t[oa]{1}t/{print $0}'
$ echo "toot" | awk '/t[oa]{1}t/{print $0}'
$ echo "tt" | sed -r -n '/t[oa]{1}t/p'
$ echo "tot" | sed -r -n '/t[oa]{1}t/p'
$ echo "toot" | sed -r -n '/t[oa]{1}t/p'
As expected, if any of the characters <PB>  exists for one time, the pattern will match.
Regular Expressions Chapter 11
[ 206 ]The pipe character
The pipe character ( ]) tells the regex engine to match any of the passed strings. So, if one of
them exists, that is enough for the pattern to match. It's like a logical 03 between the passed
strings:
$ echo "welcome to shell scripting" | awk '/Linux|bash|shell/{print $0}'
$ echo "welcome to bash scripting" | awk '/Linux|bash|shell/{print $0}'
$ echo "welcome to Linux scripting" | awk '/Linux|bash|shell/{print $0}'
$ echo "welcome to shell scripting" | sed -r -n '/Linux|bash|shell/p'
$ echo "welcome to bash scripting" | sed -r -n '/Linux|bash|shell/p'
$ echo "welcome to Linux scripting" | sed -r -n '/Linux|bash|shell/p'
All the previous examples have a match, since any of the three words exists in each
example.
There are no spaces between the pipes and the words. 
Regular Expressions Chapter 11
[ 207 ]Expression grouping
You can use parentheses 
 to group characters or words to make them one piece in the
eyes of the regex engine:
$ echo "welcome to shell scripting" | awk '/(shell scripting)/{print $0}'
$ echo "welcome to bash scripting" | awk '/(shell scripting)/{print $0}'
$ echo "welcome to shell scripting" | sed -r -n '/(shell scripting)/p'
$ echo "welcome to bash scripting" | sed -r -n '/(shell scripting)/p'
Since the TIFMMTDSJQUJOH  string is grouped with parentheses, it will be treated as a
single piece.
So, if the entire sentence doesn't exist, the pattern will fail.
You may have realized that you can achieve that without parentheses like this:
$ echo "welcome to shell scripting" | sed -r -n '/shell scripting/p'
So, what is the benefit of using parentheses or expression grouping? Check the following
examples to know the difference.
You can use any of the ERE characters with the grouping parentheses:
$ echo "welcome to shell scripting" | awk '/(bash scripting)?/{print $0}'
$ echo "welcome to shell scripting" | awk '/(bash scripting)+/{print $0}'
$ echo "welcome to shell scripting" | sed -r -n '/(bash scripting)?/p'
$ echo "welcome to shell scripting" | sed -r -n '/(bash scripting)+/p'
Regular Expressions Chapter 11
[ 208 ]
In the first example, we search for the whole sentence CBTITDSJQUJOH  for zero or one
time using the question mark, and because the whole sentence doesn't exist, the pattern
succeeds.
Without expression grouping, you won't get the same result.
Using grep
If we wanted to talk properly about HSFQ , an entire book would not be enough. HSFQ
supports many engines along with BRE and ERE. It supports engines such as Perl-
compatible regular expression  (PCRE ).
The HSFQ  is a very powerful tool that most system administrators use every day. We just
want to enlighten the point of using BRE and ERE patterns as we did with sed and AWK.
HSFQ  tool understands BRE patterns by default, and if you want to use ERE patterns, you
should use the & option.
Let's work with the following example file and use a BRE pattern:
8FMDPNFUPTIFMMTDSJQUJOH
MPWFTIFMMTDSJQUJOH
TIFMMTDSJQUJOHJTBXFTPNF
Regular Expressions Chapter 11
[ 209 ]Let's test a BRE pattern:
$ grep '.sh' myfile
The results are colored in red.
Let's test an ERE pattern:
$ grep -E 'to+' myfile
All other ERE characters can be used in the same way.
Summary
In this chapter, we covered regular expressions and the regex engines BRE and ERE. We
learned how to define patterns for them.
We learned how to write these patterns for sed, AWK, and HSFQ .
Also, we saw how the special character classes make it easy to match sets of characters.
We saw how to use the powerful ERE patterns and how to group expressions.
Finally, we saw how to use the HSFQ  tool and how to define BRE and ERE patterns.
In the next two chapters, we will see some practical examples for AWK.
Regular Expressions Chapter 11
[ 210 ]Questions
Assume that you have the following file:1.
8FMDPNFUPTIFMMTDSJQUJOH
*MPWFTIFMMTDSJQUJOH
TIFMMTDSJQUJOHJTBXFTPNF
Say you run the following command:
$ awk '/awesome$/{print $0}' myfile
How many lines will be printed in the output?
How many lines will be printed if we use the following command against the2.
previous file?
$ awk '/scripting\..*/{print $0}' myfile
How many lines will be printed if we use the following command against the3.
previous sample file?
$ awk '/^[Ww]?/{print $0}' myfile
What is the output of the following command?4.
$ echo "welcome to shell scripting" | sed -n '/Linux|bash|shell/p'
Further reading
Please see the following for further reading related to this chapter:
IUUQTXXXSFHVMBSFYQSFTTJPOTJOGPFOHJOFIUNM
IUUQUMEQPSH-%1#BTI#FHJOOFST(VJEFIUNMDIBQ@IUNM
112
Summarizing Logs with AWK
In the previous chapter, we talked about regular expressions and we saw how to use them
to empower TFE and AWK. In this chapter, we will discuss some practical examples of
using AWK.
One of the tasks that AWK is really good at is filtering data from log files. These log files
may be many lines in length, perhaps 250,000 or more. I have worked with data with over a
million lines. AWK can process these lines quickly and effectively. As an example, we will
work with a web server access log with 30,000 lines to show how effective and well-written
AWK code can be. As we work our way through the chapter, we will also see different log
files and review some of the techniques that we can employ with the BXL command and the
AWK programming language to help with the reporting and administration of our services.
In this chapter, we will cover the following topics:
HTTPD log file format
Displaying data from web logs
Displaying the highest ranking client IP addresses
Displaying the browser data
Working with email logs
Technical requirements
The source code for this chapter can be downloaded from here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Summarizing Logs with AWK Chapter 12
[ 212 ]The HTTPD log file format
When working with any file, the first task is to become familiar with the file schema. In
simple terms, we need to know what is represented by each field and what is used to
delimit the fields. We will be working with the access log file from an Apache HTTPD web
server. The location of the log file can be controlled from the IUUQEDPOG  file. The default
log file location on a Debian-based system is WBSMPHBQBDIFBDDFTTMPH ; other
systems may use the IUUQE  directory in place of BQBDIF .
The MPH file is already in the code bundle, so you can download it and use it directly.
Using the UBJM  command, we can display the end of the MPH file. Although, to be fair, the
use of DBU will do just as well with this file, as it will have just a few lines:
$ tail /var/log/apache2/access.log
The output of the command and the contents of the file are shown in the following
screenshot:
The output does wrap a little onto the new lines, but we do get a feel of the layout of the
log. We can also see that even though we feel that we access just one web page, we are in
fact accessing two items: the JOEFYIUNM  and the VCVOUVMPHPQOH . We also failed to
access the GBWJDPOJDP  file. We can see that the file is space separated. The meaning of
each of the fields is laid out in the following table:
Field Purpose
1 Client IP address.
2Client identity as defined by RFC 1413 and the JEFOUE  client. This is not read
unless *EFOUJUZ$IFDL  is enabled. If it is not read, the value will be with a hyphen.
3The user ID of the user authentication if enabled. If authentication is not enabled,
the value will be a hyphen.
Summarizing Logs with AWK Chapter 12
[ 213 ]4The date and time of the request in the format of
EBZNPOUIZFBSIPVSNJOVUFTFDPOEPGGTFU .
5 The actual request and method.
6 The return status code, such as  or .
7 File size in bytes.
Even though these fields are defined by Apache, we have to be careful. The time, date, and
time zone is a single field and is defined within square braces; however, there are additional
spaces inside the field between that data and the time zone. To ensure that we print the
complete time field if required, we need to print both  and . This is shown in the
following command example:
$ awk ' { print $4,$5 } ' /var/log/apache2/access.log
We can view the command and the output it produces in the following screenshot:
Displaying data from web logs
We have already had a preview of how we can use AWK to view the log files from the
Apache web server; however, we will now move onto our demonstration file that has 
greater and more varied content.
Selecting entries by date
Having seen how we can display the date, we should perhaps look at how we print entries
from just one day. To do this, we can use the match operator in BXL. This is denoted by the
tilde or squiggly line, if you prefer. As we only need the date element, there is no need for
us to use both the date and time zone field. The following command shows how to print
entries from September 10, 2014:
$ awk ' ( $4 ~ /10\/Sep\/2014/ ) ' access.log
Summarizing Logs with AWK Chapter 12
[ 214 ]For completeness, this command and partial output is shown in the following screenshot:
The round brackets or parentheses embrace the range of lines that we are looking for and
we have omitted the main block, which ensures that we print the complete matching lines
from the range. There is nothing stopping us from further filtering on the fields to print
from the matching lines. For example, if we want to print out just the client IP address that
is being used to access the web server, we can print field . This is shown in the following
command example:
$ awk ' ( $4 ~ /10\/Sep\/2014/ ) { print $1 } ' access.log
If we want to be able to print the total number of accesses on a given date, we could pipe
the entries through to the XD command. This is demonstrated in the following:
$ awk ' ( $4 ~ /10\/Sep\/2014/ ) { print $1 } ' access.log | wc -l
However, if we want to use BXL to do this for us, this will be more efficient than starting a
new process and we can count the entries. If we use the built-in variable /3, we can print
entire lines in the files, not just those within the range. It is best to increment our own
variable in the main block instead of matching the range for each line. The &/% block can be
implemented to print the DPVOU  variable we use. The following command line acts as an
example:
$ awk ' ( $4 ~ /10\/Sep\/2014/ ) { print $1; COUNT++ }  END { print COUNT
}' access.log
The output of the count from both XD and the internal counter will give us   as a result
from the demonstration file. We should use the variable increment within the main block if
we want to count and nothing else:
$ awk ' ( $4 ~ /10\/Sep\/2014/ ) { COUNT++ }  END { print COUNT }'
access.log
We can see this in the following output:

Summarizing Logs with AWK Chapter 12
[ 215 ]Summarizing 404 errors
The status code of the request page is shown in field  of the log. The  status will
represent the page not found error on the server. I am sure we have all seen that in our
browsers at some stage. This may be indicative of a misconfigured link on your site or just
produced by a browser searching for the icon image to display in tabbed browsers for the
page. You can also identify potential threats to your site by requests looking for standard
pages that may give access to additional information on PHP driven sites, such as
WordPress.
Firstly, we can solely print the status of the request:
$ awk '{ print $9 } ' access.log
We can now extend the code a little as well as ourselves and just print the  errors:
$ awk ' ( $9 ~ /404/ ) { print $9 } ' access.log
We can extend this a little further by printing both the status code and the page that was
being accessed. This will need us to print field  and field . Simply put, this will be as
shown in the following code:
$ awk ' ( $9 ~ /404/ ) { print $9, $7 } ' access.log
Many of these failed accessed pages will be duplicated. To summarize these records, we can
use the command pipeline to achieve this with the TPSU  and VOJR  commands:
$ awk ' ( $9 ~ /404/ ) { print $9, $7 } ' access.log | sort -u
To use the VOJR  command, the data must be pre-sorted; hence, we use the TPSU  command
to prepare the data.
Summarizing HTTP access codes
It is time for us to leave the pure command line and start working with the AWK control
files. As always, when the complexity of the required result set increases, we see an increase
in the complexity of the BXL code. We will create a TUBUVTBXL  file in our current
directory. The file should look similar to the following file:
\SFDPSE<>^
&/%\
GPS
SJOSFDPSE
QSJOUSIBTPDDVSSFESFDPSE<S>UJNFT^
Summarizing Logs with AWK Chapter 12
[ 216 ]First, we will strip down the main code block and this is very simple and sparse. This is a
simple way to count each unique occurrence of a status code. Instead of using a simple
variable, we feed this into an array. The array in this case is called a record. An array is a
multi-values variable and the slots in the array are known as keys. So we will have a
collection of variables stored in the array. For example, we expect to see entries for
SFDPSE<>  and SFDPSE<> . We populate each key with their occurrence count. Each
time we find a  code, we increment the count that is stored in the associated key:
\SFDPSE<>^
In the &/% block, we create the summary information using a GPS loop to print out each key
and value from the array:
&/%\
GPS
SJOSFDPSE
QSJOUSIBTPDDVSSFESFDPSE<S>UJNFT^
To run this, the associated command line will be similar to the following:
$ awk -f status.awk access.log
To view the command and output, we have included the following screenshot:
We can take this further and focus on the  errors. You could, of course, choose any of the
status codes. We can see from the results that we have   status codes. To
summarize these  codes, we will copy the TUBUVTBXL  to a new file named BXL .
We can edit the BXL  adding an JG statement to work only on the  codes. The file
should be similar to the following code:
\JG

SFDPSE<>^
&/%\
GPS
SJOSFDPSE
QSJOUSIBTPDDVSSFESFDPSE<S>UJNFT^
Summarizing Logs with AWK Chapter 12
[ 217 ]If we execute the code with the following command:
$ awk -f 404.awk access.log
The output will be similar to the following screenshot:
Resources hits
You can check how many times a specific page or a resource was requested using AWK:
$ awk '{print $7}' access.log | sort | uniq -c | sort -rn
The preceding command will sort the requested resources from the highest requested
resource to the lowest:

Summarizing Logs with AWK Chapter 12
[ 218 ]The resources could be images, text files, or CSS files.
If you want to look at the requested PHP files, you can use HSFQ  to get PHP files only:
$ awk ' ($7 ~ /php/) {print $7}' access.log | sort | uniq -c | sort -nr
Alongside each page, there is the number of hits.
You can grab any statistics from the MPH file and get unique values and sort them the same
way.
Identify image hotlinking
As we talk about resources, there is a problem that you may face, which is image
hotlinking. It's about using your images from other servers by linking to them. This
behavior of image hotlinking can leak your bandwidth.
And since we are talking about AWK, we will see how to use AWK to find out how it is
using our images:
$ awk -F\" '($2 ~ /\.(png|jpg|gif)/ && $4 !~
/^https:\/\/www\.yourdomain\.com/){print $4}' access.log | sort | uniq -c |
sort
Summarizing Logs with AWK Chapter 12
[ 219 ]Note that you can prevent image hotlinking by a small IUBDDFTT  file if you are using
Apache, by checking if the referrer is not your domain:
3FXSJUF&OHJOFPO
3FXSJUF$POE\)551@3&'&3&3^?
3FXSJUF$POE\)551@3&'&3&3^?IUUQT
XXX=ZPVSEPNBJODPN</$>
3FXSJUF3VMF=
HJG]KQH]KQFH]CNQ]QOH<'>
Displaying the highest ranking IP address
You should now be aware of some the powers of BXL and how immense the language
structure is in itself. The data we have been able to produce from the 30,000 line file is truly
powerful and easily extracted. We just need to replace the field we have used before with
. This field represents the client IP address. If we make use of the following code, we will
be able to print each IP Address and also the number of times it has been used to access the
web server:
\JQ<>^
&/%\
GPS
JJOJQ
QSJOUJIBTBDDFTTFEUIFTFSWFSJQ<J>UJNFT^
We want to be able to extend this to show only the highest ranking IP address, the address
that has been used the most to access the site. The work, again, will mainly be in the &/%
block and will make use of a comparison against the current highest ranking address. The
following file can be created and saved as JQBXL :
\JQ<>^
&/%\
GPS
JJOJQ
JG
NBYJQ<J>\
NBYJQ<J>
NBYOVNCFSJ^
QSJOUJIBTBDDFTTFEJQ<J>UJNFT^
We can see the output of the command in the following screenshot. Part of the client IP
address has been obscured as it is from my public web server:

Summarizing Logs with AWK Chapter 12
[ 220 ]The functionality of the code comes from within the &/% block. On entering the &/% block,
we run into a GPS loop. We iterate through each entry in the JQ array. We use the
conditional JG statement to see whether the current value that we are iterating through is
higher than the current maximum. If it is, this becomes the new highest entry. When the
MPPQ  has finished, we print the IP address that has the highest entry.
Displaying the browser data
The browser that is used to access the website is contained within the log file in field . It
may be interesting to display the list of browsers used to access your site. The following
code will assist you in displaying the list of accesses by the reported browser:
\CSPXTFS<>^
&/%\
GPS
CJOCSPXTFS
QSJOUCIBTBDDFTTFECSPXTFS<C>UJNFT
^
You can see how we can create little plugins to BXL with these files and adjust the field and
array names to suit. The output is shown in the following screenshot:
Interestingly, we see that Mozilla 4 and 5 make up the majority of the requesting client. We
see that Mozilla 4 is listed here   times. The Mozilla/5.0 entry here is malformed with an
extra double quote. It appears later with 27,000 accesses.
Summarizing Logs with AWK Chapter 12
[ 221 ]Working with email logs
We have worked with logs from the Apache HTTP web server. The reality is that we can
apply the same ideals and methodology to any log file. We will take a look at Postfix mail
logs. The mail log holds all activity from the SMTP server and we can then see who has
been sending emails to whom. The log file is usually located at WBSMPHNBJMMPH . I will
access this on my Ubuntu 15.10 server that has a local email delivery. All this means is that
the STMP server is listening only to the localhost interface of  .
The log format will change a little depending on the type of message. For example,  will
contain GSPN  logs on outbound messages, whereas inbound messages will contain UP.
If we want to list all the inbound messages to the SMTP server, we can use the following
command:
$ awk '  ( $7 ~ /^to/ ) ' /var/log/mail.log
As the string UP is very short, we can add identification to it by ensuring that the field
begins with UP using the ?. The command and output is shown in the following screenshot:
It will be easy to extend the UP or GSPN  searches to also include usernames. We can see the
format of the delivered or received mail. Working with the same template we used with the
Apache logs, we can easily display the highest recipient or sender.
Summary
We now have some heavy ammunition behind our text processing and we can begin to
understand just how powerful AWK can be. Working with real data is particularly useful in
gauging the performance and accuracy of our searches. Having begun working with simple
Apache entries on the newly installed Ubuntu 15.10 Apache web server, we soon migrated
to the larger sample data from a live web server. With 30,000 lines, this file gives us some
real meat to work with and in no time, we were able to produce credible reports. We closed
up the return to the Ubuntu 15.10 server to analyze the Postfix SMTP logs. We can see that
we can very much drag and drop the technology that we have previously used into the new
log files.
Summarizing Logs with AWK Chapter 12
[ 222 ]Next up, we stick with AWK and look at how we can report on the MBTUMPH  data and on
flat XML files.
Questions
Which field in the BDDFTT@MPH  file contains the IP address? 1.
What is the command used to count the lines processed by AWK?2.
How do you get IP addresses of unique visitors from the Apache access log file?3.
How do you get the most visited PHP page from the Apache access log file?4.
Further reading
Please see the following for further reading relating to this chapter:
IUUQTIUUQEBQBDIFPSHEPDTMPHTIUNM
113
A Better lastlog with AWK
We have already seen, in $IBQUFS , Summarizing Logs with AWK , how we can create
complex reports from large amounts of data mined from purely text files. Similarly, we can
create extensive reports using the output from standard command-line tools, such as the
MBTUMPH  tool. In itself, MBTUMPH  can report the last login time for all users. Often, though,
we may wish to filter the output from MBTUMPH . Perhaps you need to exclude user accounts
that have never been used to log in to the system. It may also be irrelevant to report on
SPPU , as the account may be predominately used for TVEP  only and not used to record
regularly for standard logins.
In working through this chapter, we will work with MBTUMPH  and formatting XML data. As
this is the last chapter in which we investigate AWK, we will configure record separators.
We have already seen the use of field separators in AWK but we can change the default
record separator from a newline to something more specific to our needs. More specifically,
within this chapter we will cover:
Using AWK ranges to exclude data
Conditions based on the number of fields
Manipulating the AWK record separator to report on XML data
Technical requirements
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
A Better lastlog with AWK Chapter 13
[ 224 ]Using AWK ranges to exclude data
So far in this book, we have predominately looked at including data with ranges either for
TFE or for BXL. With both of these tools, we can negate the range so that we exclude the
specified rows. For a perfect explanation, we will use the output from the MBTUMPH
command. This will print all the login data for all the users, including accounts that have
never been logged in. These accounts that have never been logged in might be service
accounts or for new users that have not logged into the system so far.
The lastlog command
If we look at the output from MBTUMPH , when it is used without any options, we can begin
to understand the issue. From the command line, we execute the command as a standard
user. There is no requirement to run it as the root account. The command is shown in the
following example:
$ lastlog
The partial output is shown within the following screenshot:
We can see, even from this limited output, that we have a cluttered output due to the virtual
noise being created by the accounts that have not logged in. It is possible to alleviate this to
some degree using the MBTUMPH  options but it may not entirely resolve the issue. To
demonstrate this, we can add an option to MBTUMPH  to show standard users only and filter
out other system and services users. This may vary on your system but on the sample
CentOS 6 host that I am using, the first user will be UID 500. On CentOS 7, standard users
UID starts from 1000.
If we use the MBTUMPHV  command, we will only print data for those users
with a UID within this range. On the simple demonstration system, we have just three user
accounts for which the output is acceptable. However, we can understand that we may still
have some clutter due to these accounts that have not yet been used. This is shown in the
following screenshot:
A Better lastlog with AWK Chapter 13
[ 225 ]
In addition to the superfluous data being printed from /FWFSMPHHFEJO  accounts, we
may only be interested in the 6TFSOBNF  and -BUFTU  fields. This is another reason to
support the need to use AWK as our data filter. In this way, we can provide both horizontal
and vertical data filtering, rows, and columns.
Horizontally filtering rows with AWK
To provide this filtering using AWK, we will pipe the data from MBTUMPH  directly to BXL.
We will make use of a simple control file, initially providing the horizontal filtering or 
reducing the rows that we see. First, the command pipeline will be as simple as the
following command example:
$ lastlog | awk -f lastlog.awk
Of course, the complexity is abstracted from the command line and concealed within the
control file that we use. Initially, the control file is kept simple and reads as follows:

/FWFSMPHHFEJO]]?6TFSOBNF]]?SPPU\
QSJOU
^
The range is set up as we have seen previously and precedes the main code block. Using the
exclamation mark in front of the parentheses negates or reverses the selected range. The
double vertical bar acts as a logical 03. We do not include lines that contain /FWFSMPHHFE
JO, nor do we include lines that start with 6TFSOBNF . This removes the header-line that is
printed by MBTUMPH . Finally, we exclude the root account from the display. This initiates the
rows that we work with and the main code block will print those lines.
Counting matched rows
We may also want to count the number of rows returned by the filter. For example, using
the internal /3 variable will show all rows and not just matched rows; for us to be able to
report the number of users that have logged in, we must use our own variable. The
following code will maintain the count within the variable that we name DOU. We increment
this using the C style  for each iteration of the main code block.
A Better lastlog with AWK Chapter 13
[ 226 ]We can use the &/% code block to display the closing value of this variable:

/FWFSMPHHFEJO]]?6TFSOBNF]]?SPPU\
DOU
QSJOU
^
&/%\
QSJOU
QSJOU5PUBM/VNCFSPG6TFST1SPDFTTFEDOU
^
We can see from the following code and output how this appears on my system:
From the display output, we can now see that we show only users that have logged in and,
in this case, it is just the single user. However, we may also decide that we want to abstract
the data further and display only certain fields from the matched rows. This should be a
simple task but it is complicated, as the number of fields will vary depending on how the
login was executed.
Conditions based on the number of fields
If a user logs onto the server's physical console directly rather than logging on through a
remote or graphical pseudo-terminal, then the MBTUMPH  output will not display the host
field. To demonstrate this, I have logged on to my CentOS host directly to the UUZ  console
and avoided the GUI. The output from the previous AWK control file shows that we now
have the users UVY and CPC; CPC though lacks the host field as he is connected to a console:

A Better lastlog with AWK Chapter 13
[ 227 ]Although in itself it's not an issue, it will be if we want to filter the fields and the two row's
field numbers will vary where a field is omitted from some lines. For MBTUMPH  we will
have  fields for most connections and only  fields for those that connect directly to the
server console. The goal for the application is that we print the username and the date, but
not the time of the last login. We will also print our own header in the #&(*/  block. To
ensure that we use the correct placements we will need to count the fields in each row using
the /' internal variable.
For the  fields' lines we want to print fields , , , and ; for the longer lines with
additional host information, we will use fields , ,  and . We will also use QSJOUG  so that
we can align the column data correctly. The control file should be edited, as shown in the
following example:
#&(*/\
QSJOUGTT=O6TFSOBNF-PHJOEBUF
QSJOU
^

/FWFSMPHHFEJO]]?6TFSOBNF]]?SPPU\
DOU
JG
/'
QSJOUGTTTT=O
FMTF
QSJOUGTTTT=O
^
&/%\
QSJOU
QSJOU5PUBM/VNCFSPG6TFST1SPDFTTFEDOU
^
We can see the command and the output it produces in the following screenshot. We can
see how we can create a more suitable display based on information that we want to focus
on:

A Better lastlog with AWK Chapter 13
[ 228 ]If we look at the output, I have chosen to display the date before the month so we do not
display the fields in numerical order. This, of course, is a personal choice and customizable
to suit the way you feel the data should be displayed.
We can use the principles of what we have seen in the MBTUMPH  control file with output
from any command and you should practice with the commands that you want to filter the
data from.
Manipulating the AWK record separator to
report on XML data
So far, while we have been working with AWK we have limited ourselves to working with
individual rows, with each new row representing a new record. Although this is often what
we want, where we work with tagged data, such as XML where an individual record may
span multiple lines. In this case, we may need to look at setting the 34 or SFDPSE  separator
internal variable.
Apache Virtual Hosts
In $IBQUFS , Automating Apache Virtual Hosts , we worked with Apache Virtual Hosts . This
uses tagged data that defines the start and end of each virtual host. Even though we prefer
to store each virtual host in its own file, they can be combined into a single file. Consider the
following file that stores the possible virtual host definitions; this can be stored as the
WJSUVBMIPTUDPOG  file, as shown:
7JSUVBM)PTU 
%PDVNFOU3PPUXXXFYBNQMF
4FSWFS/BNFXXXFYBNQMFPSH
0UIFSEJSFDUJWFTIFSF
7JSUVBM)PTU 
7JSUVBM)PTU 
%PDVNFOU3PPUXXXUIFVSCBOQFOHVJO
4FSWFS/BNFXXXUIFVSCBOQFOHVJODPN
0UIFSEJSFDUJWFTIFSF
7JSUVBM)PTU 
7JSUVBM)PTU 
%PDVNFOU3PPUXXXQBDLU
4FSWFS/BNFXXXQBDLUQVCDPN
A Better lastlog with AWK Chapter 13
[ 229 ]0UIFSEJSFDUJWFTIFSF
7JSUVBM)PTU 
We have the three virtual hosts within a single file. Each record is separated by an empty
line, meaning that we have two new line characters that logically separate each entry. We
will explain this to AWK by setting the 34 variable as follows: 34=O=O . With this in
place, we can then print the required virtual host record. This will be set in the #&(*/  code
block of the control file.
We will also need to dynamically search the command line for the desired host
configuration. We build this into the control file. The control file should look similar to the
following code:
#&(*/\34=O=O^
_TFBSDI\QSJOU^
The #&(*/  block sets the variable and then we move onto the range. The range is set so that
the record ( ) matches ( _) the TFBSDI  variable. We must set the variable when BXL is
executed. The following command demonstrates the command line execution where the
control file and configuration file are located within our working directory:
$ awk -f vh.awk search=packt virtualhost.conf
We can see this more clearly by looking at the command and the output that is produced in
the following screenshot:
XML catalog
We can extend this further into XML files where we may not want to display the complete
record, but just certain fields. Consider the following product DBUBMPH :
QSPEVDUT 
QSPEVDU 
OBNF ESJMMOBNF 
QSJDF QSJDF 
A Better lastlog with AWK Chapter 13
[ 230 ]TUPDL TUPDL 
QSPEVDU 
QSPEVDU 
OBNF IBNNFSOBNF 
QSJDF QSJDF 
TUPDL TUPDL 
QSPEVDU 
QSPEVDU 
OBNF TDSFXESJWFSOBNF 
QSJDF QSJDF 
TUPDL TUPDL 
QSPEVDU 
QSPEVDU 
OBNF UBCMFTBXOBNF 
QSJDF QSJDF 
TUPDL TUPDL 
QSPEVDU 
QSPEVDUT 
Logically, each record is delimited as before with the empty line. Each field though is a little
more detailed and we need to use the delimiter as follows: '4< > . We define either the
opening or closing angle bracket as the field delimiter.
To help analyze this, we can print a single record as follows:
QSPEVDU OBNF UPQOBNF QSJDF QSJDF TUPDL TUPDL QSPEVDU 
Each angle brace is a field separator, which means that we will have some empty fields. We
could rewrite this line as a CSV file:
QSPEVDUOBNFUPQOBNFQSJDFQSJDFTUPDLTUPDLQSPEVDU
We just replace each angle bracket with a comma; in this way it is more easily read by us.
We can see that the content of field  is the UPQ value.
Of course, we will not edit the XML file, we will leave it in the XML format. The conversion
here is just to highlight how the field separators can be read.
The control file that we use to extract data from the XML file is illustrated in the following
code example:
#&(*/\'4< >34=O=O0'4^
_TFBSDI\QSJOU^
A Better lastlog with AWK Chapter 13
[ 231 ]Within the #&(*/  code block, we set the '4 and 34 variables as we have discussed. We also
set the Output Field Separator  (0'4) or  to a space. In this way, when we print the fields we
separate the values with a space rather than leaving in the angle brackets. The range makes
use of the same match as we used before when looking at the virtual hosts.
If we need to search for the product drill from within the DBUBMPH , we can use the
command in the following example:
$ awk -f catalog.awk search=drill catalog.xml
The following screenshot shows the output in detail:
We have now been able to take a rather messy XML file and create readable reports from
the catalog. The power of AWK is highlighted again and, for us, the last time within this
book. By now, I hope you too can start to make use of this on a regular basis.
Summary
We had three chapters where we used AWK, starting with some basic usage statements in
$IBQUFS , AWK Fundamentals  where we became comfortable with AWK. In $IBQUFS ,
Summarizing Logs with AWK , and this chapter, we started building our bespoke applications.
Specifically, in this chapter we saw how we could create reports from the output of
standard commands, such as MBTUMPH . We saw that we could negate ranges and
additionally make use of the 03 statement. We then built an application that will allow us to
query XML data.
For the next two chapters, we will move away from shell scripts and look at scripts using
perl and Python so we can compare these scripting languages and make appropriate
choices.
A Better lastlog with AWK Chapter 13
[ 232 ]Questions
How do we get the users who never logged into the system?1.
From the previous question, how do you count the number of users who never2.
logged in?
How many lines will be printed from the following command?3.
Further reading
Please see the following for further reading relating to this chapter:
IUUQTMJOVYEJFOFUNBOMBTUMPH
IUUQTFOXJLJQFEJBPSHXJLJ-BTUMPH
114
Using Python as a Bash
Scripting Alternative
In the previous chapter, we saw a practical example of using AWK and we saw how to
process MBTUMPH  output to produce better reports. In this chapter, we will take a look at
another scripting alternative for bash. We will talk about Python. Python is another
scripting language and the newest that we have looked at so far. Similar to bash, Python is
an interpreted language and makes use of the shebang. Although it does not have a shell
interface, we can access a console called REPL where we can type Python code to interact
with the system. In this chapter, we will cover the following topics:
What is Python?
Saying Hello World the Python way
Pythonic arguments
Significant whitespace
Reading user input
String manipulation
Technical requirements
The source code for this chapter can be downloaded here:
IUUQTHJUIVCDPN1BDLU1VCMJTIJOH.BTUFSJOH-JOVY4IFMM4DSJQUJOH4FDPOE
&EJUJPOUSFFNBTUFS$IBQUFS
Using Python as a Bash Scripting Alternative Chapter 14
[ 234 ]What is Python?
Python is an object-oriented interpreted language that is designed to be easy to use and to
aid Rapid Application Development . This is achieved by the use of simplified semantics in
the language.
Python was created at the end of the 1980s, towards the very end of December 1989, by the
Dutch developer Guido van Rossum. The majority of the design of the language aims for
clarity and simplicity, and one of the main rules of the Zen of Python  is:
There should be one, and preferable only one, obvious way to do it.
Often systems will have both Python 2 and Python 3 installed; however, all newer
distributions are switching to Python 3. We will be working with Python 3.
Since we are using Linux Mint, it comes shipped with Python 3 already.
If you are using another Linux distribution or Python 3 is not found for any reason, you can
install it like this:
On RedHat based distributions:
$ sudo yum install python36
On Debian based distributions:
$ sudo apt-get install python3.6
Although there is no shell, we can interact with Python using REPL cread, evaluate, print,
and loop. We can access this by typing QZUIPO  in the command line or QZUIPO  if you
are using CentOS 7. You should see something similar to the following screenshot:
We can see that we are presented with the     prompt and this is known as the REPL
console. We should emphasize that this is a scripting language and, like bash and Perl, we
will normally execute code through the text files that we create. Those text files will
normally be expected to have a QZ suffix to their name.
Using Python as a Bash Scripting Alternative Chapter 14
[ 235 ]While working with REPL, we can print the version independently by importing a module.
In Perl, we will use the keyword; in bash we will use the command source; and in Python
we use import:
>>>import sys
With the module loaded, we can now investigate the object-oriented nature of Python by
printing the version:
>>> sys.version
We will navigate to the TZT object within our namespace and call the version method from
that object.
Combining these two commands, we should see the following output:
To close this section describing Python, we should take a look at the Zen of Python. From
REPL, we can type JNQPSUUIJT , as shown in the following screenshot:

Using Python as a Bash Scripting Alternative Chapter 14
[ 236 ]This is far more than just the Zen of Python; it's actually a good rule for all programming
languages and a guide for developers.
Finally, to close the REPL, we will use Ctrl + D in Linux or Ctrl + Z in Windows.
Saying Hello World the Python way
The code we write in Python should be clear and uncluttered: sparse is better than dense.
We will need the shebang on the first line and then the QSJOU  statement. The QSJOU
function includes the newline and we do not need semicolons at the end of the line. We can
see the edited version of )0.&CJOIFMMPQZ  in the following example:
VTSCJOQZUIPO
QSJOU
)FMMP8PSME
We will still need to add the execute permission, but we can run the code as earlier using
DINPE . This is shown in the following command but we should be a little used to this now:
$ chmod u+x $HOME/bin/hello.py
Finally, we can now execute the code to see our greeting.
Similarly, you can run the file using the Python interpreter from the command line like this:
$ python3 $HOME/bin/hello.py
Or in some Linux distributions, you can run it like this:
$ python36 $HOME/bin/hello.py
Again, knowing at least one language makes it easier to adapt to others and there aren't
many new features in this.
Pythonic arguments
We should know by now that we will want to pass command-line arguments to Python and
we can do this using the BSHW  array. However, we are more like bash; with Python we
combine the program name into the array with the other arguments.
Using Python as a Bash Scripting Alternative Chapter 14
[ 237 ]Python also uses lowercase instead of uppercase in the object name:
The BSHW  array is a part of the TZT object
TZTBSHW<>  is the script name
TZTBSHW<>  is the first argument supplied to the script
TZTBSHW<>  is the second supplied argument and so on
The argument count will always be at least 1, so, keep this in mind when
checking for supplied arguments
Supplying arguments
If we create the )0.&CJOBSHTQZ  file we can see this in action. The file should be
created as follows and made executable:
VTSCJOQZUIPO
JNQPSUTZT
QSJOU
)FMMPTZTBSHW<>
If we run the script with a supplied argument, we should see something similar to the
following screenshot:
Our code is still quite clean and simple; however, you may have noticed that we cannot
combine the quoted text in the QSJOU  statement with the argument. We use the + symbol to
join or concatenate the two strings together. As there is no specific symbol to denote a
variable or any other type of object, they cannot appear as static text within quotes.
Counting arguments
As previously mentioned, the script name is the first argument at index  of the array. So, if
we try to count the arguments, then the count should always be at the very least 1. In other
words, if we have not supplied arguments, the argument count will be 1. To count the items
in an array, we can use the MFO
  function.
Using Python as a Bash Scripting Alternative Chapter 14
[ 238 ]If we edit the script to include a new line we will see this work, as follows:
VTSCJOQZUIPO
JNQPSUTZT
QSJOU
)FMMPTZTBSHW<>
QSJOU
MFOHUIJTTUS
MFO
TZTBSHW
Executing the code as we have earlier, we can see that we have supplied two
arguments cthe script name and then the string .PLIUBS :
If we try and have a single QSJOU  statement to print the output and the number of
arguments, then it will produce an error because we can't concatenate integers with strings.
The length value is an integer and this cannot be mixed with strings without conversion.
That's why we used the TUS function to convert the integer to a string. The following code
will fail:
VTSCJOQZUIPO
JNQPSUTZT
QSJOU
)FMMPTZTBSHW<>MFO
TZTBSHW
If we try to run the script and omit to supply an argument, then there will be a null value in
the array when we reference index . This will give an error, as shown in the following
screenshot:
Using Python as a Bash Scripting Alternative Chapter 14
[ 239 ]
We of course need to handle this to prevent the error; enter the concept of significant
whitespace.
Significant whitespace
A major difference between Python and most other languages is that additional whitespace
can mean something. The indent level of your code defines the block of code to which it
belongs. So far, we have not indented the code we have created past the start of the line.
This means that all of the code is at the same indent level and belongs to the same code
block. Rather than using brace brackets or the do and done keywords to define the code
block, we use indents. If we indent with two or four spaces or even tabs, then we must stick
to those spaces or tabs. When we return to the previous indent level, we return to the
previous code block.
This seems complex but it is really quite simple and keeps your code clean and uncluttered.
If we edit the BSHQZ  file to prevent unwelcomed errors, if an argument is not supplied, we
can see this in action:
VTSCJOQZUIPO
JNQPSUTZT
DPVOUMFO
TZTBSHW
JG
DPVOU 
QSJOU
"SHVNFOUTTVQQMJFETUS
DPVOU
QSJOU
)FMMPTZTBSHW<>
QSJOU
&YJUJOHTZTBSHW<>
The JG statement checks if the argument count is greater than  or not. We now store for
ease, the argument count has its own variable, which we call DPVOU . The code block starts
with the colon and then all of the following code that is indented with four spaces is part of
the code that will execute when the condition returns to true.
When we return to the previous indent level, we return to the main code block and we
execute the code regardless of the status of the condition.
Using Python as a Bash Scripting Alternative Chapter 14
[ 240 ]We can see this illustrated in the following screenshot, where we can execute the script with
and without the argument:
Reading user input
If we want the welcome message to greet us by name, no matter whether we supply the
argument to the script or not, we can add in a prompt to capture the data while the script is
running. Python makes this simple and easy to implement. We can see, from the edited file
shown in the screenshot that follows, how this is achieved:
We make use of a new variable in the script that we set in the main block, initially, to be an
empty string. We set it here to make the variable available to the complete script and all
code blocks:

Using Python as a Bash Scripting Alternative Chapter 14
[ 241 ]The JOQVU  function in Python 3 (or SBX@JOQVU  in Python 2) can be used to get user input.
We store that input in the OBNF  variable. If we have supplied an argument we pick it up on
the code in the FMTF  block and set the OBNF  variable to the first supplied argument. It is this
that is used in the QSJOU  statement back in the main block.
Using Python to write to files
To add some variety to this chapter, we will now look at printing this data to a file. Again
using Python, this is quite a simple and easy way to pick up. We will start by making a copy
of our existing BSHTQZ . We will copy this to )0.&CJOGJMFQZ . The new GJMFQZ
should read similar to the following screenshot and have the execute permission set:
You will notice that we have just altered the final lines and instead of print we now open a
file. We also see more of the object-orientated nature of Python, whereby it dynamically
assigns the XSJUF
  and DMPTF
  methods to the object log, as it is seen as an instance of a
file. When we open the file, we open it up for the purpose of appending, meaning that we
do not overwrite the existing content if it is already there. If the file is not there, we will
create a new file. If we use X, we will open the file for writing, which might translate to
overwriting, so take care.
You can see that this is an easy task; this is why Python is used in many applications and is
taught widely in schools.
Using Python as a Bash Scripting Alternative Chapter 14
[ 242 ]String manipulation
Dealing with strings in Python is very simple: you can search, replace, change character
case, and perform other manipulations with ease:
To search for a string, you can use the find method like this:
VTSCJOQZUIPO
TUS8FMDPNFUP1ZUIPOTDSJQUJOHXPSME
QSJOU
TUSGJOE
TDSJQUJOH
The string count in Python starts from zero too, so the position of the word TDSJQUJOH  is at
.
You can get a specific substring using square brackets like this:
VTSCJOQZUIPO
TUS8FMDPNFUP1ZUIPOTDSJQUJOHXPSME
QSJOU
TUS<>(FUUIFGJSTUMFUUFST
[FSPCBTFE
QSJOU
TUS<>4UBSUGSPNUIFTFDPOEMFUUFS
QSJOU
TUS<>GSPNUIFUIJSEUPGJGUIMFUUFS
QSJOU
TUS<>NFBOTUIFMBTUMFUUFSJGZPVEPO	ULOPXUIFMFOHUI

Using Python as a Bash Scripting Alternative Chapter 14
[ 243 ]To replace a string, you can use the replace method like this:
VTSCJOQZUIPO
TUS8FMDPNFUP1ZUIPOTDSJQUJOHXPSME
TUSTUSSFQMBDF
1ZUIPO4IFMM
QSJOU
TUS
To change the character case, you can use VQQFS
  and MPXFS
  functions:
As you can see, working with strings in Python is very simple. Python as an alternative
scripting language is an awesome choice.
The power of Python lies in the libraries available out there. Literally, there are thousands
of libraries for everything you can imagine.
Summary
This now finishes our look at Python and it certainly has been a brief tour. We can again
emphasize the similarities that you will see in many languages and the importance of
learning any coding language. What you learn in one language will help in most other
languages that you come across.
What we learn from the Zen of Python will help us design and develop great code. We can
print the Zen of Python using the following Python code:
>>>import this
Using Python as a Bash Scripting Alternative Chapter 14
[ 244 ]We can type the code on the REPL prompt. Keeping your code clean and well spaced-out
will aid readability and ultimately this will help with code maintenance.
We have also seen that Python likes you to be explicit in your code and will not implicitly
convert data types.
Finally, we saw how to manipulate strings using Python.
We are also at the end of the book but hopefully, the start of your scripting career. Good
luck and thank you for reading.
Questions
How many characters will be printed from the following code?1.
VTSCJOQZUIPO
TUS5FTUJOH1ZUIPO
QSJOU
TUS<>
How many words will be printed from the following code?2.
VTSCJOQZUIPO
QSJOU
MFO
TZTBSHW
4PMVUJPO/PUIJOH
How many words will be printed from the following code?3.
VTSCJOQZUIPO
JNQPSUTZT
QSJOU
)FMMPTZTBSHW<>
Further reading
Please see the following for further reading relating to this chapter:
IUUQTXXXQZUIPOPSHBCPVUHFUUJOHTUBSUFE
IUUQTEPDTQZUIPOPSH
Assessments
Chapter 1
The error is in the second line: There should be no spaces in the variable1.
declaration.
CJOCBTI
WBS8FMDPNFUPCBTITDSJQUJOH
FDIPWBS
The result will be 5VFTEBZ  because the array is zero based. 2.
There are two errors here: the first error is the space in the variable declaration3.
and the second error is the usage of single quotes where we should use backticks
instead.
Solution:
CJOCBTIGJMFT	MTMB	FDIPGJMFT
The value of C variable will be D and the value of D will be B. 4.
Since we didn't use dollar signs in the assignment lines, the variable will take the
character value instead of the integer value.
Chapter 2
Three1.
This is because the whole bash shebang is primarily a comment, so there are three
lines of comment.
There is no space between the option C and its value, so it will be treated as an 2.
option.
B
C
D
Assessments
[ 246 ]13.
Four
This is because we have five passed parameters and we use the shift to drop a
parameter.
24.
O
This is because it's on the left and the TIJGU  command drops parameters from the
left.
Chapter 3
'BMTF1.
Since lower-case characters have a higher ASCII order, the statement will
return 'BMTF .
Both are correct and will return the same result, which is 4USJOHT  are not 2.
identical.
5ISFF3.
We can use the following:
HF: Greater than or equal to
HU: Greater than
OF: Not equal to
True4.
Since one test is enough to return true, so we can be sure that the second test will
return true.
Assessments
[ 247 ]Chapter 4
We can make the following changes :1.
)FMMPNFTTBHF\
QSFGJYIFMMP
CPEZ<
FDIP	)FMMP\]GJSTUTFDPOEUIJSE]^	
>
EFTDSJQUJPO)FMMPNFTTBHF
^
The TPVSDF  command. 2.
Chapter 5
By using 

: 1.
CJOCBTI
OVN


FDIPOVN
The problem is with the space in the filename. To fix it, put the filename between2.
quotes:
$ rm "my file"
There is no dollar sign before the parentheses:3.
#!/bin/bash
a=$(( 8 + 4 ))
echo $a
Chapter 6
No lines. Since the loop output is redirected to a file, nothing will appear on the1.
screen.
Four. The loop will start at  and continue until it reaches , it will match the 2.
condition which is greater than or equal, and it will break the loop.
Assessments
[ 248 ]The problem is with the comma in the GPS loop definition. It should be semicolon 3.
instead. So the correct script should be as follows:
#!/bin/bash
for (( v=1; v <= 10; v++ ))
do
echo "value is $v"
done
Since the decrement statement is outside the loop, the count variable will be the4.
same value, which is . It's an endless loop, it will print  forever, and to stop
it, you need to press  Ctrl + C.
Chapter 7
Since we used the  variable not !, the function will return the first element 1.
only.
. Yes, it's a global variable, but because we printed the value before the 2.
function call, the variable isn't affected.
Missing brackets 
 or adding the keyword function before the function name. It 3.
should be written like this:
DMFBO@GJMF
\
JT@GJMF
#&'03&
XDM
FDIP5IFGJMFTUBSUTXJUI#&'03&
TFEJCBL	?=TE?E	
"'5&3
XDM
FDIP5IFGJMFJTOPX"'5&3
^
The problem is in the function call. We shouldn't use brackets 
 during a 4.
function call. Brackets should only be used in function definitions. The correct
code will be like this:
CJOCBTI
NZGVOD
\
BSS!
FDIP5IFBSSBZGSPNJOTJEFUIFGVODUJPO\BSS<>^
^
Assessments
[ 249 ]UFTU@BSS

FDIP5IFPSJHJBOMBSSBZJT\UFTU@BSS<>^
NZGVOD\UFTU@BSS<>^
Chapter 8
None. Because you are searching for Sed with a capital letter which does not exist1.
None. The delete command E only deletes lines from the stream, not the file. To 2.
delete from the file, you can use the J option.
Fourth line. Because we used append command a, it will be inserted after the 3.
specified position.
None, because the X flag is only used with the substitute command T. 4.
Chapter 9
You can use the following command to print line number 50:1.
$ sed -n '50 p ' /etc/httpd/conf/httpd.conf
You can use following command to change Apache default port  to  : 2.
$ sed -i '0,/Listen [0-9]*/s//Listen 8080/'
/etc/httpd/conf/httpd.conf
We search for -JTUFO , where it defines the Apache default port, search for the
number beside it, and change it to -JTUFO .
Chapter 10
Nothing1.
You should use the variable name without the dollar sign to print it.
Solution: zero2.
Because you should print  instead of  where  is the first field.
Assessments
[ 250 ]The XIJMF  loop should iterate with J value less than  not . 3.
4.
Because the only user that has UID less than  is root (6*% ), so one line will be
printed.
Chapter 11
0 lines1.
Because there is a period after the word BXFTPNF , if you want to print that line,
you can use the following command:
$ awk '/awesome\.$/{print $0}' myfile
2 lines2.
Since we search for the line that contains the word TDSJQUJOH . With a period
after it followed by any text, this pattern only exists in two lines because the third
line doesn't contain a period after the word.
3 lines3.
As we used the question mark that means the character class is not a must for the
pattern to match.
Nothing4.
As we used the pipe symbol, which is an ERE character, and as we used sed, we
must use the S option for sed to turn the extended engine on.
Assessments
[ 251 ]Chapter 12
Field 11.
You can use QSJOU/3  or alternatively pipe the output to XDM 2.
We must use M otherwise, it will count words instead.
$ awk '{print $1}' access.log | sort | uniq -c
$ awk '{print $7}' access.log | grep 'php' | sort | uniq -c | sort
-nr | head -n 1
You should use head O  to get the one page only.
Chapter 13
Use the MBTUMPH  command 1.
$ lastlog | awk ' /Never logged/ { print $1}'
Use the XD command 2.
$ lastlog | awk ' /Never logged/ { print $1}' | wc -l
Zero. Because the line ends with two asterisks.3.
Chapter 14
81.
Since we are using the TZT module, we should import it first. 2.
So the correct code should look like this:
VTSCJOQZUIPO
JNQPSUTZT
QSJOU
MFO
TZTBSHW
23.
Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books by Packt:
Mastering Linux Security and Hardening
Donald A. Tevault
ISBN: 978-1-78862-030-7
Use various techniques to prevent intruders from accessing sensitive data
Prevent intruders from planting malware, and detect whether malware has been
planted
Prevent insiders from accessing data that they aren't authorized to access
Do quick checks to see whether a computer is running network services that it
doesn't need to run
Learn security techniques that are common to all Linux distros, and some that are
distro-specific
Other Books You May Enjoy
[ 253 ]
Linux Shell Scripting Cookbook
Clif Flynt, Sarath Lakshman, Shantanu Tushar
ISBN: 978-1-78588-198-5
Interact with websites via scripts
Write shell scripts to mine and process data from the Web
Automate system backups and other repetitive tasks with crontab
Create, compress, and encrypt archives of your critical data.
Configure and monitor Ethernet and wireless networks
Monitor and log network and system activity
Tune your system for optimal performance
Improve your system's security
Identify resource hogs and network bottlenecks
Extract audio from video files
Create web photo albums
Use git or fossil to manage revision control and interact with FOSS projects
Create and maintain Linux containers and Virtual Machines
Run a private Cloud server
Other Books You May Enjoy
[ 254 ]Leave a review - let other readers know what
you think
Please share your thoughts on this book with others by leaving a review on the site that you
bought it from. If you purchased the book from Amazon, please leave us an honest review
on this book's Amazon page. This is vital so that other potential readers can see and use
your unbiased opinion to make purchasing decisions, we can understand what our
customers think about our products, and our authors can see your feedback on the title that
they have worked with Packt to create. It will only take a few minutes of your time, but is
valuable to other potential customers, our authors, and Packt. Thank you!
Index
A
abbreviations  , 
advanced tests
   with double brackets [[  
anchor characters
   used, for defining BRE patterns  , 
Apache name-based Virtual Hosts
   about  
   lines, isolating  
   sed script files  
   template, creating  , 
Apache Virtual Hosts  , 
append command  
arguments
   counting  , 
   passing  
   supplying  
arithmetic operations
   with double parenthesis ((  
arrays
   passing, to functions  , 
asterisk
   used, for defining BRE patterns  , , 
AWK ranges
   lastlog command  
   matched rows, counting  , 
   rows, horizontal filtering with AWK  
   used, for excluding data  
AWK record separator
   manipulating, for XML catalog  , 
   manipulating, for XML data report  
   manipulating, with Apache Virtual Hosts  ,

AWK
   built-in functions  
   built-in variables  ,    BWK awk  
   control files  
   gawk  
   history  
   mawk  
   tawk  
   user-defined variables  , 
B
backup2.sh
   creating, with elif  , 
bash command
   hierarchy  
   PATH command  , 
   type command  , 
bash debug plugin
   about  
   URL  
   using  
bash scripting  
Bash shell  
bashdb package
   installing  
Basic Regular Expression (BRE) engine  
Bourne shell  
BRE patterns
   anchor characters, using  , 
   asterisk, using  , , 
   character class, using  , 
   character range, specifying  , 
   defining  
   dot character  
   special character classes  , 
browser data
   displaying  
built-in functions, AWK  
[ 256 ]built-in variables, AWK
   about  , , 
   FIELDWIDTHS  
   FILENAME  
   FNR  
   FS  
   IGNORECASE  
   NF  
   OFS  
   ORS  
   RS  
BWK awk  
C
case statement
   using  , 
change command  
character class
   used, for defining BRE patterns  , 
code snippets
   color, using in Linux Terminal  , 
   using  , 
command substitution  
command-line lists
   AND notation  
   OR notation  
   used, for decision paths  , 
conditional statements
   about  
   creating, with if  
   for loops  
   if command  , 
   while loops  , 
control files  
Csh  
curly braces
   used, for defining ERE patterns  , 
custom snippets
   creating, with VS Code  , , , , 
D
data
   displaying, from web logs  
   excluding, with AWK ranges  decision paths
   with command-line lists  , 
delete command  
directories
   checking, with test and if command  
   counting  
dot character
   used, for defining BRE patterns  
double brackets [[
   advanced features  
   pattern matching  
   regular expressions  
   used, for advanced tests  
   white space  
double parenthesis ((
   for standard arithmetic tests  
   used, for arithmetic operations  
   used, for parameter manipulation  , 
   using, in simplest format  
E
echo command
   about  
   using, with options  , 
elif
   backup2.sh, creating  , 
   using  
email logs  
entered characters
   limiting  
entered text
   visibility, controlling  
environment variables  
ERE patterns
   curly braces, using  , 
   defining  
   expression grouping  
   pipe character, using  
   plus sign, using  , 
   question mark, using  , 
exit command  
Extended Regular Expression (ERE) engine  
[ 257 ]F
file types
   testing  
files
   checking, with test and if command  
   content, displaying  , , 
   content, filtering  , , 
   counting  
   input, reading  , 
   reading  
   writing, with Python  
for loops
   about  , , , 
   advanced for loops  , 
   in C-style  , 
frontend
   building, with grep command  , 
functions
   about  , 
   arrays, passing  , 
   parameters, passing  , , , 
   using, in menus  
   values, returning from  , 
G
gawk  
gedit
   configuring  , 
global regular expression print (grep)
   catalog entries, isolating  
   CPU count, listing  , , , 
   CSV file  , 
   CSV files, parsing  
   received data, displaying on interface  , 
   used, for displaying text  
   user account data, displaying  , 
   using  , 
global replacement  , 
GNU's Not Unix (GNU)  
grep command
   frontend, building  , H
Hello World program
   writing, in Python  
hello world script
   creating  , 
   echo command  
   executing  , 
   executing, with arguments  , 
   exit command  
   exit status, checking  
   quotes, using  , 
   script name, printing  
   shebang  
   unique name, ensuring  
highest ranking IP address
   displaying  , 
HTTPD log file
   format  , 
I
if command  , 
if command, with test command
   directories, checking  
   files, checking  
   numbers, checking  
   strings, checking  , , 
   tests, combining  , 
   using  
if
   conditional statements, creating  , 
   extending, with else  
insert command  
integers
   testing  
integrated development environments (IDEs)  
internal field separator (IFS)
   about  
   modifying  
K
Ksh shell  
[ 258 ]L
lastlog command  
Linux shells
   Bash shell  
   Csh  
   Ksh shell  
   Sh shell  
   tcsh  
   types  
Linux Terminal
   code snippets, coloring  , 
lists
   user inputs, verifying  
loop output
   loop, controlling  , 
   redirecting  
M
mawk  
multiple sed commands  
MySQL/MariaDB
   connecting to  , 
N
named elements  
nano
   configuring  
nested loops  
numbers
   checking, with test and if command  
O
operator menus
   creating  , 
options
   -a  
   -c  
   -d  
   -e  
   -f  
   -h  
   -i  
   -l     -o  
   -q  
   -r  
   -s  
   -v  
   -x  
   -y  
   parameters, passing  , 
   passing  , , 
   standard  
   values, reading  , 
Output Field Separator (OFS)  
output
   formatting  , , 
P
parameter expansion  
parameter substitution  
parameters
   defaults, setting  , 
   manipulating, with double parenthesis ((  , 
   passing, with options  , 
   providing  
   quotes, using  , , 
   special parameters  , 
   types  
   variables  
PATH command  , 
pattern matching
   with double brackets [[  
Perl-compatible regular expression (PCRE)  
ping command
   using  
pipe character
   used, for defining ERE patterns  
plus sign
   used, for defining ERE patterns  , 
Portable Operating System Interface (POSIX)  
Python
   about  , 
   arguments, passing  
   Hello World program  
   used, for writing to files  
[ 259 ]Q
question mark
   used, for defining ERE patterns  , 
quotes
   using, in parameters  , , 
R
Rapid Application Development  
read command
   used, for enhancing script  , 
read prompts
   used, for enhancing scripts  , 
recursive functions  
regex engine
   about  
   Basic Regular Expression (BRE) engine  
   Extended Regular Expression (ERE) engine  
regular expressions
   about  
   script  
   with double brackets [[  
rows
   horizontal filtering, with AWK  
   matched rows, counting  , 
   number of fields, filtering based on conditions 
, 
S
script
   backing-up  , 
   commenting  
   debugging  , , , , 
   enhancing, with read command  , 
   enhancing, with read prompts  , 
   server, connecting  
sed commands
   about  
   append command  
   change command  
   delete command  
   insert command  
   multiple sed commands  
   transform command  
server connection   establishing, with script  
   files, reading  
   with MySQL/MariaDB  , 
   with ping command  
   with SSH  
Sh shell  
shebang  
single square bracket
   used, for conditional tests  , , 
special character classes
   used, for defining BRE patterns  
special parameters
   about  , 
   B option  
   h option  
   H option  
   i option  
   m option  
SSH
   connecting to  
standard arithmetic tests
   with double parenthesis ((  
stream editor (sed)
   about  , 
   file, editing  , 
   global replacement  , 
   substitute command  , 
   substitution, limiting  
strings
   checking, with test and if command  , , 
   manipulation  , 
   testing  , 
substitute command  , 
substitution
   limiting  
T
tawk  
tcsh  
technical requisites  , , 
test command, with if command
   directories, checking  
   files, checking  
   numbers, checking  
   strings, checking  , , 
   tests, combining  , 
   using  
test command
   files, testing  
   logic, adding  
   recapping  
   single square bracket, using  , , 
test shell built-in
   file types, testing  
   integers, testing  
   strings, testing  , 
   using  , 
text editors
   gedit, configuring  , 
   nano, configuring  
   preparing, for scripting  
   vim, configuring  
transform command  
type command  , 
U
until loops  
user input
   reading  
   verifying, with lists  
user-defined variables  , 
user-defined variables, AWK  , 
users, with UID
   displaying  , 
V
values
   returning, from functions  variables
   about  
   declaring  
   environment variables  , 
   scope  , , 
   user-defined variables  , 
vim, options
   set autoindent  
   set expandtab  
   set nohlsearch  
   set showmode  
   set tabstop=4  
   syntax on  
vim
   configuring  
virtual host creation
   automating  , 
   data, prompting during site creation  
Visual Studio Code
   custom snippets, creating  , , , , 
   URL  
   using  
W
web logs
   404 errors, summarizing  
   data, displaying  
   entries, selecting by date  , 
   HTTP access codes, summarizing  , ,

   image hotlinking, identifying  , 
   resources hits, checking  , 
while loops  , , 
whitespace  